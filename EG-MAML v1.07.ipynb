{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNDj4JlmGpD_"
   },
   "source": [
    "# Koding Menggunakan EG-MAML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcM2_ZyNRKHT",
    "outputId": "9d9d3f09-041b-440d-ac05-f1eed69a2a7b"
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# By: Imam Cholissodin | imamcs@ub.ac.id\n",
    "# Dosen Fakultas Ilmu Komputer (Filkom)\n",
    "# Universitas Brawijaya (UB)\n",
    "# Tgl 12 Oktober 2024, \n",
    "# Semoga Bermanfaat. Aamiin :D\n",
    "# ====================================\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data - Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Get Feature val. use One Hot - Target Rekomendasi Makanan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang data awal =  49\n",
      "Jumlah baris data =  5\n",
      "Panjang data unik =  44\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air kelapa</th>\n",
       "      <th>air putih</th>\n",
       "      <th>bawang putih</th>\n",
       "      <th>bayam</th>\n",
       "      <th>biji-bijian utuh dan kacang-kacangan</th>\n",
       "      <th>brokoli</th>\n",
       "      <th>bubur</th>\n",
       "      <th>bubur gandum dari biji food grade (oatmeal)</th>\n",
       "      <th>bubur kacang hijau</th>\n",
       "      <th>daging (sapi)</th>\n",
       "      <th>...</th>\n",
       "      <th>sup ayam</th>\n",
       "      <th>sup hangat</th>\n",
       "      <th>susu kedelai</th>\n",
       "      <th>teh chamomile</th>\n",
       "      <th>telur</th>\n",
       "      <th>tiram</th>\n",
       "      <th>tuna</th>\n",
       "      <th>wijen</th>\n",
       "      <th>wortel</th>\n",
       "      <th>yoghurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   air kelapa  air putih  bawang putih  bayam  \\\n",
       "0           0          0             0      0   \n",
       "1           0          0             0      1   \n",
       "2           1          0             0      0   \n",
       "3           0          0             0      0   \n",
       "4           0          1             1      0   \n",
       "\n",
       "   biji-bijian utuh dan kacang-kacangan  brokoli  bubur  \\\n",
       "0                                     1        0      0   \n",
       "1                                     0        0      0   \n",
       "2                                     0        1      0   \n",
       "3                                     0        0      1   \n",
       "4                                     0        0      0   \n",
       "\n",
       "   bubur gandum dari biji food grade (oatmeal)  bubur kacang hijau  \\\n",
       "0                                            0                   0   \n",
       "1                                            0                   0   \n",
       "2                                            0                   0   \n",
       "3                                            1                   1   \n",
       "4                                            0                   0   \n",
       "\n",
       "   daging (sapi)  ...  sup ayam  sup hangat  susu kedelai  teh chamomile  \\\n",
       "0              0  ...         0           0             0              0   \n",
       "1              0  ...         0           0             0              0   \n",
       "2              1  ...         0           0             1              0   \n",
       "3              0  ...         0           1             0              0   \n",
       "4              0  ...         1           0             0              1   \n",
       "\n",
       "   telur  tiram  tuna  wijen  wortel  yoghurt  \n",
       "0      1      0     0      0       0        0  \n",
       "1      0      0     1      0       0        0  \n",
       "2      1      1     0      1       0        1  \n",
       "3      1      0     0      0       0        0  \n",
       "4      0      0     0      0       1        0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data one-hot encoded telah ditambahkan ke dataset/one_hot_encoded_data.xlsx pada sheet 'OneHotEncodedDataTarget'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Data awal\n",
    "data = \"\"\"\n",
    "1. Biji-bijian Utuh dan Kacang-kacangan 2. Sayuan Berdaun Gelap 3. Telur 4. Daging Merah Segar\n",
    "1. Jahe 2. Bayam 3. Kuning Telur 4. Tuna\n",
    "1. Jeruk 2. Delima 3. Yoghurt 4. Madu 5. Telur 6. Ikan 7. Susu Kedelai 8. Daging (Sapi) 9. Tiram 10. Hati Ayam 11. Kalkun 12. Wijen 13. Labu 14. Kentang 15. Brokoli 16. Air Kelapa\n",
    "1. Sikat Kayu Siwak 2. Sup Hangat 3. Bubur 4. Jus Pisang 5. Jus Melon 6. Jus Blewah 7. Jus Tomat 8. Bubur Gandum dari Biji Food Grade (Oatmeal) 9. Telur 10. Bubur Kacang Hijau 11. Smoothie Bowl (Mix Jus Bus Buah dan Chia Seed) 12. Pasta\n",
    "1. Air Putih 2. Wortel 3. Kubis 4. Kentang 5. Sup Ayam 6. Jus Delima 7. Pisang 8. Peppermint 9. Madu 10. Kunyit 11. Jahe 12. Teh chamomile 13. Bawang putih\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Hapus \"number + dot\" di awal baris sebelum spasi\n",
    "cleaned_data = re.sub(r'^\\d+\\.\\s', '', data, flags=re.MULTILINE)\n",
    "\n",
    "# Step 2: Split baris menjadi list untuk memanipulasi tiap baris\n",
    "lines = cleaned_data.strip().split('\\n')\n",
    "\n",
    "# Step 3: Tambahkan \",\\n\" di akhir setiap baris kecuali baris terakhir\n",
    "formatted_data = \",\\n\".join(lines) + '\\n'\n",
    "\n",
    "# Step: Replace \"number + dot\" with comma\n",
    "cleaned_data2 = re.sub(r'\\d+\\.\\s', ', ', cleaned_data)\n",
    "\n",
    "# Replace newline characters with commas\n",
    "cleaned_data3 = re.sub(r'\\n', ', ', cleaned_data2)\n",
    "\n",
    "# Step 4: Split the data by comma to count elements\n",
    "split_data = [item.strip() for item in cleaned_data3.split(',') if item.strip()]\n",
    "\n",
    "# Hitung jumlah elemen\n",
    "count = len(split_data)\n",
    "print('Panjang data awal = ', count)\n",
    "\n",
    "# Step 5: Split cleaned_data into a list of lines\n",
    "lines = cleaned_data2.strip().split('\\n')\n",
    "\n",
    "# Step 6: Store each line as a string in a list\n",
    "lines_as_strings = [line.strip() for line in lines]\n",
    "\n",
    "# Split each row into individual items and strip any extra whitespace\n",
    "rows = [set(item.strip().lower() for item in row.split(',')) for row in lines_as_strings]\n",
    "\n",
    "# Hitung jumlah baris data\n",
    "print('Jumlah baris data = ', len(rows))\n",
    "\n",
    "# Ambil semua item unik dari semua baris\n",
    "unique_items = sorted(set(item for row in rows for item in row))\n",
    "print('Panjang data unik = ', len(unique_items))\n",
    "\n",
    "# Buat DataFrame dengan one-hot encoding\n",
    "one_hot_data = []\n",
    "for row in rows:\n",
    "    one_hot_data.append([1 if item in row else 0 for item in unique_items])\n",
    "\n",
    "# Buat tabel dengan pandas\n",
    "df = pd.DataFrame(one_hot_data, columns=unique_items)\n",
    "\n",
    "# Tampilkan tabel yang dihasilkan\n",
    "display(df)\n",
    "\n",
    "# Simpan tabel hasil ke file Excel dengan nama sheet tertentu\n",
    "output_file = \"dataset/one_hot_encoded_data.xlsx\"\n",
    "sheet_name = \"OneHotEncodedDataTarget\"  # Nama sheet yang diinginkan\n",
    "\n",
    "# Cek apakah file sudah ada dan append data jika ada\n",
    "try:\n",
    "    with pd.ExcelWriter(output_file, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "    print(f\"Data one-hot encoded telah ditambahkan ke {output_file} pada sheet '{sheet_name}'\")\n",
    "except FileNotFoundError:\n",
    "    # Jika file tidak ada, simpan sebagai file baru\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "    print(f\"File {output_file} telah dibuat dan data ditambahkan pada sheet '{sheet_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Get Feature val. use One Hot - Input dr Gejala Penyakit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang data awal =  14\n",
      "baris data =  5\n",
      "panjang data unik =  14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batuk</th>\n",
       "      <th>kehilangan keseimbangan</th>\n",
       "      <th>luka kecil pada mulut/lidah</th>\n",
       "      <th>migraine</th>\n",
       "      <th>nyeri di gigi atau gusi</th>\n",
       "      <th>nyeri di sekitar mulut/lidah/pipi/kepala</th>\n",
       "      <th>nyeri tenggorokan</th>\n",
       "      <th>pusing berputar</th>\n",
       "      <th>radang tenggorokan</th>\n",
       "      <th>sakit gigi</th>\n",
       "      <th>sakit kepala berdenyut di satu sisi</th>\n",
       "      <th>sariawan (di mulut/lidah)</th>\n",
       "      <th>suara serak</th>\n",
       "      <th>vertigo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batuk  kehilangan keseimbangan  luka kecil pada mulut/lidah  migraine  \\\n",
       "0      0                        0                            0         1   \n",
       "1      0                        1                            0         0   \n",
       "2      0                        0                            1         0   \n",
       "3      0                        0                            0         0   \n",
       "4      1                        0                            0         0   \n",
       "\n",
       "   nyeri di gigi atau gusi  nyeri di sekitar mulut/lidah/pipi/kepala  \\\n",
       "0                        0                                         0   \n",
       "1                        0                                         0   \n",
       "2                        0                                         1   \n",
       "3                        1                                         0   \n",
       "4                        0                                         0   \n",
       "\n",
       "   nyeri tenggorokan  pusing berputar  radang tenggorokan  sakit gigi  \\\n",
       "0                  0                0                   0           0   \n",
       "1                  0                1                   0           0   \n",
       "2                  0                0                   0           0   \n",
       "3                  0                0                   0           1   \n",
       "4                  1                0                   1           0   \n",
       "\n",
       "   sakit kepala berdenyut di satu sisi  sariawan (di mulut/lidah)  \\\n",
       "0                                    1                          0   \n",
       "1                                    0                          0   \n",
       "2                                    0                          1   \n",
       "3                                    0                          0   \n",
       "4                                    0                          0   \n",
       "\n",
       "   suara serak  vertigo  \n",
       "0            0        0  \n",
       "1            0        1  \n",
       "2            0        0  \n",
       "3            0        0  \n",
       "4            1        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data one-hot encoded telah ditambahkan ke dataset/one_hot_encoded_data.xlsx pada sheet 'OneHotEncodedDataInput'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provided data in separate lines\n",
    "# data = [\n",
    "#     \"Biji-bijian Utuh dan Kacang-kacangan, Sayuan Berdaun Gelap, Telur, Daging Merah Segar\",\n",
    "#     \"Jahe, Bayam, Kuning Telur, Tuna\",\n",
    "#     \"Jeruk, Delima, Yoghurt, Madu, Telur, Ikan, Susu Kedelai, Daging (Sapi), Tiram, Hati Ayam, Kalkun, Wijen, Labu, Kentang, Brokoli, Air Kelapa\",\n",
    "#     \"Sikat Kayu Siwak, Sup Hangat, Bubur, Jus Pisang, Jus Melon, Jus Blewah, Jus Tomat, Bubur Gandum dari Biji Food Grade (Oatmeal), Telur, Bubur Kacang Hijau, Smoothie Bowl (Mix Jus Bus Buah dan Chia Seed), Pasta\",\n",
    "#     \"Air Putih, Wortel, Kubis, Kentang, Sup Ayam, Jus Delima, Pisang, Peppermint, Madu, Kunyit, Jahe, Teh chamomile, Bawang putih\"\n",
    "# ]\n",
    "\n",
    "data = [\n",
    "    \"Migraine, Sakit kepala berdenyut di satu sisi\",\n",
    "    \"Vertigo, Pusing berputar, kehilangan keseimbangan\",\n",
    "    \"Sariawan (di mulut/lidah), Luka kecil pada mulut/lidah, nyeri di sekitar mulut/lidah/pipi/kepala\",\n",
    "    \"Sakit Gigi, Nyeri di gigi atau gusi\",\n",
    "    \"Radang Tenggorokan, Nyeri tenggorokan, batuk, suara serak\"\n",
    "]\n",
    "\n",
    "# data = \"\"\"\n",
    "# 1. Biji-bijian Utuh dan Kacang-kacangan 2. Sayuan Berdaun Gelap 3. Telur 4. Daging Merah Segar\n",
    "# 1. Jahe 2. Bayam 3. Kuning Telur 4. Tuna\n",
    "# 1. Jeruk 2. Delima 3. Yoghurt 4. Madu 5. Telur 6. Ikan 7. Susu Kedelai 8. Daging (Sapi) 9. Tiram 10. Hati Ayam 11. Kalkun 12. Wijen 13. Labu 14. Kentang 15. Brokoli 16. Air Kelapa\n",
    "# 1. Sikat Kayu Siwak 2. Sup Hangat 3. Bubur 4. Jus Pisang 5. Jus Melon 6. Jus Blewah 7. Jus Tomat 8. Bubur Gandum dari Biji Food Grade (Oatmeal) 9. Telur 10. Bubur Kacang Hijau 11. Smoothie Bowl (Mix Jus Bus Buah dan Chia Seed) 12. Pasta\n",
    "# 1. Air Putih 2. Wortel 3. Kubis 4. Kentang 5. Sup Ayam 6. Jus Delima 7. Pisang 8. Peppermint 9. Madu 10. Kunyit 11. Jahe 12. Teh chamomile 13. Bawang putih\n",
    "# \"\"\"\n",
    "\n",
    "# Fungsi untuk menghitung banyak kata\n",
    "def count_words(data):\n",
    "    word_count = 0\n",
    "    for entry in data:\n",
    "        # Memisahkan setiap string berdasarkan koma\n",
    "        # print(entry)\n",
    "        parts = entry.split(',')\n",
    "        # print(parts)\n",
    "        word_count += len(parts)\n",
    "        # for part in parts:\n",
    "        #     # Menghitung jumlah kata dalam setiap bagian\n",
    "        #     word_count += len(part.strip().split())\n",
    "        #     print(word_count)\n",
    "    return word_count\n",
    "print('Panjang data awal = ', count_words(data))\n",
    "\n",
    "# Split each row into individual items and strip any extra whitespace\n",
    "rows = [set(item.strip().lower() for item in row.split(',')) for row in data]\n",
    "\n",
    "# print(rows)\n",
    "\n",
    "print('baris data = ', len(rows))\n",
    "\n",
    "# Get all unique items from all rows\n",
    "unique_items = sorted(set(item for row in rows for item in row))\n",
    "\n",
    "print('panjang data unik = ', len(unique_items))\n",
    "\n",
    "# Create a DataFrame with rows of data, where each column is a unique item (one-hot encoded)\n",
    "one_hot_data = []\n",
    "for row in rows:\n",
    "    one_hot_data.append([1 if item in row else 0 for item in unique_items])\n",
    "\n",
    "# Create the table with pandas\n",
    "df = pd.DataFrame(one_hot_data, columns=unique_items)\n",
    "\n",
    "# Display the resulting table\n",
    "display(df)\n",
    "\n",
    "# # Save the resulting table to an Excel file\n",
    "# output_file = \"dataset/one_hot_encoded_data.xlsx\"\n",
    "# df.to_excel(output_file, index=False)\n",
    "\n",
    "# print(f\"The one-hot encoded data has been saved to {output_file}\")\n",
    "\n",
    "# Simpan tabel hasil ke file Excel dengan nama sheet tertentu\n",
    "output_file = \"dataset/one_hot_encoded_data.xlsx\"\n",
    "sheet_name = \"OneHotEncodedDataInput\"  # Nama sheet yang diinginkan\n",
    "\n",
    "# # Simpan DataFrame ke Excel dengan nama sheet tertentu\n",
    "# with pd.ExcelWriter(output_file) as writer:\n",
    "#     df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "\n",
    "# print(f\"Data one-hot encoded telah disimpan ke {output_file} pada sheet '{sheet_name}'\")\n",
    "\n",
    "# Cek apakah file sudah ada dan append data jika ada\n",
    "try:\n",
    "    with pd.ExcelWriter(output_file, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "    print(f\"Data one-hot encoded telah ditambahkan ke {output_file} pada sheet '{sheet_name}'\")\n",
    "except FileNotFoundError:\n",
    "    # Jika file tidak ada, simpan sebagai file baru\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "    print(f\"File {output_file} telah dibuat dan data ditambahkan pada sheet '{sheet_name}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Tiny Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'dataset_init/dataset_v5.xlsx' created with sheets: 'Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg', and 'CombinedSheet'.\n",
      "File 'dataset_init/dataset_v4.json' created with data from all sheets in JSON format.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define constants for the number of rows to take from each sheet\n",
    "n_data_model_1 = 3\n",
    "n_data_model_2 = 2\n",
    "\n",
    "# Define file paths\n",
    "output_file = \"dataset/one_hot_encoded_data.xlsx\"\n",
    "file_path = \"dataset_init/dataset_v5.xlsx\"\n",
    "file_path_json = \"dataset_init/dataset_v4.json\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "# Load the input and target data sheets from the output file\n",
    "input_data = pd.read_excel(output_file, sheet_name=\"OneHotEncodedDataInput\")\n",
    "target_data = pd.read_excel(output_file, sheet_name=\"OneHotEncodedDataTarget\")\n",
    "\n",
    "# Create datasets for each required sheet by concatenating input and target side-by-side\n",
    "sheet1_data = pd.concat([input_data.iloc[:n_data_model_1, :], target_data.iloc[:n_data_model_1, :]], axis=1)\n",
    "sheet2_data = pd.concat([input_data.iloc[-n_data_model_2:, :], target_data.iloc[-n_data_model_2:, :]], axis=1)\n",
    "combined_data = pd.concat([input_data, target_data], axis=1)\n",
    "\n",
    "# Save these datasets into a new Excel file with the specified sheet names\n",
    "with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "    sheet1_data.to_excel(writer, sheet_name=\"Sheet1-KM-SAR-Tiny-Reg\", index=False)\n",
    "    sheet2_data.to_excel(writer, sheet_name=\"Sheet2-M-SAK-T-Tiny-Reg\", index=False)\n",
    "    combined_data.to_excel(writer, sheet_name=\"CombinedSheet\", index=False)\n",
    "\n",
    "print(f\"File '{file_path}' created with sheets: 'Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg', and 'CombinedSheet'.\")\n",
    "\n",
    "# Convert the Excel data to JSON and save to a new file\n",
    "# We load the entire workbook, convert each sheet to JSON, and save it to the specified JSON file path\n",
    "excel_data = pd.read_excel(file_path, sheet_name=None)  # Load all sheets\n",
    "json_data = {sheet_name: data.to_dict(orient=\"records\") for sheet_name, data in excel_data.items()}\n",
    "\n",
    "# Save the JSON data to file\n",
    "with open(file_path_json, \"w\") as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(f\"File '{file_path_json}' created with data from all sheets in JSON format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'dataset_init/dataset_v5.xlsx' created with sheets: 'Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg', and 'Comb-KMT-Tiny-Reg'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define constants for the number of rows to take from each sheet\n",
    "n_data_model_1 = 3\n",
    "n_data_model_2 = 2\n",
    "\n",
    "# Define file paths\n",
    "output_file = \"dataset/one_hot_encoded_data.xlsx\"\n",
    "file_path = \"dataset_init/dataset_v5.xlsx\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "# Load the input and target data sheets from the output file\n",
    "input_data = pd.read_excel(output_file, sheet_name=\"OneHotEncodedDataInput\")\n",
    "target_data = pd.read_excel(output_file, sheet_name=\"OneHotEncodedDataTarget\")\n",
    "\n",
    "# Create datasets for each required sheet in the new file\n",
    "# sheet1_data = pd.concat([input_data.iloc[:n_data_model_1, :], target_data.iloc[:n_data_model_1, :]], axis=0, ignore_index=True)\n",
    "# sheet2_data = pd.concat([input_data.iloc[-n_data_model_2:, :], target_data.iloc[-n_data_model_2:, :]], axis=0, ignore_index=True)\n",
    "# combined_data = pd.concat([input_data, target_data], axis=0, ignore_index=True)\n",
    "\n",
    "# Create datasets for each required sheet by concatenating input and target side-by-side\n",
    "sheet1_data = pd.concat([input_data.iloc[:n_data_model_1, :], target_data.iloc[:n_data_model_1, :]], axis=1)\n",
    "sheet2_data = pd.concat([input_data.iloc[-n_data_model_2:, :], target_data.iloc[-n_data_model_2:, :]], axis=1)\n",
    "combined_data = pd.concat([input_data, target_data], axis=1)\n",
    "\n",
    "# Save these datasets into a new Excel file with the specified sheet names\n",
    "with pd.ExcelWriter(file_path, engine=\"openpyxl\") as writer:\n",
    "    sheet1_data.to_excel(writer, sheet_name=\"Sheet1-KM-SAR-Tiny-Reg\", index=False)\n",
    "    sheet2_data.to_excel(writer, sheet_name=\"Sheet2-M-SAK-T-Tiny-Reg\", index=False)\n",
    "    combined_data.to_excel(writer, sheet_name=\"Comb-KMT-Tiny-Reg\", index=False)\n",
    "\n",
    "print(f\"File '{file_path}' created with sheets: 'Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg', and 'Comb-KMT-Tiny-Reg'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAML and Reptile Alg. for Regression - Deep ELM - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.]\n",
      " [ 0.]\n",
      " [ 5.]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "x_all = np.linspace(-5, 5, 3)[:, None]  # All of the x points\n",
    "print(x_all)\n",
    "print(x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "idx_x_all = np.arange(0,5)\n",
    "print(idx_x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.amin()\n",
    "np.amin((n_sample, n_data_all), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_test(x_all_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "def get_data_test_v2(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Konversi id_test_data ke array 1D dengan tipe integer\n",
    "    if isinstance(id_test_data, (list, np.ndarray)):\n",
    "        id_test_data = np.array(id_test_data).flatten().astype(int)  # Konversi ke array 1D integer\n",
    "\n",
    "    # Ambil data dari indeks tertentu atau banyak indeks\n",
    "    data_test_rows = df.iloc[id_test_data, :n_input]\n",
    "    data_test = data_test_rows.values.tolist()  # Mengembalikan sebagai list of lists\n",
    "    \n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]]\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "(5, 14)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "n_data_all = 5\n",
    "n_sample = n_data_all # minimum 1, maks = n_data_all\n",
    "idx_x_all = np.arange(0,n_data_all)[:,None]\n",
    "# x_all_init = np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None]  # All of the x points\n",
    "x_all = get_data_test(np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None])\n",
    "n_train = 3\n",
    "\n",
    "# print(x_all_init)\n",
    "print()\n",
    "print(x_all)\n",
    "print()\n",
    "print(idx_x_all)\n",
    "print(np.array(x_all).shape)\n",
    "print(idx_x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_all = [\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
    "]\n",
    "\n",
    "# Indeks yang ingin diambil\n",
    "indices = rng.choice(len(x_all), size=n_train)\n",
    "\n",
    "# Mengambil subset\n",
    "subset = [x_all[i] for i in indices]\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(np.array(subset).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "[[0 0 1 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Choose a fixed task and minibatch for visualization\n",
    "f_plot = gen_task_eg_maml_base_val_data()\n",
    "\n",
    "# Indeks yang ingin diambil\n",
    "indices = rng.choice(len(x_all), size=n_train)\n",
    "\n",
    "# Mengambil subset\n",
    "xtrain_plot = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(xtrain_plot)\n",
    "# xtrain_plot = x_all[rng.choice(len(x_all), size=n_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xtrain_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function gen_task.<locals>.<lambda> at 0x7fa929bb7790>\n"
     ]
    }
   ],
   "source": [
    "f_randomsine = gen_task()\n",
    "print(f_randomsine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.52061968]\n",
      " [-2.66772754]\n",
      " [-2.362139  ]\n",
      " [ 0.11518924]\n",
      " [ 2.48661303]]\n"
     ]
    }
   ],
   "source": [
    "f = gen_task()\n",
    "y_all = f(x_all)\n",
    "print(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "<function gen_task_eg_maml_base_val_data.<locals>.<lambda> at 0x7fa919177430>\n"
     ]
    }
   ],
   "source": [
    "f_randoms_eg_maml_base_val_data = gen_task_eg_maml_base_val_data()\n",
    "print(f_randoms_eg_maml_base_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_eg_maml2 = f_randoms_eg_maml_base_val_data(x_all)\n",
    "print(y_all_eg_maml2)\n",
    "print(y_all_eg_maml2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "<function gen_task_eg_maml_base_idx_data.<locals>.<lambda> at 0x7fa929b218b0>\n"
     ]
    }
   ],
   "source": [
    "f_randoms_eg_maml_base_idx_data = gen_task_eg_maml_base_idx_data()\n",
    "print(f_randoms_eg_maml_base_idx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_eg_maml = f_randoms_eg_maml_base_idx_data(idx_x_all)\n",
    "print(y_all_eg_maml)\n",
    "print(y_all_eg_maml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n"
     ]
    }
   ],
   "source": [
    "# Generate task\n",
    "f = gen_task_eg_maml_base_val_data()\n",
    "y_all = f(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 44)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "# def gen_task_eg_maml(n_input, n_hidden1, n_hidden2, n_output, n_data_all, input_file_path, sheet_name):\n",
    "#     elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "#     load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "#     # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "#     f_randoms_eg_maml = lambda idx_x_all: np.array(\n",
    "#         [test_single_data_return_pred(elm_model, get_data_test(idx_x[0], n_input, input_file_path, sheet_name))\n",
    "#          for idx_x in idx_x_all]\n",
    "#     )\n",
    "    \n",
    "#     return f_randoms_eg_maml\n",
    "\n",
    "# def gen_task_eg_maml_base_idx_data():\n",
    "#     elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "#     load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "#     # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "#     f_randoms_eg_maml = lambda idx_x_all: np.array(\n",
    "#         [test_single_data_return_pred(elm_model, get_data_test(idx_x[0]))\n",
    "#          for idx_x in idx_x_all]\n",
    "#     )\n",
    "    \n",
    "#     return f_randoms_eg_maml\n",
    "\n",
    "def gen_task_eg_maml_base_idx_data():\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda idx_x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, get_data_test(idx_x_single[0]))\n",
    "             for idx_x_single in idx_x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "\n",
    "# def gen_task_eg_maml_base_val_data():\n",
    "#     elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "#     load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "#     # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "#     f_randoms_eg_maml = lambda x_all: np.array(\n",
    "#         [test_single_data_return_pred(elm_model, x[0]) for x in x_all]\n",
    "#     )\n",
    "    \n",
    "#     return f_randoms_eg_maml\n",
    "\n",
    "def gen_task_eg_maml_base_val_data():\n",
    "    elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "    load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "    # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "    f_randoms_eg_maml = lambda x: np.array(\n",
    "        [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "    )\n",
    "    \n",
    "    return f_randoms_eg_maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_x_all(x_all_in, x_all_to_search):\n",
    "    # Data utama yang akan dicari indeksnya\n",
    "    # x_all = [\n",
    "    #     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    #     [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "    #     [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    #     [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    #     [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
    "    # ]\n",
    "\n",
    "    # List untuk menyimpan indeks hasil\n",
    "    idx_result = []\n",
    "\n",
    "    # Cari indeks dari setiap elemen di x_all_in dalam x_all\n",
    "    for x_in in x_all_in:\n",
    "        if x_in in x_all_to_search:\n",
    "            idx_result.append(x_all_to_search.index(x_in))\n",
    "\n",
    "    return idx_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Contoh penggunaan\n",
    "x_all_in = [\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
    "]\n",
    "print(get_idx_x_all(x_all, x_all))  # Output: [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "def train_on_batch_eg_maml(x, y):\n",
    "    x = to_torch(x)\n",
    "    y = to_torch(y)\n",
    "\n",
    "    model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # elm_model_reptile.zero_grad()\n",
    "    # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "    # loss = (ypred - y).pow(2).mean()\n",
    "    # loss.backward()\n",
    "\n",
    "    # Forward and backward pass\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    ypred = model(x)\n",
    "    loss = criterion(ypred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.data -= inner_step_size * param.grad.data\n",
    "\n",
    "    # Save model checkpoint\n",
    "    filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "    save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "\n",
    "    # return loss.item()  # Optionally return the loss for monitoring\n",
    "    \n",
    "def save_model_reptile_checkpoint(model, filename):\n",
    "    # Dapatkan state_dict dari model\n",
    "    model_state = model.state_dict()\n",
    "\n",
    "    # Konversi tensor menjadi list untuk serialisasi JSON\n",
    "    model_state_serializable = {k: v.numpy().tolist() for k, v in model_state.items()}\n",
    "\n",
    "    # Simpan model ke file JSON\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(model_state_serializable, f)\n",
    "\n",
    "def load_model_reptile_checkpoint(model, filename):\n",
    "    # Muat model dari file JSON\n",
    "    with open(filename, 'r') as f:\n",
    "        model_state_serializable = json.load(f)\n",
    "\n",
    "    # Konversi kembali dari list ke tensor\n",
    "    model_state = {k: torch.tensor(np.array(v)) for k, v in model_state_serializable.items()}\n",
    "\n",
    "    # Memuat state_dict ke model\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()  # Set model ke mode evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil dimuat dari: model_reg_ckpt/model_reptile_checkpoint_31-10-2024-01-16-30.json\n",
      "Hasil prediksi: [[-0.01834962  0.0013385   0.08651553  0.24707103  0.1532599   0.09907089\n",
      "   0.05412611 -0.0422676   0.00641911 -0.03033484  0.13128929 -0.05562863\n",
      "  -0.09788775  0.19593163  0.32983837 -0.00145775  0.03513634 -0.04001291\n",
      "  -0.13261203 -0.18741111 -0.09910194  0.06469305 -0.06089606 -0.01480678\n",
      "   0.22528563  0.0186011  -0.10041821  0.1484561   0.08975185  0.26444772\n",
      "   0.11350157 -0.02509129 -0.13131104  0.21480459  0.13090637 -0.10971496\n",
      "  -0.02247317  0.06064396 -0.08210115 -0.02624491  0.30296224  0.02375437\n",
      "   0.01628181  0.0253423 ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual losses: tensor([0.0934, 0.0573], grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09342473],\n",
       "       [0.05727865]], dtype=float32)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse_or_loss_val([0,1]).data.numpy()[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07535"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.0934, 0.0573]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_x_all(x_all_in, x_all_to_search):\n",
    "    idx_result = []\n",
    "    \n",
    "    # Iterate through each element in x_all_in\n",
    "    for x_in in x_all_in:\n",
    "        # Iterate through x_all_to_search to find a matching element\n",
    "        for i, x in enumerate(x_all_to_search):\n",
    "            # Use np.array_equal to compare arrays element-wise\n",
    "            if np.array_equal(x_in, x):\n",
    "                idx_result.append(i)\n",
    "                break  # Stop after the first match is found\n",
    "    \n",
    "    return idx_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 0]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_idx_x_all(xtrain_plot, x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2045639455318451"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse_or_loss_val(get_idx_x_all(x_all, x_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_plot = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]), array([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]), array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(list(xtrain_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_or_loss_val(get_idx_x_all_in):\n",
    "    # print()\n",
    "    x = to_torch(np.array([x_all[i] for i in get_idx_x_all_in]))\n",
    "    y = to_torch(np.array([y_all[i] for i in get_idx_x_all_in]))\n",
    "\n",
    "    # cara 1\n",
    "    model.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # print(y_pred)\n",
    "    # print()\n",
    "    \n",
    "    individual_losses = (y_pred - y).pow(2).mean(dim=1)  # Loss for each sample along feature dimension\n",
    "    # print(\"Individual losses:\", individual_losses)\n",
    "\n",
    "    # loss = (y_pred - y).pow(2).mean()\n",
    "    # loss = (y_pred - y).pow(2).mean()\n",
    "    \n",
    "    # print(loss)\n",
    "    \n",
    "    # cara 2\n",
    "    # criterion = nn.MSELoss()\n",
    "    # model.zero_grad()\n",
    "    # ypred = model(x)\n",
    "    # loss = criterion(ypred, y)\n",
    "\n",
    "    return individual_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    x = to_torch(x)\n",
    "    return model(x).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EG-MAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device, Lib n Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_device support : mps\n",
      "set_device =  cpu\n",
      "\n",
      "Float64 support: True\n",
      "Float32 support: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from scipy.stats import ttest_ind, norm  # Add this line to import the norm distribution\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "my_device = torch.device('cuda' if torch.cuda.is_available() else 'mps' \\\n",
    "                      if torch.backends.mps.is_available() else 'cpu')\n",
    "print('my_device support :', my_device)\n",
    "\n",
    "#set params global\n",
    "# num_iterations_all = 10\n",
    "# pop_size_all = 3\n",
    "\n",
    "# \n",
    "num_iterations_all = 250\n",
    "pop_size_all = 30\n",
    "\n",
    "# set manual, misal ingin dgn cpu\n",
    "my_device = 'cpu'\n",
    "print('set_device = ', my_device)\n",
    "print()\n",
    "\n",
    "def check_dtype_support(dtype):\n",
    "    try:\n",
    "        torch.tensor(1, dtype=dtype, device=my_device)\n",
    "        # print('sukses bro')\n",
    "        return True\n",
    "    except TypeError:\n",
    "        # print('ada error bro')\n",
    "        return False\n",
    "    \n",
    "def decode_run_params_type1(run_params_tensor):\n",
    "    # Mendecode sesuai urutan tipe data yang dibutuhkan\n",
    "    decoded_params = [\n",
    "        int(run_params_tensor[0].item()),     # int\n",
    "        float(run_params_tensor[1].item()),   # float\n",
    "        int(run_params_tensor[2].item()),     # int\n",
    "        float(run_params_tensor[3].item()),   # float\n",
    "        float(run_params_tensor[4].item()),   # float\n",
    "        int(run_params_tensor[5].item())      # int\n",
    "    ]\n",
    "    return decoded_params\n",
    "\n",
    "# Contoh penggunaan\n",
    "# run_params_tensor = torch.tensor([1.6910, 0.0688, 3.2756, 0.0754, 0.0218, 5.0350])\n",
    "# decoded_params = decode_run_params_type1(run_params_tensor)\n",
    "\n",
    "# print(\"Decoded run_params type 1:\", decoded_params)\n",
    "\n",
    "def decode_run_params_type2(run_params_tensor): \n",
    "    decoded_params = [\n",
    "        int(run_params_tensor[0].item()),     # int\n",
    "        float(run_params_tensor[1].item()),   # float\n",
    "        int(run_params_tensor[2].item()),     # int\n",
    "        float(run_params_tensor[3].item()),   # float\n",
    "        float(run_params_tensor[4].item()),   # float\n",
    "        int(run_params_tensor[5].item()),     # int\n",
    "        \"E-MAML\" if float(run_params_tensor[-1].item()) <= 0.5 else \"E-MAML_Synthetic_E-Reptile\"  # kondisi if-else\n",
    "    ]\n",
    "    \n",
    "    return decoded_params\n",
    "\n",
    "# Contoh penggunaan\n",
    "# run_params_tensor = torch.tensor([1.6910, 0.0688, 3.2756, 0.0754, 0.0218, 5.0350, 0.5430])\n",
    "# decoded_params = decode_run_params_type2(run_params_tensor)\n",
    "\n",
    "# print(\"Decoded run_params type 2:\", decoded_params)\n",
    "\n",
    "def save_last_model_reptile_checkpoint(model, filename):\n",
    "    # Dapatkan state_dict dari model\n",
    "    model_state = model.state_dict()\n",
    "\n",
    "    # Konversi tensor menjadi list untuk serialisasi JSON\n",
    "    model_state_serializable = {k: v.numpy().tolist() for k, v in model_state.items()}\n",
    "\n",
    "    # Simpan model ke file JSON\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(model_state_serializable, f)\n",
    "\n",
    "def load_last_model_reptile_checkpoint(model, filename):\n",
    "    # Muat model dari file JSON\n",
    "    with open(filename, 'r') as f:\n",
    "        model_state_serializable = json.load(f)\n",
    "\n",
    "    # Konversi kembali dari list ke tensor\n",
    "    model_state = {k: torch.tensor(np.array(v)) for k, v in model_state_serializable.items()}\n",
    "\n",
    "    # Memuat state_dict ke model\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()  # Set model ke mode evaluasi\n",
    "    \n",
    "def save_last_info_params(\n",
    "        n_iterations, n_data_all, n_sample, n_train, seed, inner_step_size,\n",
    "        inner_epochs, outer_stepsize_reptile, outer_stepsize_maml,\n",
    "        run, final_lossval, filename_last_Model, path_last_Model\n",
    "    ):\n",
    "    # Construct the info_params dictionary\n",
    "    info_params = {\n",
    "        \"n_iterations\": n_iterations,\n",
    "        \"n_data_all\": n_data_all,\n",
    "        \"n_sample\": n_sample,\n",
    "        \"n_train\": n_train,\n",
    "        \"seed\": seed,\n",
    "        \"inner_step_size\": inner_step_size,\n",
    "        \"inner_epochs\": inner_epochs,\n",
    "        \"outer_stepsize_reptile\": outer_stepsize_reptile,\n",
    "        \"run\": run,\n",
    "        \"outer_stepsize_maml\": outer_stepsize_maml,\n",
    "        \"final_lossval\": float(final_lossval),\n",
    "        \"filename_last_Model\": filename_last_Model,\n",
    "        \"path_filename_last_Model\": path_last_Model\n",
    "    }\n",
    "\n",
    "    # Construct the filename with all the specified information\n",
    "    filename = (\n",
    "        f\"model_reg_last/model_params_last_{run}_{final_lossval:.3f}_\"\n",
    "        f\"{filename_last_Model}.json\"\n",
    "    )\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    # Save the info_params dictionary to the file as JSON\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(info_params, json_file, indent=4)\n",
    "\n",
    "    # print(f\"Parameters saved to {filename}\")\n",
    "    # return filename\n",
    "\n",
    "# Check support for float64\n",
    "supports_float64 = check_dtype_support(torch.float64)\n",
    "print(f\"Float64 support: {supports_float64}\")\n",
    "\n",
    "# Check support for float32\n",
    "supports_float32 = check_dtype_support(torch.float32)\n",
    "print(f\"Float32 support: {supports_float32}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def E-MAML dlm def experiment(run_params, last_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(run_params, last_generation=False):\n",
    "    # Mengambil parameter dari run_params\n",
    "    seed = int(run_params[0])  # Asumsikan run_params[0] adalah seed\n",
    "    inner_step_size = float(run_params[1])  # inner step size\n",
    "    inner_epochs = int(run_params[2])  # inner epochs\n",
    "    outer_stepsize_reptile = float(run_params[3])  # outer step size for reptile\n",
    "    outer_stepsize_maml = float(run_params[4])  # outer step size for MAML\n",
    "    n_iterations = int(run_params[5])  # number of outer updates\n",
    "    \n",
    "    run = \"E-MAML\" if float(run_params[-1]) <= 0.5 else \"E-MAML_Synthetic_E-Reptile\"  # kondisi if-else\n",
    "    \n",
    "    # print('Type of Meta-Learning:', run)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Define task distribution\n",
    "    n_data_all = 5\n",
    "    n_sample = n_data_all # minimum 1, maks = n_data_all\n",
    "    idx_x_all = np.arange(0,n_data_all)[:,None]\n",
    "    \n",
    "    # All of the x points data, dengan fitur input pepanjang n_input = 14\n",
    "    x_all = get_data_test(np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None])\n",
    "    # n_train = 10  # Size of training minibatches\n",
    "    n_train = 3  # Size of training minibatches, harus < n_data_all\n",
    "    \n",
    "    # info_params = f\"imax-{n_iterations}-ndata-{n_data_all}-nspl-{n_sample}-ntrain-{n_train}-s-{seed}-iss-{inner_step_size}-ie-{inner_epochs}-osr-{outer_stepsize_reptile}-osm-{outer_stepsize_maml}\"\n",
    "       \n",
    "    def get_mse_or_loss_val(get_idx_x_all_in):\n",
    "        x = to_torch(np.array([x_all[i] for i in get_idx_x_all_in]))\n",
    "        y = to_torch(np.array([y_all[i] for i in get_idx_x_all_in]))\n",
    "\n",
    "        # cara 1\n",
    "        model.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        individual_losses = (y_pred - y).pow(2).mean(dim=1)  # Loss for each sample along feature dimension\n",
    "        # print(\"Individual losses:\", individual_losses)\n",
    "\n",
    "        return individual_losses.data.numpy()\n",
    "    \n",
    "    def get_idx_x_all(x_all_in, x_all_to_search):\n",
    "        idx_result = []\n",
    "\n",
    "        # Iterate through each element in x_all_in\n",
    "        for x_in in x_all_in:\n",
    "            # Iterate through x_all_to_search to find a matching element\n",
    "            for i, x in enumerate(x_all_to_search):\n",
    "                # Use np.array_equal to compare arrays element-wise\n",
    "                if np.array_equal(x_in, x):\n",
    "                    idx_result.append(i)\n",
    "                    break  # Stop after the first match is found\n",
    "\n",
    "        return idx_result\n",
    "\n",
    "    def gen_task_eg_maml_base_idx_data():\n",
    "        # elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        # load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "        \n",
    "        elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda idx_x: np.array(\n",
    "            [test_single_data_return_pred(elm_model_n_hidden_layers, get_data_test(idx_x_single[0]))\n",
    "             for idx_x_single in idx_x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task_eg_maml_base_val_data():\n",
    "        # elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        # load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "        \n",
    "        elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task(): # sama dengan gen_task_eg_maml_base_val_data()\n",
    "        \n",
    "        elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "\n",
    "    # Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "    # ==========\n",
    "    # nn.Sequential: Mudah dan cocok untuk deep learning sederhana yang berurutan, \n",
    "    # tidak cocok jika arsitektur membutuhkan koneksi yang kompleks.\n",
    "    ## -------\n",
    "    # nn.Linear dalam Kelas nn.Module: Lebih fleksibel dan dapat digunakan untuk arsitektur kompleks, \n",
    "    # yang melibatkan banyak hidden layer, skip connections, atau jalur paralel.\n",
    "    \n",
    "    # Custom activation function NRReLU\n",
    "    def NRReLU(x):\n",
    "        return 1 / (torch.exp(-x) - torch.exp(x))\n",
    "\n",
    "    activations = [NRReLU, nn.Sigmoid(), nn.Tanh(), nn.ReLU()]  # Custom activations, including NRReLU\n",
    "    \n",
    "    # Define a function to create the model with configurable layers and activation functions\n",
    "    def define_model_type1(n_input, n_hidden_layers, n_output, activations=None):\n",
    "        layers = []\n",
    "        input_dim = n_input\n",
    "\n",
    "        # Ensure activations list matches the number of hidden layers, or use ReLU as default\n",
    "        if activations is None:\n",
    "            activations = [F.relu] * len(n_hidden_layers)  # Default to ReLU for all layers\n",
    "        elif len(activations) != len(n_hidden_layers):\n",
    "            raise ValueError(\"Length of activations must match number of hidden layers\")\n",
    "\n",
    "        # Add each hidden layer with the specified number of neurons and activation\n",
    "        for hidden_units, activation in zip(n_hidden_layers, activations):\n",
    "            layers.append(nn.Linear(input_dim, hidden_units))\n",
    "            input_dim = hidden_units  # Update input_dim for the next layer\n",
    "\n",
    "            # Add the activation layer as a callable function\n",
    "            layers.append(activation)  # Add the activation function directly\n",
    "\n",
    "        # Add the final output layer without activation\n",
    "        layers.append(nn.Linear(input_dim, n_output))\n",
    "\n",
    "        # Create the model with nn.Sequential\n",
    "        model = nn.Sequential(*layers)\n",
    "        return model\n",
    "    \n",
    "    def define_model_type2(n_input, n_hidden_layers, n_output):\n",
    "        #     model = nn.Sequential(\n",
    "        #         nn.Linear(1, 64),\n",
    "        #         nn.Tanh(),\n",
    "        #         nn.Linear(64, 64),\n",
    "        #         nn.Tanh(),\n",
    "        #         nn.Linear(64, 1),\n",
    "        #     )\n",
    "\n",
    "        layers = []\n",
    "        input_dim = n_input\n",
    "\n",
    "        # Add each hidden layer with alternating activation functions\n",
    "        for i, hidden_units in enumerate(n_hidden_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_units))\n",
    "\n",
    "            # Use ReLU for the first layer, Tanh for others\n",
    "            if i % 2 == 0:\n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                layers.append(nn.Tanh())\n",
    "\n",
    "            input_dim = hidden_units  # Update input_dim for the next layer\n",
    "\n",
    "        # Add the final output layer\n",
    "        layers.append(nn.Linear(input_dim, n_output))\n",
    "\n",
    "        # Create the model with nn.Sequential\n",
    "        model = nn.Sequential(*layers)\n",
    "        return model\n",
    "    \n",
    "    # Define sintesis model. Reptile dengan ELMRegressionForReptile - nn.Linear\n",
    "    model = ModelForSyntheticReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        \n",
    "    def save_model_reptile_checkpoint(model, filename):\n",
    "        # Dapatkan state_dict dari model\n",
    "        model_state = model.state_dict()\n",
    "\n",
    "        # Konversi tensor menjadi list untuk serialisasi JSON\n",
    "        model_state_serializable = {k: v.numpy().tolist() for k, v in model_state.items()}\n",
    "\n",
    "        # Simpan model ke file JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(model_state_serializable, f)\n",
    "\n",
    "    def load_model_reptile_checkpoint(model, filename):\n",
    "        # Muat model dari file JSON\n",
    "        with open(filename, 'r') as f:\n",
    "            model_state_serializable = json.load(f)\n",
    "\n",
    "        # Konversi kembali dari list ke tensor\n",
    "        model_state = {k: torch.tensor(np.array(v)) for k, v in model_state_serializable.items()}\n",
    "\n",
    "        # Memuat state_dict ke model\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()  # Set model ke mode evaluasi\n",
    "\n",
    "    def to_torch(x):\n",
    "        return ag.Variable(torch.Tensor(x))\n",
    "\n",
    "    # def train_on_batch(x, y):\n",
    "    #     x = to_torch(x)\n",
    "    #     y = to_torch(y)\n",
    "    #     model.zero_grad()\n",
    "    #     ypred = model(x)\n",
    "    #     loss = (ypred - y).pow(2).mean()\n",
    "    #     loss.backward()\n",
    "    #     for param in model.parameters():\n",
    "    #         param.data -= inner_step_size * param.grad.data\n",
    "    \n",
    "               \n",
    "    # using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "    def train_on_batch_eg_maml(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        # filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        # save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "            \n",
    "    def train_on_batch(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        # filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        # save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "        \n",
    "    # Cara Memuat model dari checkpoint\n",
    "    #     try:\n",
    "    #         load_model_reptile_checkpoint(model, filename_ckpt)\n",
    "    #         print(\"Model berhasil dimuat dari:\", filename_ckpt)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Terjadi kesalahan saat memuat model:\", e)\n",
    "\n",
    "    #     # Sekarang Anda bisa menggunakan model untuk melakukan prediksi atau melanjutkan pelatihan\n",
    "    #     # Contoh prediksi\n",
    "    #     x_test = to_torch([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]])  # Ganti dengan data yang sesuai\n",
    "    #     model.eval()  # Set model ke mode evaluasi\n",
    "    #     with torch.no_grad():\n",
    "    #         prediction = model(x_test)\n",
    "    #         print(\"Hasil prediksi:\", prediction.numpy())\n",
    "\n",
    "    def predict(x):\n",
    "        x = to_torch(x)\n",
    "        return model(x).data.numpy()\n",
    "    \n",
    "    #     def save_info_params(\n",
    "    #         n_iterations, n_data_all, n_sample, n_train, seed, inner_step_size,\n",
    "    #         inner_epochs, outer_stepsize_reptile, outer_stepsize_maml,\n",
    "    #         run, final_lossval, filename_last_Model\n",
    "    #     ):\n",
    "    #         # Construct the info_params dictionary\n",
    "    #         info_params = {\n",
    "    #             \"imax\": n_iterations,\n",
    "    #             \"ndata\": n_data_all,\n",
    "    #             \"nspl\": n_sample,\n",
    "    #             \"ntrain\": n_train,\n",
    "    #             \"s\": seed,\n",
    "    #             \"iss\": inner_step_size,\n",
    "    #             \"ie\": inner_epochs,\n",
    "    #             \"osr\": outer_stepsize_reptile,\n",
    "    #             \"run\": run,\n",
    "    #             \"osm\": outer_stepsize_maml,\n",
    "    #             \"final_lossval\": float(final_lossval)\n",
    "    #         }\n",
    "\n",
    "    #         # Construct the filename with all the specified information\n",
    "    #         filename = (\n",
    "    #             f\"model_reg_last/model_params_last_{run}_{final_lossval:.3f}_\"\n",
    "    #             f\"{filename_last_Model}.json\"\n",
    "    #         )\n",
    "\n",
    "    #         # Ensure the directory exists\n",
    "    #         os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    #         # Save the info_params dictionary to the file as JSON\n",
    "    #         with open(filename, 'w') as json_file:\n",
    "    #             json.dump(info_params, json_file, indent=4)\n",
    "\n",
    "        # print(f\"Parameters saved to {filename}\")\n",
    "        # return filename\n",
    "\n",
    "    # Choose a fixed task and minibatch for visualization\n",
    "    f_plot = gen_task()\n",
    "    # xtrain_plot = x_all[rng.choice(len(x_all), size=n_train)]\n",
    "    xtrain_plot = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])\n",
    "\n",
    "    # plt.cla()\n",
    "    # Set figure and axis properties\n",
    "    # plt.figure()\n",
    "    \n",
    "    # Set gray background color\n",
    "    # plt.gca().set_facecolor('#f0f0f0')  # Light gray color for the background\n",
    "    # plt.gca().set_facecolor('lightgray')\n",
    "    \n",
    "    # Add grid lines\n",
    "    # plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    filename_first_n_last_Loss = datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    for iteration in range(n_iterations):\n",
    "        weights_before = deepcopy(model.state_dict())\n",
    "\n",
    "        # Generate task\n",
    "        f = gen_task()\n",
    "        y_all = f(x_all)\n",
    "\n",
    "        # Do SGD on this task\n",
    "        inds = rng.permutation(len(x_all))\n",
    "        train_ind = inds[:-1 * n_train]\n",
    "        val_ind = inds[-1 * n_train:]       # Val contains 1/5th of the gt model (com. model)\n",
    "\n",
    "        for _ in range(inner_epochs):\n",
    "            for start in range(0, len(train_ind), n_train):\n",
    "                mbinds = train_ind[start:start + n_train]\n",
    "                # print('mbinds =', mbinds)\n",
    "                # print()\n",
    "                # print('x_all[mbinds] =', x_all[mbinds])\n",
    "                # print()\n",
    "                # print('y_all[mbinds] =', y_all[mbinds])\n",
    "                x_all_mbinds = np.array([x_all[i] for i in mbinds])\n",
    "                y_all_mbinds = np.array([y_all[i] for i in mbinds])\n",
    "                # train_on_batch(x_all[mbinds], y_all[mbinds])\n",
    "                train_on_batch(x_all_mbinds, y_all_mbinds)\n",
    "                \n",
    "                # print('=======================')\n",
    "\n",
    "        if run == 'E-MAML':\n",
    "            outer_step_size = outer_stepsize_maml * (1 - iteration / n_iterations)  # linear schedule\n",
    "            for start in range(0, len(val_ind), n_train):\n",
    "                dpinds = val_ind[start:start + n_train]\n",
    "                # print('dpinds =', dpinds)\n",
    "                # print()\n",
    "                \n",
    "                # x = to_torch(x_all[dpinds])\n",
    "                x = to_torch(np.array([x_all[i] for i in dpinds]))\n",
    "                \n",
    "                # y = to_torch(y_all[dpinds])\n",
    "                y = to_torch(np.array([y_all[i] for i in dpinds]))\n",
    "\n",
    "                # Compute the grads\n",
    "                model.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = (y_pred - y).pow(2).mean()\n",
    "                loss.backward()\n",
    "                \n",
    "\n",
    "                # Reload the model\n",
    "                model.load_state_dict(weights_before)\n",
    "\n",
    "                # SGD on the params\n",
    "                for param in model.parameters():\n",
    "                    param.data -= outer_step_size * param.grad.data\n",
    "            # print(weights_before)\n",
    "        else:\n",
    "            # Interpolate between current weights and trained weights from this task\n",
    "            # I.e. (weights_before - weights_after) is the meta-gradient\n",
    "            weights_after = model.state_dict()\n",
    "            outerstepsize = outer_stepsize_reptile * (1 - iteration / n_iterations)  # linear schedule\n",
    "            model.load_state_dict({name: weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize\n",
    "                                   for name in weights_before})\n",
    "\n",
    "        # Periodically plot the results on a particular task and minibatch\n",
    "        # if (plot and ((iteration == 0) or ((iteration + 1) % 1000 == 0))):\n",
    "#         if (plot and ((iteration == 0) or ((iteration + 1) % n_iterations == 0))):\n",
    "#             plt.cla()\n",
    "#             # plt.cla()\n",
    "            \n",
    "#             # Set gray background color\n",
    "#             # plt.gca().set_facecolor('#f0f0f0')  # Light gray color for the background\n",
    "            \n",
    "#             # Set gray background color\n",
    "#             # plt.gca().set_facecolor('#f0f0f0')  # Light gray color for the background\n",
    "#             plt.gca().set_facecolor('lightgray')\n",
    "\n",
    "#             # Add grid lines\n",
    "#             plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "            \n",
    "#             f = f_plot\n",
    "#             weights_before = deepcopy(model.state_dict())  # save snapshot before evaluation\n",
    "            \n",
    "#             # plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0, 0, 1))\n",
    "#             get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "#             get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "#             plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"pred after 0\", color=(0, 0, 1))\n",
    "            \n",
    "#             for inneriter in range(32):\n",
    "#                 train_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "#                 if (inneriter + 1) % 8 == 0:\n",
    "#                     frac = (inneriter + 1) / 32\n",
    "#                     # plt.plot(x_all, predict(x_all), label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "                    \n",
    "#                     get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "#                     get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "#                     plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "            \n",
    "#             # plt.plot(x_all, f(x_all), label=\"true\", color=(0, 1, 0))\n",
    "#             # plt.plot(x_all, f(x_all), label=\"ground truth from sin(x)\", color=(0, 1, 0))\n",
    "            \n",
    "#             get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "#             get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "#             plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"ground truth from comb. model\", color=(0, 1, 0))\n",
    "            \n",
    "            \n",
    "#             lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "#             # plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "            \n",
    "#             # print(\"xtrain_plot: \",xtrain_plot)\n",
    "            \n",
    "#             get_idx_x_all_to_2d_plot = get_idx_x_all(xtrain_plot, x_all) # agar dapat diplot pd 2D\n",
    "#             get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all_to_2d_plot)\n",
    "            \n",
    "#             # print(\"idx xtrain_plot: \",get_idx_x_all_to_2d_plot)\n",
    "        \n",
    "            \n",
    "#             plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, \"x\", label=\"train\", color=\"k\")\n",
    "            \n",
    "#             plt.ylim(-4, 4)\n",
    "#             plt.xlim(0, 4)  # Set x-axis limits\n",
    "#             plt.xticks(range(5))  # Set x-ticks to show 0, 1, 2, 3, 4\n",
    "#             plt.xlabel(\"index of data\")  # Label for x-axis\n",
    "#             plt.ylabel(\"loss value\")     # Label for y-axis\n",
    "#             plt.legend(loc=\"lower right\")\n",
    "            \n",
    "#             plt.savefig(f\"loss_e_maml/plot_{run}_{lossval:.3f}_iter_{iteration}_{filename_first_n_last_Loss}.png\")\n",
    "#             plt.savefig(f\"loss_e_maml/plot_{run}_{lossval:.3f}_iter_{iteration}_{filename_first_n_last_Loss}.pdf\")\n",
    "\n",
    "            \n",
    "#             plt.pause(0.01)\n",
    "#             model.load_state_dict(weights_before)  # restore from snapshot\n",
    "#             print(f\"-----------------------------\")\n",
    "#             print(f\"iteration               {iteration + 1}\")\n",
    "#             print(f\"loss on plotted curve   {lossval:.3f}\")  # would be better to average loss over a set of examples, but this is optimized for brevity\n",
    "\n",
    "    \n",
    "    # print()\n",
    "    final_lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "    # print(f\"final loss on last model = {final_lossval:.3f}\") \n",
    "    filename_last_Model = datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    \n",
    "    # Save last loss\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml_{filename_last_Model_n_Loss}.png\")\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml{filename_last_Model_n_Loss}.pdf\")\n",
    "    \n",
    "    # Save the plot as PNG and PDF only after plotting\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml_{filename_last_Model_n_Loss}.png\")\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml_{filename_last_Model_n_Loss}.pdf\")\n",
    "\n",
    "    # Optionally, show the plot if you want to display it interactively\n",
    "    # plt.show()  # Use this only if you want to display the plot interactively\n",
    "    \n",
    "    \n",
    "    # Save last model checkpoint\n",
    "    # path_last_Model = f'model_reg_last/model_last_{run}_{final_lossval:.3f}_{filename_last_Model}.json'\n",
    "    path_last_Model = f'model_reg_last/model_last_{run}_{final_lossval:.3f}_{filename_last_Model}'\n",
    "    # save_model_reptile_checkpoint(model, path_last_Model)\n",
    "    # save_info_params(\n",
    "    #     n_iterations, n_data_all, n_sample, n_train, seed, inner_step_size,\n",
    "    #     inner_epochs, outer_stepsize_reptile, outer_stepsize_maml,\n",
    "    #     run, final_lossval, filename_last_Model)\n",
    "    \n",
    "    config = {\n",
    "        \"n_iterations\": n_iterations,\n",
    "        \"n_data_all\": n_data_all,\n",
    "        \"n_sample\": n_sample,\n",
    "        \"n_train\": n_train,\n",
    "        \"seed\": seed,\n",
    "        \"inner_step_size\": inner_step_size,\n",
    "        \"inner_epochs\": inner_epochs,\n",
    "        \"outer_stepsize_reptile\": outer_stepsize_reptile,\n",
    "        \"outer_stepsize_maml\": outer_stepsize_maml,\n",
    "        \"run\": run,\n",
    "        \"final_lossval\": final_lossval,\n",
    "        \"filename_last_Model\": filename_last_Model,\n",
    "        \"path_filename_last_Model\": path_last_Model\n",
    "        \n",
    "    }\n",
    "    if(last_generation==True):\n",
    "        return final_lossval, path_last_Model, model, config\n",
    "    else:\n",
    "        return final_lossval  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fitness_ind_eg_maml_tp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2407, 0.2413, 0.2461, 0.2211, 0.2313])"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_ind_eg_maml_tp_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_ind_eg_maml_tp_1(x=None, last_generation=False):\n",
    "    # Tentukan batas untuk variabel\n",
    "    bounds_tp_1 = [\n",
    "        (0, 4),  # Bound untuk seed (misalnya dari 0 sampai 100)\n",
    "        (0.01, 0.1),  # Bound untuk inner_step_size (misalnya dari 0.01 sampai 0.1)\n",
    "        (1, 7),  # Bound untuk inner_epochs (misalnya dari 1 sampai 10)\n",
    "        (0.01, 0.2),  # Bound untuk outer_stepsize_reptile (misalnya dari 0.01 sampai 0.2)\n",
    "        (0.001, 0.05),  # Bound untuk outer_stepsize_maml (misalnya dari 0.001 sampai 0.05)\n",
    "        (1, 10),  # Bound untuk n_iterations (misalnya dari 1 sampai 50)\n",
    "        (0, 1)  # Bound untuk run type (misalnya dari 0 sampai 1) \"E-MAML\" if <= 0.5 else \"E-MAML_Synthetic_E-Reptile\"\n",
    "    ]\n",
    "    \n",
    "    def calculate_fitness(run_params_in, last_generation_in=False):\n",
    "        # Asumsikan 'run' adalah tensor yang berisi parameter individu\n",
    "        # Konversi 'run' ke format yang sesuai untuk experiment\n",
    "        # Misalnya, kita dapat mengonversi tensor menjadi list atau numpy array, tergantung implementasi experiment\n",
    "        run_params = run_params_in.detach().cpu().numpy().tolist()  # Jika menggunakan PyTorch, ambil data ke CPU dan konversi ke list\n",
    "\n",
    "        # Memanggil fungsi experiment dengan parameter yang sesuai\n",
    "        # fitness_value = experiment(run_params)  # plot=False untuk tidak menampilkan plot\n",
    "        # fitness_value, path_last_Model, model, config = experiment(run_params)  # plot=False untuk tidak menampilkan plot\n",
    "                \n",
    "        if(last_generation_in == True):\n",
    "            fitness_value, path_last_Model, model, config = experiment(run_params, True)  # plot=False untuk tidak menampilkan plot\n",
    "            return fitness_value, path_last_Model, model, config\n",
    "        else:\n",
    "            # print()\n",
    "            fitness_value = experiment(run_params)\n",
    "            return fitness_value\n",
    "\n",
    "        # Mengembalikan nilai fitness\n",
    "        # return fitness_value\n",
    "        # return fitness_value, path_last_Model, model, config\n",
    "\n",
    "    # Jika x tidak diberikan, inisialisasi dengan nilai acak\n",
    "    if x is None:\n",
    "        pop_size_all = 5\n",
    "        \n",
    "        dim = len(bounds_tp_1)\n",
    "        num_ind = pop_size_all\n",
    "        \n",
    "        x = torch.rand((num_ind, dim), device=my_device) * \\\n",
    "            (torch.tensor(bounds_tp_1, device=my_device)[:, 1] - \\\n",
    "             torch.tensor(bounds_tp_1, device=my_device)[:, 0]) + \\\n",
    "            torch.tensor(bounds_tp_1, device=my_device)[:, 0]\n",
    "        \n",
    "        # print(x)\n",
    "    else:\n",
    "        # Menyesuaikan tipe data tensor x\n",
    "        if check_dtype_support(torch.float64):\n",
    "            x = x.to(torch.float64)\n",
    "            # print('utilize torch.float64')\n",
    "        else:\n",
    "            x = x.to(torch.float32)\n",
    "            # print('utilize torch.float32')\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Mendapatkan ukuran populasi\n",
    "    get_pop_size = x.shape[0]\n",
    "    \n",
    "    fitness_all = []  # tampung semua nilai fitness dari individu\n",
    "    \n",
    "    # Inisialisasi variabel sementara\n",
    "    temp_path_last_Model = None\n",
    "    temp_model = None\n",
    "    temp_config = None\n",
    "    temp_i_ind = None\n",
    "    \n",
    "    for i_ind in range(get_pop_size):\n",
    "        # print(x[i_ind])\n",
    "        # Hitung fitness untuk individu ke-i\n",
    "        # fitness = calculate_fitness(x[i_ind])  # Misalkan ada fungsi calculate_fitness\n",
    "        # fitness, path_last_Model, model, config = calculate_fitness(x[i_ind])  # Misalkan ada fungsi calculate_fitness\n",
    "        \n",
    "        if(last_generation==True):\n",
    "            fitness, path_last_Model, model, config = calculate_fitness(x[i_ind], True)  # Misalkan ada fungsi calculate_fitness\n",
    "            \n",
    "            if i_ind == 0:\n",
    "                # Simpan path_last_Model, model, dan config pada variabel sementara untuk pertama kali\n",
    "                temp_path_last_Model = path_last_Model\n",
    "                temp_model = model\n",
    "                temp_config = config\n",
    "                temp_i_ind = i_ind\n",
    "            else:\n",
    "                # Jika fitness dari individu ke-i lebih kecil, replace variabel sementara\n",
    "                if fitness < fitness_all[i_ind - 1]:\n",
    "                    temp_path_last_Model = path_last_Model\n",
    "                    temp_model = model\n",
    "                    temp_config = config\n",
    "                    temp_i_ind = i_ind\n",
    "        else:\n",
    "            fitness = calculate_fitness(x[i_ind])  # Misalkan ada fungsi calculate_fitness\n",
    "            \n",
    "        fitness_all.append(fitness)\n",
    "        \n",
    "        # if i_ind == 0:\n",
    "        #     # Simpan path_last_Model, model, dan config pada variabel sementara untuk pertama kali\n",
    "        #     temp_path_last_Model = path_last_Model\n",
    "        #     temp_model = model\n",
    "        #     temp_config = config\n",
    "        #     temp_i_ind = i_ind\n",
    "        # else:\n",
    "        #     # Jika fitness dari individu ke-i lebih kecil, replace variabel sementara\n",
    "        #     if fitness < fitness_all[i_ind - 1]:\n",
    "        #         temp_path_last_Model = path_last_Model\n",
    "        #         temp_model = model\n",
    "        #         temp_config = config\n",
    "        #         temp_i_ind = i_ind\n",
    "\n",
    "    # return torch.tensor(fitness_all, device=my_device)\n",
    "    # return torch.tensor(fitness_all, device=my_device), temp_path_last_Model, temp_model, temp_config, temp_i_ind\n",
    "    if(last_generation==True):\n",
    "        return torch.tensor(fitness_all, device=my_device), temp_path_last_Model, temp_model, temp_config, temp_i_ind\n",
    "    else:\n",
    "        return torch.tensor(fitness_all, device=my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fitness_ind_eg_maml_tp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_ind_eg_maml_tp_2(x=None, last_generation=False):\n",
    "    # Tentukan batas untuk variabel\n",
    "    bounds_tp_2 = [\n",
    "        (0, 8),  # Bound untuk seed (misalnya dari 0 sampai 100)\n",
    "        (0.001, 0.2),  # Bound untuk inner_step_size (misalnya dari 0.01 sampai 0.1)\n",
    "        (1, 14),  # Bound untuk inner_epochs (misalnya dari 1 sampai 10)\n",
    "        (0.001, 0.4),  # Bound untuk outer_stepsize_reptile (misalnya dari 0.01 sampai 0.2)\n",
    "        (0.0001, 0.1),  # Bound untuk outer_stepsize_maml (misalnya dari 0.001 sampai 0.05)\n",
    "        (1, 20),  # Bound untuk n_iterations (misalnya dari 1 sampai 50)\n",
    "        (0, 1)  # Bound untuk run type (misalnya dari 0 sampai 1) \"E-MAML\" if <= 0.5 else \"E-MAML_Synthetic_E-Reptile\"\n",
    "    ]\n",
    "    \n",
    "    def calculate_fitness(run_params_in, last_generation_in=False):\n",
    "        # Asumsikan 'run' adalah tensor yang berisi parameter individu\n",
    "        # Konversi 'run' ke format yang sesuai untuk experiment\n",
    "        # Misalnya, kita dapat mengonversi tensor menjadi list atau numpy array, tergantung implementasi experiment\n",
    "        run_params = run_params_in.detach().cpu().numpy().tolist()  # Jika menggunakan PyTorch, ambil data ke CPU dan konversi ke list\n",
    "\n",
    "        # Memanggil fungsi experiment dengan parameter yang sesuai\n",
    "        # fitness_value = experiment(run_params)  # plot=False untuk tidak menampilkan plot\n",
    "        # fitness_value, path_last_Model, model, config = experiment(run_params)  # plot=False untuk tidak menampilkan plot\n",
    "                \n",
    "        if(last_generation_in == True):\n",
    "            fitness_value, path_last_Model, model, config = experiment(run_params, True)  # plot=False untuk tidak menampilkan plot\n",
    "            return fitness_value, path_last_Model, model, config\n",
    "        else:\n",
    "            # print()\n",
    "            fitness_value = experiment(run_params)\n",
    "            return fitness_value\n",
    "\n",
    "        # Mengembalikan nilai fitness\n",
    "        # return fitness_value\n",
    "        # return fitness_value, path_last_Model, model, config\n",
    "\n",
    "    # Jika x tidak diberikan, inisialisasi dengan nilai acak\n",
    "    if x is None:\n",
    "        pop_size_all = 5\n",
    "        \n",
    "        dim = len(bounds_tp_2)\n",
    "        num_ind = pop_size_all\n",
    "        \n",
    "        x = torch.rand((num_ind, dim), device=my_device) * \\\n",
    "            (torch.tensor(bounds_tp_2, device=my_device)[:, 1] - \\\n",
    "             torch.tensor(bounds_tp_2, device=my_device)[:, 0]) + \\\n",
    "            torch.tensor(bounds_tp_2, device=my_device)[:, 0]\n",
    "        \n",
    "        # print(x)\n",
    "    else:\n",
    "        # Menyesuaikan tipe data tensor x\n",
    "        if check_dtype_support(torch.float64):\n",
    "            x = x.to(torch.float64)\n",
    "            # print('utilize torch.float64')\n",
    "        else:\n",
    "            x = x.to(torch.float32)\n",
    "            # print('utilize torch.float32')\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Mendapatkan ukuran populasi\n",
    "    get_pop_size = x.shape[0]\n",
    "    \n",
    "    fitness_all = []  # tampung semua nilai fitness dari individu\n",
    "    \n",
    "    # Inisialisasi variabel sementara\n",
    "    temp_path_last_Model = None\n",
    "    temp_model = None\n",
    "    temp_config = None\n",
    "    temp_i_ind = None\n",
    "    \n",
    "    for i_ind in range(get_pop_size):\n",
    "        # print(x[i_ind])\n",
    "        # Hitung fitness untuk individu ke-i\n",
    "        # fitness = calculate_fitness(x[i_ind])  # Misalkan ada fungsi calculate_fitness\n",
    "        # fitness, path_last_Model, model, config = calculate_fitness(x[i_ind])  # Misalkan ada fungsi calculate_fitness\n",
    "        \n",
    "        if(last_generation==True):\n",
    "            fitness, path_last_Model, model, config = calculate_fitness(x[i_ind], True)  # Misalkan ada fungsi calculate_fitness\n",
    "            \n",
    "            if i_ind == 0:\n",
    "                # Simpan path_last_Model, model, dan config pada variabel sementara untuk pertama kali\n",
    "                temp_path_last_Model = path_last_Model\n",
    "                temp_model = model\n",
    "                temp_config = config\n",
    "                temp_i_ind = i_ind\n",
    "            else:\n",
    "                # Jika fitness dari individu ke-i lebih kecil, replace variabel sementara\n",
    "                if fitness < fitness_all[i_ind - 1]:\n",
    "                    temp_path_last_Model = path_last_Model\n",
    "                    temp_model = model\n",
    "                    temp_config = config\n",
    "                    temp_i_ind = i_ind\n",
    "        else:\n",
    "            fitness = calculate_fitness(x[i_ind])  # Misalkan ada fungsi calculate_fitness\n",
    "            \n",
    "        fitness_all.append(fitness)\n",
    "        \n",
    "        # if i_ind == 0:\n",
    "        #     # Simpan path_last_Model, model, dan config pada variabel sementara untuk pertama kali\n",
    "        #     temp_path_last_Model = path_last_Model\n",
    "        #     temp_model = model\n",
    "        #     temp_config = config\n",
    "        #     temp_i_ind = i_ind\n",
    "        # else:\n",
    "        #     # Jika fitness dari individu ke-i lebih kecil, replace variabel sementara\n",
    "        #     if fitness < fitness_all[i_ind - 1]:\n",
    "        #         temp_path_last_Model = path_last_Model\n",
    "        #         temp_model = model\n",
    "        #         temp_config = config\n",
    "        #         temp_i_ind = i_ind\n",
    "\n",
    "    # return torch.tensor(fitness_all, device=my_device)\n",
    "    # return torch.tensor(fitness_all, device=my_device), temp_path_last_Model, temp_model, temp_config, temp_i_ind\n",
    "    if(last_generation==True):\n",
    "        return torch.tensor(fitness_all, device=my_device), temp_path_last_Model, temp_model, temp_config, temp_i_ind\n",
    "    else:\n",
    "        return torch.tensor(fitness_all, device=my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Optimizer Fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(objective_func, bounds, num_generations=num_iterations_all, population_size=pop_size_all, crossover_prob=0.7, mutation_prob=0.1, elite_percentage=1., full_logger=1):\n",
    "    # Inisialisasi populasi awal dengan torch\n",
    "    bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "    # population = torch.rand(population_size, len(bounds)).to(bounds_tensor.device) * (bounds_tensor[:, 1] - bounds_tensor[:, 0]) + bounds_tensor[:, 0]\n",
    "    \n",
    "    dim = len(bounds)\n",
    "    \n",
    "    # Initialize population within the specified bounds\n",
    "    population = torch.rand((population_size, dim), device=my_device) * \\\n",
    "    (torch.tensor(bounds, device=my_device)[:, 1] - \\\n",
    "     torch.tensor(bounds, device=my_device)[:, 0]) + \\\n",
    "    torch.tensor(bounds, device=my_device)[:, 0]\n",
    "    \n",
    "    # print(population)\n",
    "    \n",
    "    # Inisialisasi variabel sementara\n",
    "    temp_path_last_Model_at_ga = None\n",
    "    temp_model_at_ga = None\n",
    "    temp_config_at_ga = None\n",
    "    temp_i_ind_at_ga = None\n",
    "    temp_best_fitness_in_ga = None\n",
    "    last_generation = False\n",
    "    \n",
    "    # Initialize untuk grafik konvergensi\n",
    "    max_gbest_each_iter = torch.empty(num_generations, device=my_device) # makna max disini tdk selalu nilai Max, tetapi krn loss, maka nilai yg paling min\n",
    "    mean_gbest_each_iter = torch.empty(num_generations, device=my_device)\n",
    "    \n",
    "    # Fungsi untuk mengevaluasi fitness\n",
    "    # def evaluate_fitness(pop):\n",
    "    #     return torch.tensor([objective_func(ind) for ind in pop], dtype=torch.float32)\n",
    "\n",
    "    # Melakukan proses GA\n",
    "    for generation in range(num_generations):\n",
    "        # print('generation -> ',generation)\n",
    "        \n",
    "        if(generation==(num_generations-1)):\n",
    "            last_generation = True\n",
    "        \n",
    "        # Langkah 1: Crossover untuk menghasilkan offspring\n",
    "        offspring_crossover = []\n",
    "        for _ in range(population_size):\n",
    "            parent1, parent2 = population[torch.randint(0, population_size, (1,)).item()], population[torch.randint(0, population_size, (1,)).item()]\n",
    "            if torch.rand(1).item() < crossover_prob:\n",
    "                crossover_point = torch.randint(1, len(bounds) - 1, (1,)).item()\n",
    "                child = torch.cat((parent1[:crossover_point], parent2[crossover_point:]))\n",
    "            else:\n",
    "                child = parent1  # Tidak crossover\n",
    "            offspring_crossover.append(child)\n",
    "        offspring_crossover = torch.stack(offspring_crossover)\n",
    "\n",
    "        # Langkah 2: Mutasi untuk menghasilkan offspring\n",
    "        offspring_mutation = []\n",
    "        for ind in offspring_crossover:\n",
    "            if torch.rand(1).item() < mutation_prob:\n",
    "                mutation_point = torch.randint(0, len(bounds), (1,)).item()\n",
    "                mutated_ind = ind.clone()\n",
    "                mutated_ind[mutation_point] = torch.rand(1).item() * (bounds_tensor[mutation_point, 1] - bounds_tensor[mutation_point, 0]) + bounds_tensor[mutation_point, 0]\n",
    "                offspring_mutation.append(mutated_ind)\n",
    "            else:\n",
    "                offspring_mutation.append(ind)  # Tidak mutasi\n",
    "        offspring_mutation = torch.stack(offspring_mutation)\n",
    "\n",
    "        # Langkah 3: Gabungkan populasi awal dan offspring\n",
    "        combined_population = torch.cat((population, offspring_crossover, offspring_mutation), dim=0)\n",
    "\n",
    "        # Langkah 4: Seleksi elitisme\n",
    "        # fitness_values = evaluate_fitness(combined_population)\n",
    "        fitness_values = objective_func(combined_population)\n",
    "        # fitness_values, _, _, _, _ = objective_func(combined_population)\n",
    "        \n",
    "        elite_count = int(population_size * elite_percentage)\n",
    "        elite_indices = torch.argsort(fitness_values)[:elite_count]\n",
    "        population = combined_population[elite_indices]\n",
    "\n",
    "        # Optional: Logging\n",
    "        # if full_logger and (generation % 10 == 0 or generation == num_generations - 1):\n",
    "        #     best_fitness = torch.min(fitness_values[elite_indices])\n",
    "        #     print(f\"Generation {generation}: Best Fitness = {best_fitness.item()}\")\n",
    "            \n",
    "        # Get the best solution found\n",
    "        # all_fitness_new_population = objective_func(population)\n",
    "        # all_fitness_new_population, path_last_Model_in_ga, model_in_ga, config_in_ga, i_ind_in_ga = objective_func(population)\n",
    "        \n",
    "        if(last_generation):\n",
    "            all_fitness_new_population, path_last_Model_in_ga, model_in_ga, config_in_ga, i_ind_in_ga = objective_func(population, True)\n",
    "        else:\n",
    "            all_fitness_new_population = objective_func(population)\n",
    "        \n",
    "        # best_index = torch.argmin(fitness_values)\n",
    "        # best_index = torch.argmin(fitness_values[elite_indices])\n",
    "        best_index = torch.argmin(all_fitness_new_population)\n",
    "        # best_solution = population[best_index]\n",
    "        best_solution = population[best_index]\n",
    "        # best_fitness = fitness_values[best_index]\n",
    "        best_fitness = all_fitness_new_population[best_index]\n",
    "        \n",
    "        # untuk membuat grafik konvergensi\n",
    "        max_gbest_each_iter[generation] = best_fitness\n",
    "        mean_gbest_each_iter[generation] = fitness_values.mean()\n",
    "        \n",
    "        # if generation == 0:\n",
    "        #     # Simpan path_last_Model, model, dan config pada variabel sementara untuk pertama kali\n",
    "        #     temp_path_last_Model_in_ga = path_last_Model_in_ga\n",
    "        #     temp_model_in_ga = model_in_ga\n",
    "        #     temp_config_in_ga = config_in_ga\n",
    "        #     temp_i_ind_in_ga = i_ind_in_ga\n",
    "        #     temp_best_fitness_in_ga = best_fitness\n",
    "        # else:\n",
    "        #     # Jika fitness dari individu ke-i lebih kecil, replace variabel sementara\n",
    "        #     if best_fitness < temp_best_fitness_in_ga:\n",
    "        #         temp_path_last_Model_in_ga = path_last_Model_in_ga\n",
    "        #         temp_model_in_ga = model_in_ga\n",
    "        #         temp_config_in_ga = config_in_ga\n",
    "        #         temp_i_ind_in_ga = i_ind_in_ga\n",
    "        #         temp_best_fitness_in_ga = best_fitness\n",
    "\n",
    "    # Mengembalikan populasi terbaik dan nilai fitness terbaik\n",
    "    # best_fitness = objective_func(population)\n",
    "    # best_solution_index = torch.argmin(best_fitness)\n",
    "    \n",
    "    # return population[best_solution_index], best_fitness[best_solution_index]\n",
    "    \n",
    "    temp_path_last_Model_in_ga = path_last_Model_in_ga\n",
    "    temp_model_in_ga = model_in_ga\n",
    "    temp_config_in_ga = config_in_ga\n",
    "    temp_i_ind_in_ga = i_ind_in_ga\n",
    "    temp_best_fitness_in_ga = best_fitness\n",
    "\n",
    "    if(full_logger == None):\n",
    "        return best_solution, best_fitness, population\n",
    "    else:\n",
    "        # return torch.tensor(xp_by_linspace, device=my_device), \\\n",
    "        #        torch.tensor(xp_by_linspace_to_p, device=my_device), log, f_new\n",
    "        # return X_ori, Y_ori, xp_by_non_or_with_linspace, \\\n",
    "        #        xp_by_non_or_with_linspace_to_p, log, log_TARty, f_new\n",
    "        # return best_solution, best_fitness[best_solution_index], population[best_solution_index], \\\n",
    "        #            max_gbest_each_iter, mean_gbest_each_iter\n",
    "        return best_solution, best_fitness, population, \\\n",
    "                   max_gbest_each_iter, mean_gbest_each_iter, \\\n",
    "                    temp_path_last_Model_in_ga, temp_model_in_ga, temp_config_in_ga, temp_i_ind_in_ga, temp_best_fitness_in_ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_func = fitness_ind_eg_maml\n",
    "# Tentukan batas untuk variabel\n",
    "bounds = [\n",
    "    (0, 4),  # Bound untuk seed (misalnya dari 0 sampai 100)\n",
    "    (0.01, 0.1),  # Bound untuk inner_step_size (misalnya dari 0.01 sampai 0.1)\n",
    "    (1, 7),  # Bound untuk inner_epochs (misalnya dari 1 sampai 10)\n",
    "    (0.01, 0.2),  # Bound untuk outer_stepsize_reptile (misalnya dari 0.01 sampai 0.2)\n",
    "    (0.001, 0.05),  # Bound untuk outer_stepsize_maml (misalnya dari 0.001 sampai 0.05)\n",
    "    (1, 10),  # Bound untuk n_iterations (misalnya dari 1 sampai 50)\n",
    "    (0, 1)\n",
    "]\n",
    "\n",
    "ga(objective_func, bounds, num_generations=num_iterations_all, population_size=pop_size_all, crossover_prob=0.7, mutation_prob=0.1, elite_percentage=1.0, full_logger=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GTVGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtvga(objective_func, bounds, num_generations=num_iterations_all, population_size=pop_size_all, crossover_prob=0.7, mutation_prob=0.1, elite_percentage=1., full_logger=1):\n",
    "    # Inisialisasi populasi awal dengan torch\n",
    "    bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "    # population = torch.rand(population_size, len(bounds)).to(bounds_tensor.device) * (bounds_tensor[:, 1] - bounds_tensor[:, 0]) + bounds_tensor[:, 0]\n",
    "    \n",
    "    # Constants\n",
    "    c1i, c1f = 2.5, 0.5\n",
    "    c2i, c2f = 0.5, 2.5\n",
    "    cr, mr = crossover_prob, mutation_prob\n",
    "    \n",
    "    dim = len(bounds)\n",
    "    \n",
    "    # Initialize population within the specified bounds\n",
    "    population = torch.rand((population_size, dim), device=my_device) * \\\n",
    "    (torch.tensor(bounds, device=my_device)[:, 1] - \\\n",
    "     torch.tensor(bounds, device=my_device)[:, 0]) + \\\n",
    "    torch.tensor(bounds, device=my_device)[:, 0]\n",
    "    \n",
    "    # print(population)\n",
    "    \n",
    "    # Inisialisasi variabel sementara\n",
    "    temp_path_last_Model_at_gtvga = None\n",
    "    temp_model_at_gtvga = None\n",
    "    temp_config_at_gtvga = None\n",
    "    temp_i_ind_at_gtvga = None\n",
    "    temp_best_fitness_in_gtvga = None\n",
    "    last_generation = False\n",
    "    \n",
    "    # Initialize untuk grafik konvergensi\n",
    "    max_gbest_each_iter = torch.empty(num_generations, device=my_device) # makna max disini tdk selalu nilai Max, tetapi krn loss, maka nilai yg paling min\n",
    "    mean_gbest_each_iter = torch.empty(num_generations, device=my_device)\n",
    "    \n",
    "    # Fungsi untuk mengevaluasi fitness\n",
    "    # def evaluate_fitness(pop):\n",
    "    #     return torch.tensor([objective_func(ind) for ind in pop], dtype=torch.float32)\n",
    "\n",
    "    # Melakukan proses GA\n",
    "    for generation in range(num_generations):\n",
    "        \n",
    "        # print('generation -> ',generation)\n",
    "        \n",
    "        if(generation==(num_generations-1)):\n",
    "            last_generation = True\n",
    "        \n",
    "        # Adaptive crossover and mutation rates utilive geometric time variant (GTV) \n",
    "        cr_ = ((c2f - c2i) * (generation / num_generations)) + c2i\n",
    "        mr_ = ((c1f - c1i) * (generation / num_generations)) + c1i\n",
    "        \n",
    "        crossover_prob = cr_\n",
    "        mutation_prob = mr_\n",
    "        \n",
    "        # Langkah 1: Crossover untuk menghasilkan offspring\n",
    "        offspring_crossover = []\n",
    "        for _ in range(population_size):\n",
    "            parent1, parent2 = population[torch.randint(0, population_size, (1,)).item()], population[torch.randint(0, population_size, (1,)).item()]\n",
    "            if torch.rand(1).item() < crossover_prob:\n",
    "                crossover_point = torch.randint(1, len(bounds) - 1, (1,)).item()\n",
    "                child = torch.cat((parent1[:crossover_point], parent2[crossover_point:]))\n",
    "            else:\n",
    "                child = parent1  # Tidak crossover\n",
    "            offspring_crossover.append(child)\n",
    "        offspring_crossover = torch.stack(offspring_crossover)\n",
    "\n",
    "        # Langkah 2: Mutasi untuk menghasilkan offspring\n",
    "        offspring_mutation = []\n",
    "        for ind in offspring_crossover:\n",
    "            if torch.rand(1).item() < mutation_prob:\n",
    "                mutation_point = torch.randint(0, len(bounds), (1,)).item()\n",
    "                mutated_ind = ind.clone()\n",
    "                mutated_ind[mutation_point] = torch.rand(1).item() * (bounds_tensor[mutation_point, 1] - bounds_tensor[mutation_point, 0]) + bounds_tensor[mutation_point, 0]\n",
    "                offspring_mutation.append(mutated_ind)\n",
    "            else:\n",
    "                offspring_mutation.append(ind)  # Tidak mutasi\n",
    "        offspring_mutation = torch.stack(offspring_mutation)\n",
    "\n",
    "        # Langkah 3: Gabungkan populasi awal dan offspring\n",
    "        combined_population = torch.cat((population, offspring_crossover, offspring_mutation), dim=0)\n",
    "\n",
    "        # Langkah 4: Seleksi elitisme\n",
    "        # fitness_values = evaluate_fitness(combined_population)\n",
    "        fitness_values = objective_func(combined_population)\n",
    "        # fitness_values, _, _, _, _ = objective_func(combined_population)\n",
    "        \n",
    "        elite_count = int(population_size * elite_percentage)\n",
    "        elite_indices = torch.argsort(fitness_values)[:elite_count]\n",
    "        population = combined_population[elite_indices]\n",
    "\n",
    "        # Optional: Logging\n",
    "        # if full_logger and (generation % 10 == 0 or generation == num_generations - 1):\n",
    "        #     best_fitness = torch.min(fitness_values[elite_indices])\n",
    "        #     print(f\"Generation {generation}: Best Fitness = {best_fitness.item()}\")\n",
    "            \n",
    "        # Get the best solution found\n",
    "        # all_fitness_new_population = objective_func(population)\n",
    "        # all_fitness_new_population, path_last_Model_in_gtvga, model_in_gtvga, config_in_gtvga, i_ind_in_gtvga = objective_func(population)\n",
    "        \n",
    "        if(last_generation):\n",
    "            all_fitness_new_population, path_last_Model_in_gtvga, model_in_gtvga, config_in_gtvga, i_ind_in_gtvga = objective_func(population, True)\n",
    "        else:\n",
    "            all_fitness_new_population = objective_func(population)\n",
    "        \n",
    "        # best_index = torch.argmin(fitness_values)\n",
    "        # best_index = torch.argmin(fitness_values[elite_indices])\n",
    "        best_index = torch.argmin(all_fitness_new_population)\n",
    "        # best_solution = population[best_index]\n",
    "        best_solution = population[best_index]\n",
    "        # best_fitness = fitness_values[best_index]\n",
    "        best_fitness = all_fitness_new_population[best_index]\n",
    "        \n",
    "        # untuk membuat grafik konvergensi\n",
    "        max_gbest_each_iter[generation] = best_fitness\n",
    "        mean_gbest_each_iter[generation] = fitness_values.mean()\n",
    "        \n",
    "        # if generation == 0:\n",
    "        #     # Simpan path_last_Model, model, dan config pada variabel sementara untuk pertama kali\n",
    "        #     temp_path_last_Model_in_gtvga = path_last_Model_in_gtvga\n",
    "        #     temp_model_in_gtvga = model_in_gtvga\n",
    "        #     temp_config_in_gtvga = config_in_gtvga\n",
    "        #     temp_i_ind_in_gtvga = i_ind_in_gtvga\n",
    "        #     temp_best_fitness_in_gtvga = best_fitness\n",
    "        # else:\n",
    "        #     # Jika fitness dari individu ke-i lebih kecil, replace variabel sementara\n",
    "        #     if best_fitness < temp_best_fitness_in_gtvga:\n",
    "        #         temp_path_last_Model_in_gtvga = path_last_Model_in_gtvga\n",
    "        #         temp_model_in_gtvga = model_in_gtvga\n",
    "        #         temp_config_in_gtvga = config_in_gtvga\n",
    "        #         temp_i_ind_in_gtvga = i_ind_in_gtvga\n",
    "        #         temp_best_fitness_in_gtvga = best_fitness\n",
    "\n",
    "    # Mengembalikan populasi terbaik dan nilai fitness terbaik\n",
    "    # best_fitness = objective_func(population)\n",
    "    # best_solution_index = torch.argmin(best_fitness)\n",
    "    \n",
    "    # return population[best_solution_index], best_fitness[best_solution_index]\n",
    "    \n",
    "    temp_path_last_Model_in_gtvga = path_last_Model_in_gtvga\n",
    "    temp_model_in_gtvga = model_in_gtvga\n",
    "    temp_config_in_gtvga = config_in_gtvga\n",
    "    temp_i_ind_in_gtvga = i_ind_in_gtvga\n",
    "    temp_best_fitness_in_gtvga = best_fitness\n",
    "\n",
    "    if(full_logger == None):\n",
    "        return best_solution, best_fitness, population\n",
    "    else:\n",
    "        # return torch.tensor(xp_by_linspace, device=my_device), \\\n",
    "        #        torch.tensor(xp_by_linspace_to_p, device=my_device), log, f_new\n",
    "        # return X_ori, Y_ori, xp_by_non_or_with_linspace, \\\n",
    "        #        xp_by_non_or_with_linspace_to_p, log, log_TARty, f_new\n",
    "        # return best_solution, best_fitness[best_solution_index], population[best_solution_index], \\\n",
    "        #            max_gbest_each_iter, mean_gbest_each_iter\n",
    "        return best_solution, best_fitness, population, \\\n",
    "                   max_gbest_each_iter, mean_gbest_each_iter, \\\n",
    "                    temp_path_last_Model_in_gtvga, temp_model_in_gtvga, temp_config_in_gtvga, temp_i_ind_in_gtvga, temp_best_fitness_in_gtvga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Final Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness_ind_eg_maml_tp_1\n",
      "[(0, 4), (0.01, 0.1), (1, 7), (0.01, 0.2), (0.001, 0.05), (1, 10), (0, 1)]\n",
      "ga\n",
      "\n",
      "generation ->  0\n",
      "generation ->  1\n",
      "generation ->  2\n",
      "generation ->  3\n",
      "generation ->  4\n",
      "generation ->  5\n",
      "generation ->  6\n",
      "generation ->  7\n",
      "generation ->  8\n",
      "generation ->  9\n",
      "generation ->  10\n",
      "generation ->  11\n",
      "generation ->  12\n",
      "generation ->  13\n",
      "generation ->  14\n",
      "generation ->  15\n",
      "generation ->  16\n",
      "generation ->  17\n",
      "generation ->  18\n",
      "generation ->  19\n",
      "generation ->  20\n",
      "generation ->  21\n",
      "generation ->  22\n",
      "generation ->  23\n",
      "generation ->  24\n",
      "generation ->  25\n",
      "generation ->  26\n",
      "generation ->  27\n",
      "generation ->  28\n",
      "generation ->  29\n",
      "generation ->  30\n",
      "generation ->  31\n",
      "generation ->  32\n",
      "generation ->  33\n",
      "generation ->  34\n",
      "generation ->  35\n",
      "generation ->  36\n",
      "generation ->  37\n",
      "generation ->  38\n",
      "generation ->  39\n",
      "generation ->  40\n",
      "generation ->  41\n",
      "generation ->  42\n",
      "generation ->  43\n",
      "generation ->  44\n",
      "generation ->  45\n",
      "generation ->  46\n",
      "generation ->  47\n",
      "generation ->  48\n",
      "generation ->  49\n",
      "generation ->  50\n",
      "generation ->  51\n",
      "generation ->  52\n",
      "generation ->  53\n",
      "generation ->  54\n",
      "generation ->  55\n",
      "generation ->  56\n",
      "generation ->  57\n",
      "generation ->  58\n",
      "generation ->  59\n",
      "generation ->  60\n",
      "generation ->  61\n",
      "generation ->  62\n",
      "generation ->  63\n",
      "generation ->  64\n",
      "generation ->  65\n",
      "generation ->  66\n",
      "generation ->  67\n",
      "generation ->  68\n",
      "generation ->  69\n",
      "generation ->  70\n",
      "generation ->  71\n",
      "generation ->  72\n",
      "generation ->  73\n",
      "generation ->  74\n",
      "generation ->  75\n",
      "generation ->  76\n",
      "generation ->  77\n",
      "generation ->  78\n",
      "generation ->  79\n",
      "generation ->  80\n",
      "generation ->  81\n",
      "generation ->  82\n",
      "generation ->  83\n",
      "generation ->  84\n",
      "generation ->  85\n",
      "generation ->  86\n",
      "generation ->  87\n",
      "generation ->  88\n",
      "generation ->  89\n",
      "generation ->  90\n",
      "generation ->  91\n",
      "generation ->  92\n",
      "generation ->  93\n",
      "generation ->  94\n",
      "generation ->  95\n",
      "generation ->  96\n",
      "generation ->  97\n",
      "generation ->  98\n",
      "generation ->  99\n",
      "generation ->  100\n",
      "generation ->  101\n",
      "generation ->  102\n",
      "generation ->  103\n",
      "generation ->  104\n",
      "generation ->  105\n",
      "generation ->  106\n",
      "generation ->  107\n",
      "generation ->  108\n",
      "generation ->  109\n",
      "generation ->  110\n",
      "generation ->  111\n",
      "generation ->  112\n",
      "generation ->  113\n",
      "generation ->  114\n",
      "generation ->  115\n",
      "generation ->  116\n",
      "generation ->  117\n",
      "generation ->  118\n",
      "generation ->  119\n",
      "generation ->  120\n",
      "generation ->  121\n",
      "generation ->  122\n",
      "generation ->  123\n",
      "generation ->  124\n",
      "generation ->  125\n",
      "generation ->  126\n",
      "generation ->  127\n",
      "generation ->  128\n",
      "generation ->  129\n",
      "generation ->  130\n",
      "generation ->  131\n",
      "generation ->  132\n",
      "generation ->  133\n",
      "generation ->  134\n",
      "generation ->  135\n",
      "generation ->  136\n",
      "generation ->  137\n",
      "generation ->  138\n",
      "generation ->  139\n",
      "generation ->  140\n",
      "generation ->  141\n",
      "generation ->  142\n",
      "generation ->  143\n",
      "generation ->  144\n",
      "generation ->  145\n",
      "generation ->  146\n",
      "generation ->  147\n",
      "generation ->  148\n",
      "generation ->  149\n",
      "generation ->  150\n",
      "generation ->  151\n",
      "generation ->  152\n",
      "generation ->  153\n",
      "generation ->  154\n",
      "generation ->  155\n",
      "generation ->  156\n",
      "generation ->  157\n",
      "generation ->  158\n",
      "generation ->  159\n",
      "generation ->  160\n",
      "generation ->  161\n",
      "generation ->  162\n",
      "generation ->  163\n",
      "generation ->  164\n",
      "generation ->  165\n",
      "generation ->  166\n",
      "generation ->  167\n",
      "generation ->  168\n",
      "generation ->  169\n",
      "generation ->  170\n",
      "generation ->  171\n",
      "generation ->  172\n",
      "generation ->  173\n",
      "generation ->  174\n",
      "generation ->  175\n",
      "generation ->  176\n",
      "generation ->  177\n",
      "generation ->  178\n",
      "generation ->  179\n",
      "generation ->  180\n",
      "generation ->  181\n",
      "generation ->  182\n",
      "generation ->  183\n",
      "generation ->  184\n",
      "generation ->  185\n",
      "generation ->  186\n",
      "generation ->  187\n",
      "generation ->  188\n",
      "generation ->  189\n",
      "generation ->  190\n",
      "generation ->  191\n",
      "generation ->  192\n",
      "generation ->  193\n",
      "generation ->  194\n",
      "generation ->  195\n",
      "generation ->  196\n",
      "generation ->  197\n",
      "generation ->  198\n",
      "generation ->  199\n",
      "generation ->  200\n",
      "generation ->  201\n",
      "generation ->  202\n",
      "generation ->  203\n",
      "generation ->  204\n",
      "generation ->  205\n",
      "generation ->  206\n",
      "generation ->  207\n",
      "generation ->  208\n",
      "generation ->  209\n",
      "generation ->  210\n",
      "generation ->  211\n",
      "generation ->  212\n",
      "generation ->  213\n",
      "generation ->  214\n",
      "generation ->  215\n",
      "generation ->  216\n",
      "generation ->  217\n",
      "generation ->  218\n",
      "generation ->  219\n",
      "generation ->  220\n",
      "generation ->  221\n",
      "generation ->  222\n",
      "generation ->  223\n",
      "generation ->  224\n",
      "generation ->  225\n",
      "generation ->  226\n",
      "generation ->  227\n",
      "generation ->  228\n",
      "generation ->  229\n",
      "generation ->  230\n",
      "generation ->  231\n",
      "generation ->  232\n",
      "generation ->  233\n",
      "generation ->  234\n",
      "generation ->  235\n",
      "generation ->  236\n",
      "generation ->  237\n",
      "generation ->  238\n",
      "generation ->  239\n",
      "generation ->  240\n",
      "generation ->  241\n",
      "generation ->  242\n",
      "generation ->  243\n",
      "generation ->  244\n",
      "generation ->  245\n",
      "generation ->  246\n",
      "generation ->  247\n",
      "generation ->  248\n",
      "generation ->  249\n",
      "{'n_iterations': 7, 'n_data_all': 5, 'n_sample': 5, 'n_train': 3, 'seed': 1, 'inner_step_size': 0.05798077583312988, 'inner_epochs': 3, 'outer_stepsize_reptile': 0.01681629940867424, 'outer_stepsize_maml': 0.0431055948138237, 'run': 'E-MAML', 'final_lossval': 0.22634806, 'filename_last_Model': '02-11-2024-11-24-15', 'path_filename_last_Model': 'model_reg_last/model_last_E-MAML_0.226_02-11-2024-11-24-15'}\n",
      "\n",
      "fitness_ind_eg_maml_tp_1\n",
      "[(0, 4), (0.01, 0.1), (1, 7), (0.01, 0.2), (0.001, 0.05), (1, 10), (0, 1)]\n",
      "gtvga\n",
      "\n",
      "generation ->  0\n",
      "generation ->  1\n",
      "generation ->  2\n",
      "generation ->  3\n",
      "generation ->  4\n",
      "generation ->  5\n",
      "generation ->  6\n",
      "generation ->  7\n",
      "generation ->  8\n",
      "generation ->  9\n",
      "generation ->  10\n",
      "generation ->  11\n",
      "generation ->  12\n",
      "generation ->  13\n",
      "generation ->  14\n",
      "generation ->  15\n",
      "generation ->  16\n",
      "generation ->  17\n",
      "generation ->  18\n",
      "generation ->  19\n",
      "generation ->  20\n",
      "generation ->  21\n",
      "generation ->  22\n",
      "generation ->  23\n",
      "generation ->  24\n",
      "generation ->  25\n",
      "generation ->  26\n",
      "generation ->  27\n",
      "generation ->  28\n",
      "generation ->  29\n",
      "generation ->  30\n",
      "generation ->  31\n",
      "generation ->  32\n",
      "generation ->  33\n",
      "generation ->  34\n",
      "generation ->  35\n",
      "generation ->  36\n",
      "generation ->  37\n",
      "generation ->  38\n",
      "generation ->  39\n",
      "generation ->  40\n",
      "generation ->  41\n",
      "generation ->  42\n",
      "generation ->  43\n",
      "generation ->  44\n",
      "generation ->  45\n",
      "generation ->  46\n",
      "generation ->  47\n",
      "generation ->  48\n",
      "generation ->  49\n",
      "generation ->  50\n",
      "generation ->  51\n",
      "generation ->  52\n",
      "generation ->  53\n",
      "generation ->  54\n",
      "generation ->  55\n",
      "generation ->  56\n",
      "generation ->  57\n",
      "generation ->  58\n",
      "generation ->  59\n",
      "generation ->  60\n",
      "generation ->  61\n",
      "generation ->  62\n",
      "generation ->  63\n",
      "generation ->  64\n",
      "generation ->  65\n",
      "generation ->  66\n",
      "generation ->  67\n",
      "generation ->  68\n",
      "generation ->  69\n",
      "generation ->  70\n",
      "generation ->  71\n",
      "generation ->  72\n",
      "generation ->  73\n",
      "generation ->  74\n",
      "generation ->  75\n",
      "generation ->  76\n",
      "generation ->  77\n",
      "generation ->  78\n",
      "generation ->  79\n",
      "generation ->  80\n",
      "generation ->  81\n",
      "generation ->  82\n",
      "generation ->  83\n",
      "generation ->  84\n",
      "generation ->  85\n",
      "generation ->  86\n",
      "generation ->  87\n",
      "generation ->  88\n",
      "generation ->  89\n",
      "generation ->  90\n",
      "generation ->  91\n",
      "generation ->  92\n",
      "generation ->  93\n",
      "generation ->  94\n",
      "generation ->  95\n",
      "generation ->  96\n",
      "generation ->  97\n",
      "generation ->  98\n",
      "generation ->  99\n",
      "generation ->  100\n",
      "generation ->  101\n",
      "generation ->  102\n",
      "generation ->  103\n",
      "generation ->  104\n",
      "generation ->  105\n",
      "generation ->  106\n",
      "generation ->  107\n",
      "generation ->  108\n",
      "generation ->  109\n",
      "generation ->  110\n",
      "generation ->  111\n",
      "generation ->  112\n",
      "generation ->  113\n",
      "generation ->  114\n",
      "generation ->  115\n",
      "generation ->  116\n",
      "generation ->  117\n",
      "generation ->  118\n",
      "generation ->  119\n",
      "generation ->  120\n",
      "generation ->  121\n",
      "generation ->  122\n",
      "generation ->  123\n",
      "generation ->  124\n",
      "generation ->  125\n",
      "generation ->  126\n",
      "generation ->  127\n",
      "generation ->  128\n",
      "generation ->  129\n",
      "generation ->  130\n",
      "generation ->  131\n",
      "generation ->  132\n",
      "generation ->  133\n",
      "generation ->  134\n",
      "generation ->  135\n",
      "generation ->  136\n",
      "generation ->  137\n",
      "generation ->  138\n",
      "generation ->  139\n",
      "generation ->  140\n",
      "generation ->  141\n",
      "generation ->  142\n",
      "generation ->  143\n",
      "generation ->  144\n",
      "generation ->  145\n",
      "generation ->  146\n",
      "generation ->  147\n",
      "generation ->  148\n",
      "generation ->  149\n",
      "generation ->  150\n",
      "generation ->  151\n",
      "generation ->  152\n",
      "generation ->  153\n",
      "generation ->  154\n",
      "generation ->  155\n",
      "generation ->  156\n",
      "generation ->  157\n",
      "generation ->  158\n",
      "generation ->  159\n",
      "generation ->  160\n",
      "generation ->  161\n",
      "generation ->  162\n",
      "generation ->  163\n",
      "generation ->  164\n",
      "generation ->  165\n",
      "generation ->  166\n",
      "generation ->  167\n",
      "generation ->  168\n",
      "generation ->  169\n",
      "generation ->  170\n",
      "generation ->  171\n",
      "generation ->  172\n",
      "generation ->  173\n",
      "generation ->  174\n",
      "generation ->  175\n",
      "generation ->  176\n",
      "generation ->  177\n",
      "generation ->  178\n",
      "generation ->  179\n",
      "generation ->  180\n",
      "generation ->  181\n",
      "generation ->  182\n",
      "generation ->  183\n",
      "generation ->  184\n",
      "generation ->  185\n",
      "generation ->  186\n",
      "generation ->  187\n",
      "generation ->  188\n",
      "generation ->  189\n",
      "generation ->  190\n",
      "generation ->  191\n",
      "generation ->  192\n",
      "generation ->  193\n",
      "generation ->  194\n",
      "generation ->  195\n",
      "generation ->  196\n",
      "generation ->  197\n",
      "generation ->  198\n",
      "generation ->  199\n",
      "generation ->  200\n",
      "generation ->  201\n",
      "generation ->  202\n",
      "generation ->  203\n",
      "generation ->  204\n",
      "generation ->  205\n",
      "generation ->  206\n",
      "generation ->  207\n",
      "generation ->  208\n",
      "generation ->  209\n",
      "generation ->  210\n",
      "generation ->  211\n",
      "generation ->  212\n",
      "generation ->  213\n",
      "generation ->  214\n",
      "generation ->  215\n",
      "generation ->  216\n",
      "generation ->  217\n",
      "generation ->  218\n",
      "generation ->  219\n",
      "generation ->  220\n",
      "generation ->  221\n",
      "generation ->  222\n",
      "generation ->  223\n",
      "generation ->  224\n",
      "generation ->  225\n",
      "generation ->  226\n",
      "generation ->  227\n",
      "generation ->  228\n",
      "generation ->  229\n",
      "generation ->  230\n",
      "generation ->  231\n",
      "generation ->  232\n",
      "generation ->  233\n",
      "generation ->  234\n",
      "generation ->  235\n",
      "generation ->  236\n",
      "generation ->  237\n",
      "generation ->  238\n",
      "generation ->  239\n",
      "generation ->  240\n",
      "generation ->  241\n",
      "generation ->  242\n",
      "generation ->  243\n",
      "generation ->  244\n",
      "generation ->  245\n",
      "generation ->  246\n",
      "generation ->  247\n",
      "generation ->  248\n",
      "generation ->  249\n",
      "{'n_iterations': 9, 'n_data_all': 5, 'n_sample': 5, 'n_train': 3, 'seed': 1, 'inner_step_size': 0.09094420820474625, 'inner_epochs': 4, 'outer_stepsize_reptile': 0.11987612396478653, 'outer_stepsize_maml': 0.049963757395744324, 'run': 'E-MAML', 'final_lossval': 0.22512953, 'filename_last_Model': '02-11-2024-12-18-14', 'path_filename_last_Model': 'model_reg_last/model_last_E-MAML_0.225_02-11-2024-12-18-14'}\n",
      "\n",
      "fitness_ind_eg_maml_tp_2\n",
      "[(0, 8), (0.001, 0.2), (1, 14), (0.001, 0.4), (0.0001, 0.1), (1, 20), (0, 1)]\n",
      "ga\n",
      "\n",
      "generation ->  0\n",
      "generation ->  1\n",
      "generation ->  2\n",
      "generation ->  3\n",
      "generation ->  4\n",
      "generation ->  5\n",
      "generation ->  6\n",
      "generation ->  7\n",
      "generation ->  8\n",
      "generation ->  9\n",
      "generation ->  10\n",
      "generation ->  11\n",
      "generation ->  12\n",
      "generation ->  13\n",
      "generation ->  14\n",
      "generation ->  15\n",
      "generation ->  16\n",
      "generation ->  17\n",
      "generation ->  18\n",
      "generation ->  19\n",
      "generation ->  20\n",
      "generation ->  21\n",
      "generation ->  22\n",
      "generation ->  23\n",
      "generation ->  24\n",
      "generation ->  25\n",
      "generation ->  26\n",
      "generation ->  27\n",
      "generation ->  28\n",
      "generation ->  29\n",
      "generation ->  30\n",
      "generation ->  31\n",
      "generation ->  32\n",
      "generation ->  33\n",
      "generation ->  34\n",
      "generation ->  35\n",
      "generation ->  36\n",
      "generation ->  37\n",
      "generation ->  38\n",
      "generation ->  39\n",
      "generation ->  40\n",
      "generation ->  41\n",
      "generation ->  42\n",
      "generation ->  43\n",
      "generation ->  44\n",
      "generation ->  45\n",
      "generation ->  46\n",
      "generation ->  47\n",
      "generation ->  48\n",
      "generation ->  49\n",
      "generation ->  50\n",
      "generation ->  51\n",
      "generation ->  52\n",
      "generation ->  53\n",
      "generation ->  54\n",
      "generation ->  55\n",
      "generation ->  56\n",
      "generation ->  57\n",
      "generation ->  58\n",
      "generation ->  59\n",
      "generation ->  60\n",
      "generation ->  61\n",
      "generation ->  62\n",
      "generation ->  63\n",
      "generation ->  64\n",
      "generation ->  65\n",
      "generation ->  66\n",
      "generation ->  67\n",
      "generation ->  68\n",
      "generation ->  69\n",
      "generation ->  70\n",
      "generation ->  71\n",
      "generation ->  72\n",
      "generation ->  73\n",
      "generation ->  74\n",
      "generation ->  75\n",
      "generation ->  76\n",
      "generation ->  77\n",
      "generation ->  78\n",
      "generation ->  79\n",
      "generation ->  80\n",
      "generation ->  81\n",
      "generation ->  82\n",
      "generation ->  83\n",
      "generation ->  84\n",
      "generation ->  85\n",
      "generation ->  86\n",
      "generation ->  87\n",
      "generation ->  88\n",
      "generation ->  89\n",
      "generation ->  90\n",
      "generation ->  91\n",
      "generation ->  92\n",
      "generation ->  93\n",
      "generation ->  94\n",
      "generation ->  95\n",
      "generation ->  96\n",
      "generation ->  97\n",
      "generation ->  98\n",
      "generation ->  99\n",
      "generation ->  100\n",
      "generation ->  101\n",
      "generation ->  102\n",
      "generation ->  103\n",
      "generation ->  104\n",
      "generation ->  105\n",
      "generation ->  106\n",
      "generation ->  107\n",
      "generation ->  108\n",
      "generation ->  109\n",
      "generation ->  110\n",
      "generation ->  111\n",
      "generation ->  112\n",
      "generation ->  113\n",
      "generation ->  114\n",
      "generation ->  115\n",
      "generation ->  116\n",
      "generation ->  117\n",
      "generation ->  118\n",
      "generation ->  119\n",
      "generation ->  120\n",
      "generation ->  121\n",
      "generation ->  122\n",
      "generation ->  123\n",
      "generation ->  124\n",
      "generation ->  125\n",
      "generation ->  126\n",
      "generation ->  127\n",
      "generation ->  128\n",
      "generation ->  129\n",
      "generation ->  130\n",
      "generation ->  131\n",
      "generation ->  132\n",
      "generation ->  133\n",
      "generation ->  134\n",
      "generation ->  135\n",
      "generation ->  136\n",
      "generation ->  137\n",
      "generation ->  138\n",
      "generation ->  139\n",
      "generation ->  140\n",
      "generation ->  141\n",
      "generation ->  142\n",
      "generation ->  143\n",
      "generation ->  144\n",
      "generation ->  145\n",
      "generation ->  146\n",
      "generation ->  147\n",
      "generation ->  148\n",
      "generation ->  149\n",
      "generation ->  150\n",
      "generation ->  151\n",
      "generation ->  152\n",
      "generation ->  153\n",
      "generation ->  154\n",
      "generation ->  155\n",
      "generation ->  156\n",
      "generation ->  157\n",
      "generation ->  158\n",
      "generation ->  159\n",
      "generation ->  160\n",
      "generation ->  161\n",
      "generation ->  162\n",
      "generation ->  163\n",
      "generation ->  164\n",
      "generation ->  165\n",
      "generation ->  166\n",
      "generation ->  167\n",
      "generation ->  168\n",
      "generation ->  169\n",
      "generation ->  170\n",
      "generation ->  171\n",
      "generation ->  172\n",
      "generation ->  173\n",
      "generation ->  174\n",
      "generation ->  175\n",
      "generation ->  176\n",
      "generation ->  177\n",
      "generation ->  178\n",
      "generation ->  179\n",
      "generation ->  180\n",
      "generation ->  181\n",
      "generation ->  182\n",
      "generation ->  183\n",
      "generation ->  184\n",
      "generation ->  185\n",
      "generation ->  186\n",
      "generation ->  187\n",
      "generation ->  188\n",
      "generation ->  189\n",
      "generation ->  190\n",
      "generation ->  191\n",
      "generation ->  192\n",
      "generation ->  193\n",
      "generation ->  194\n",
      "generation ->  195\n",
      "generation ->  196\n",
      "generation ->  197\n",
      "generation ->  198\n",
      "generation ->  199\n",
      "generation ->  200\n",
      "generation ->  201\n",
      "generation ->  202\n",
      "generation ->  203\n",
      "generation ->  204\n",
      "generation ->  205\n",
      "generation ->  206\n",
      "generation ->  207\n",
      "generation ->  208\n",
      "generation ->  209\n",
      "generation ->  210\n",
      "generation ->  211\n",
      "generation ->  212\n",
      "generation ->  213\n",
      "generation ->  214\n",
      "generation ->  215\n",
      "generation ->  216\n",
      "generation ->  217\n",
      "generation ->  218\n",
      "generation ->  219\n",
      "generation ->  220\n",
      "generation ->  221\n",
      "generation ->  222\n",
      "generation ->  223\n",
      "generation ->  224\n",
      "generation ->  225\n",
      "generation ->  226\n",
      "generation ->  227\n",
      "generation ->  228\n",
      "generation ->  229\n",
      "generation ->  230\n",
      "generation ->  231\n",
      "generation ->  232\n",
      "generation ->  233\n",
      "generation ->  234\n",
      "generation ->  235\n",
      "generation ->  236\n",
      "generation ->  237\n",
      "generation ->  238\n",
      "generation ->  239\n",
      "generation ->  240\n",
      "generation ->  241\n",
      "generation ->  242\n",
      "generation ->  243\n",
      "generation ->  244\n",
      "generation ->  245\n",
      "generation ->  246\n",
      "generation ->  247\n",
      "generation ->  248\n",
      "generation ->  249\n",
      "{'n_iterations': 19, 'n_data_all': 5, 'n_sample': 5, 'n_train': 3, 'seed': 1, 'inner_step_size': 0.013370887376368046, 'inner_epochs': 8, 'outer_stepsize_reptile': 0.1833643913269043, 'outer_stepsize_maml': 0.091679647564888, 'run': 'E-MAML', 'final_lossval': 0.21573043, 'filename_last_Model': '02-11-2024-14-47-42', 'path_filename_last_Model': 'model_reg_last/model_last_E-MAML_0.216_02-11-2024-14-47-42'}\n",
      "\n",
      "fitness_ind_eg_maml_tp_2\n",
      "[(0, 8), (0.001, 0.2), (1, 14), (0.001, 0.4), (0.0001, 0.1), (1, 20), (0, 1)]\n",
      "gtvga\n",
      "\n",
      "generation ->  0\n",
      "generation ->  1\n",
      "generation ->  2\n",
      "generation ->  3\n",
      "generation ->  4\n",
      "generation ->  5\n",
      "generation ->  6\n",
      "generation ->  7\n",
      "generation ->  8\n",
      "generation ->  9\n",
      "generation ->  10\n",
      "generation ->  11\n",
      "generation ->  12\n",
      "generation ->  13\n",
      "generation ->  14\n",
      "generation ->  15\n",
      "generation ->  16\n",
      "generation ->  17\n",
      "generation ->  18\n",
      "generation ->  19\n",
      "generation ->  20\n",
      "generation ->  21\n",
      "generation ->  22\n",
      "generation ->  23\n",
      "generation ->  24\n",
      "generation ->  25\n",
      "generation ->  26\n",
      "generation ->  27\n",
      "generation ->  28\n",
      "generation ->  29\n",
      "generation ->  30\n",
      "generation ->  31\n",
      "generation ->  32\n",
      "generation ->  33\n",
      "generation ->  34\n",
      "generation ->  35\n",
      "generation ->  36\n",
      "generation ->  37\n",
      "generation ->  38\n",
      "generation ->  39\n",
      "generation ->  40\n",
      "generation ->  41\n",
      "generation ->  42\n",
      "generation ->  43\n",
      "generation ->  44\n",
      "generation ->  45\n",
      "generation ->  46\n",
      "generation ->  47\n",
      "generation ->  48\n",
      "generation ->  49\n",
      "generation ->  50\n",
      "generation ->  51\n",
      "generation ->  52\n",
      "generation ->  53\n",
      "generation ->  54\n",
      "generation ->  55\n",
      "generation ->  56\n",
      "generation ->  57\n",
      "generation ->  58\n",
      "generation ->  59\n",
      "generation ->  60\n",
      "generation ->  61\n",
      "generation ->  62\n",
      "generation ->  63\n",
      "generation ->  64\n",
      "generation ->  65\n",
      "generation ->  66\n",
      "generation ->  67\n",
      "generation ->  68\n",
      "generation ->  69\n",
      "generation ->  70\n",
      "generation ->  71\n",
      "generation ->  72\n",
      "generation ->  73\n",
      "generation ->  74\n",
      "generation ->  75\n",
      "generation ->  76\n",
      "generation ->  77\n",
      "generation ->  78\n",
      "generation ->  79\n",
      "generation ->  80\n",
      "generation ->  81\n",
      "generation ->  82\n",
      "generation ->  83\n",
      "generation ->  84\n",
      "generation ->  85\n",
      "generation ->  86\n",
      "generation ->  87\n",
      "generation ->  88\n",
      "generation ->  89\n",
      "generation ->  90\n",
      "generation ->  91\n",
      "generation ->  92\n",
      "generation ->  93\n",
      "generation ->  94\n",
      "generation ->  95\n",
      "generation ->  96\n",
      "generation ->  97\n",
      "generation ->  98\n",
      "generation ->  99\n",
      "generation ->  100\n",
      "generation ->  101\n",
      "generation ->  102\n",
      "generation ->  103\n",
      "generation ->  104\n",
      "generation ->  105\n",
      "generation ->  106\n",
      "generation ->  107\n",
      "generation ->  108\n",
      "generation ->  109\n",
      "generation ->  110\n",
      "generation ->  111\n",
      "generation ->  112\n",
      "generation ->  113\n",
      "generation ->  114\n",
      "generation ->  115\n",
      "generation ->  116\n",
      "generation ->  117\n",
      "generation ->  118\n",
      "generation ->  119\n",
      "generation ->  120\n",
      "generation ->  121\n",
      "generation ->  122\n",
      "generation ->  123\n",
      "generation ->  124\n",
      "generation ->  125\n",
      "generation ->  126\n",
      "generation ->  127\n",
      "generation ->  128\n",
      "generation ->  129\n",
      "generation ->  130\n",
      "generation ->  131\n",
      "generation ->  132\n",
      "generation ->  133\n",
      "generation ->  134\n",
      "generation ->  135\n",
      "generation ->  136\n",
      "generation ->  137\n",
      "generation ->  138\n",
      "generation ->  139\n",
      "generation ->  140\n",
      "generation ->  141\n",
      "generation ->  142\n",
      "generation ->  143\n",
      "generation ->  144\n",
      "generation ->  145\n",
      "generation ->  146\n",
      "generation ->  147\n",
      "generation ->  148\n",
      "generation ->  149\n",
      "generation ->  150\n",
      "generation ->  151\n",
      "generation ->  152\n",
      "generation ->  153\n",
      "generation ->  154\n",
      "generation ->  155\n",
      "generation ->  156\n",
      "generation ->  157\n",
      "generation ->  158\n",
      "generation ->  159\n",
      "generation ->  160\n",
      "generation ->  161\n",
      "generation ->  162\n",
      "generation ->  163\n",
      "generation ->  164\n",
      "generation ->  165\n",
      "generation ->  166\n",
      "generation ->  167\n",
      "generation ->  168\n",
      "generation ->  169\n",
      "generation ->  170\n",
      "generation ->  171\n",
      "generation ->  172\n",
      "generation ->  173\n",
      "generation ->  174\n",
      "generation ->  175\n",
      "generation ->  176\n",
      "generation ->  177\n",
      "generation ->  178\n",
      "generation ->  179\n",
      "generation ->  180\n",
      "generation ->  181\n",
      "generation ->  182\n",
      "generation ->  183\n",
      "generation ->  184\n",
      "generation ->  185\n",
      "generation ->  186\n",
      "generation ->  187\n",
      "generation ->  188\n",
      "generation ->  189\n",
      "generation ->  190\n",
      "generation ->  191\n",
      "generation ->  192\n",
      "generation ->  193\n",
      "generation ->  194\n",
      "generation ->  195\n",
      "generation ->  196\n",
      "generation ->  197\n",
      "generation ->  198\n",
      "generation ->  199\n",
      "generation ->  200\n",
      "generation ->  201\n",
      "generation ->  202\n",
      "generation ->  203\n",
      "generation ->  204\n",
      "generation ->  205\n",
      "generation ->  206\n",
      "generation ->  207\n",
      "generation ->  208\n",
      "generation ->  209\n",
      "generation ->  210\n",
      "generation ->  211\n",
      "generation ->  212\n",
      "generation ->  213\n",
      "generation ->  214\n",
      "generation ->  215\n",
      "generation ->  216\n",
      "generation ->  217\n",
      "generation ->  218\n",
      "generation ->  219\n",
      "generation ->  220\n",
      "generation ->  221\n",
      "generation ->  222\n",
      "generation ->  223\n",
      "generation ->  224\n",
      "generation ->  225\n",
      "generation ->  226\n",
      "generation ->  227\n",
      "generation ->  228\n",
      "generation ->  229\n",
      "generation ->  230\n",
      "generation ->  231\n",
      "generation ->  232\n",
      "generation ->  233\n",
      "generation ->  234\n",
      "generation ->  235\n",
      "generation ->  236\n",
      "generation ->  237\n",
      "generation ->  238\n",
      "generation ->  239\n",
      "generation ->  240\n",
      "generation ->  241\n",
      "generation ->  242\n",
      "generation ->  243\n",
      "generation ->  244\n",
      "generation ->  245\n",
      "generation ->  246\n",
      "generation ->  247\n",
      "generation ->  248\n",
      "generation ->  249\n",
      "{'n_iterations': 19, 'n_data_all': 5, 'n_sample': 5, 'n_train': 3, 'seed': 1, 'inner_step_size': 0.0408119298517704, 'inner_epochs': 12, 'outer_stepsize_reptile': 0.17148680984973907, 'outer_stepsize_maml': 0.09851877391338348, 'run': 'E-MAML', 'final_lossval': 0.21484125, 'filename_last_Model': '02-11-2024-17-51-52', 'path_filename_last_Model': 'model_reg_last/model_last_E-MAML_0.215_02-11-2024-17-51-52'}\n",
      "\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b1cd8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_b1cd8_level0_col0\" class=\"col_heading level0 col0\" >Function</th>\n",
       "      <th id=\"T_b1cd8_level0_col1\" class=\"col_heading level0 col1\" >Algorithm</th>\n",
       "      <th id=\"T_b1cd8_level0_col2\" class=\"col_heading level0 col2\" >Best Solution</th>\n",
       "      <th id=\"T_b1cd8_level0_col3\" class=\"col_heading level0 col3\" >Best Fitness</th>\n",
       "      <th id=\"T_b1cd8_level0_col4\" class=\"col_heading level0 col4\" >Median Fitness</th>\n",
       "      <th id=\"T_b1cd8_level0_col5\" class=\"col_heading level0 col5\" >Worst Fitness</th>\n",
       "      <th id=\"T_b1cd8_level0_col6\" class=\"col_heading level0 col6\" >Mean Fitness</th>\n",
       "      <th id=\"T_b1cd8_level0_col7\" class=\"col_heading level0 col7\" >Std Fitness</th>\n",
       "      <th id=\"T_b1cd8_level0_col8\" class=\"col_heading level0 col8\" >Running Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_b1cd8_row0_col0\" class=\"data row0 col0\" >fitness_ind_eg_maml_tp_1</td>\n",
       "      <td id=\"T_b1cd8_row0_col1\" class=\"data row0 col1\" >ga</td>\n",
       "      <td id=\"T_b1cd8_row0_col2\" class=\"data row0 col2\" >[1.8474588  0.05798078 3.0103192  0.0168163  0.04310559 7.065363\n",
       " 0.07437909]</td>\n",
       "      <td id=\"T_b1cd8_row0_col3\" class=\"data row0 col3\" >[0.22634806]</td>\n",
       "      <td id=\"T_b1cd8_row0_col4\" class=\"data row0 col4\" >0.226348</td>\n",
       "      <td id=\"T_b1cd8_row0_col5\" class=\"data row0 col5\" >0.226348</td>\n",
       "      <td id=\"T_b1cd8_row0_col6\" class=\"data row0 col6\" >0.226348</td>\n",
       "      <td id=\"T_b1cd8_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_b1cd8_row0_col8\" class=\"data row0 col8\" >2523.112744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b1cd8_row1_col0\" class=\"data row1 col0\" >fitness_ind_eg_maml_tp_1</td>\n",
       "      <td id=\"T_b1cd8_row1_col1\" class=\"data row1 col1\" >gtvga</td>\n",
       "      <td id=\"T_b1cd8_row1_col2\" class=\"data row1 col2\" >[1.4895089  0.09094421 4.700203   0.11987612 0.04996376 9.818686\n",
       " 0.2801975 ]</td>\n",
       "      <td id=\"T_b1cd8_row1_col3\" class=\"data row1 col3\" >[0.22512953]</td>\n",
       "      <td id=\"T_b1cd8_row1_col4\" class=\"data row1 col4\" >0.225130</td>\n",
       "      <td id=\"T_b1cd8_row1_col5\" class=\"data row1 col5\" >0.225130</td>\n",
       "      <td id=\"T_b1cd8_row1_col6\" class=\"data row1 col6\" >0.225130</td>\n",
       "      <td id=\"T_b1cd8_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
       "      <td id=\"T_b1cd8_row1_col8\" class=\"data row1 col8\" >3237.147671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b1cd8_row2_col0\" class=\"data row2 col0\" >fitness_ind_eg_maml_tp_2</td>\n",
       "      <td id=\"T_b1cd8_row2_col1\" class=\"data row2 col1\" >ga</td>\n",
       "      <td id=\"T_b1cd8_row2_col2\" class=\"data row2 col2\" >[1.2286935e+00 1.3370887e-02 8.1635733e+00 1.8336439e-01 9.1679648e-02\n",
       " 1.9692362e+01 1.6210598e-01]</td>\n",
       "      <td id=\"T_b1cd8_row2_col3\" class=\"data row2 col3\" >[0.21573043]</td>\n",
       "      <td id=\"T_b1cd8_row2_col4\" class=\"data row2 col4\" >0.215730</td>\n",
       "      <td id=\"T_b1cd8_row2_col5\" class=\"data row2 col5\" >0.215730</td>\n",
       "      <td id=\"T_b1cd8_row2_col6\" class=\"data row2 col6\" >0.215730</td>\n",
       "      <td id=\"T_b1cd8_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
       "      <td id=\"T_b1cd8_row2_col8\" class=\"data row2 col8\" >8970.158284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b1cd8_row3_col0\" class=\"data row3 col0\" >fitness_ind_eg_maml_tp_2</td>\n",
       "      <td id=\"T_b1cd8_row3_col1\" class=\"data row3 col1\" >gtvga</td>\n",
       "      <td id=\"T_b1cd8_row3_col2\" class=\"data row3 col2\" >[ 1.8597403   0.04081193 12.413825    0.17148681  0.09851877 19.334478\n",
       "  0.268614  ]</td>\n",
       "      <td id=\"T_b1cd8_row3_col3\" class=\"data row3 col3\" >[0.21484125]</td>\n",
       "      <td id=\"T_b1cd8_row3_col4\" class=\"data row3 col4\" >0.214841</td>\n",
       "      <td id=\"T_b1cd8_row3_col5\" class=\"data row3 col5\" >0.214841</td>\n",
       "      <td id=\"T_b1cd8_row3_col6\" class=\"data row3 col6\" >0.214841</td>\n",
       "      <td id=\"T_b1cd8_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
       "      <td id=\"T_b1cd8_row3_col8\" class=\"data row3 col8\" >11042.997091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa8ee27e850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Analysis for fitness_ind_eg_maml_tp_1 using ga and gtvga:\n",
      "P-value: 1.6777609179189534e-182\n",
      "Reject H0: There is a significant difference in the fitness values of the two algorithms.\n",
      "95% Confidence Interval for the difference in means: (0.0011, 0.0012)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Analysis for fitness_ind_eg_maml_tp_2 using ga and gtvga:\n",
      "P-value: 6.357011855780207e-72\n",
      "Reject H0: There is a significant difference in the fitness values of the two algorithms.\n",
      "95% Confidence Interval for the difference in means: (0.0008, 0.0009)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done..!\n"
     ]
    }
   ],
   "source": [
    "def run_experiments(test_functions, optimization_algorithms, bounds_test_functions):\n",
    "    results = []\n",
    "    # for func in test_functions:\n",
    "    for func,bound in zip(test_functions,bounds_test_functions):\n",
    "        func_results = {'function': func.__name__, 'results': []}\n",
    "        for optimizer in optimization_algorithms:\n",
    "            print(func.__name__)\n",
    "            print(bound)\n",
    "            print(optimizer.__name__)\n",
    "            print()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # best_solution, best_fitness, all_solutions = optimizer(func, bounds)\n",
    "            \n",
    "            # return global_best_position, global_best_fitness, particles_position, \\\n",
    "            #        max_gbest_each_iter, mean_gbest_each_iter\n",
    "            \n",
    "            # best_solution_gpu, best_fitness_gpu, all_solutions_gpu, \\\n",
    "            # max_gbest_each_iter_gpu, mean_gbest_each_iter_gpu = optimizer(func, bound)\n",
    "            \n",
    "            best_solution_gpu, best_fitness_gpu, all_solutions_gpu, \\\n",
    "            max_gbest_each_iter_gpu, mean_gbest_each_iter_gpu, \\\n",
    "            final_path_last_Model, final_model, final_config, final_i_ind, final_best_fitness = optimizer(func, bound)\n",
    "            \n",
    "            # final_path_last_Model = f'model_reg_last/model_last_{run}_{final_lossval:.3f}_{filename_last_Model}'\n",
    "            \n",
    "            save_last_model_reptile_checkpoint(final_model, final_path_last_Model+'_'+func.__name__+'_'+optimizer.__name__+'.json')\n",
    "            # save_last_info_params(final_config[\"n_iterations\"], final_config[\"n_data_all\"], final_config[\"n_sample\"], \\\n",
    "            #                       final_config[\"n_train\"], final_config[\"seed\"], final_config[\"inner_step_size\"], \\\n",
    "            #                       final_config[\"inner_epochs\"], final_config[\"outer_stepsize_reptile\"], \\\n",
    "            #                       final_config[\"outer_stepsize_maml\"], final_config[\"run\"], final_config[\"final_lossval\"], \\\n",
    "            #                       final_config[\"filename_last_Model\"])\n",
    "            save_last_info_params(\n",
    "                final_config[\"n_iterations\"], final_config[\"n_data_all\"], final_config[\"n_sample\"],\n",
    "                final_config[\"n_train\"], final_config[\"seed\"], final_config[\"inner_step_size\"],\n",
    "                final_config[\"inner_epochs\"], final_config[\"outer_stepsize_reptile\"],\n",
    "                final_config[\"outer_stepsize_maml\"], final_config[\"run\"], final_config[\"final_lossval\"],\n",
    "                final_config[\"filename_last_Model\"]+'_'+func.__name__+'_'+optimizer.__name__,\n",
    "                final_config[\"path_filename_last_Model\"]+'_'+func.__name__+'_'+optimizer.__name__+'.json'\n",
    "            )\n",
    "            \n",
    "            print(final_config)\n",
    "            print()\n",
    "\n",
    "            \n",
    "            best_solution, best_fitness, all_solutions, \\\n",
    "            max_gbest_each_iter, mean_gbest_each_iter = \\\n",
    "            best_solution_gpu.cpu().numpy().flatten(),\\\n",
    "            best_fitness_gpu.cpu().numpy().flatten(),\\\n",
    "            all_solutions_gpu.cpu().numpy().flatten(),\\\n",
    "            max_gbest_each_iter_gpu.cpu().numpy().flatten(),\\\n",
    "            mean_gbest_each_iter_gpu.cpu().numpy().flatten()\n",
    "            \n",
    "            # my_device = 'cpu'\n",
    "            running_time = time.time() - start_time\n",
    "            # fitness_values_gpu, _, _, _, _  = func(all_solutions_gpu)\n",
    "            fitness_values_gpu = func(all_solutions_gpu)\n",
    "            # fitness_values = fitness_values_gpu.cpu().numpy().flatten()\n",
    "            fitness_values = fitness_values_gpu.cpu().numpy().flatten()\n",
    "\n",
    "            func_results['results'].append({\n",
    "                'algorithm_name': optimizer.__name__,\n",
    "                'best_solution': best_solution,\n",
    "                'best_fitness': best_fitness,\n",
    "                'median_fitness': np.median(fitness_values),\n",
    "                'worst_fitness': np.max(fitness_values),\n",
    "                'mean_fitness': np.mean(fitness_values),\n",
    "                'std_fitness': np.std(fitness_values),\n",
    "                'running_time': running_time,\n",
    "                'max_gbest_each_iter': max_gbest_each_iter,\n",
    "                'mean_gbest_each_iter': mean_gbest_each_iter\n",
    "            })\n",
    "\n",
    "        results.append(func_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "test_functions = [fitness_ind_eg_maml_tp_1, fitness_ind_eg_maml_tp_2]\n",
    "\n",
    "alias_func_name = ['eg_maml_tiny_inv', 'eg_maml_middle_inv']\n",
    "\n",
    "# optimization_algorithms = [ga]\n",
    "optimization_algorithms = [ga, gtvga]\n",
    "# optimization_algorithms = [ga, gtvga, ptvpso]\n",
    "\n",
    "bounds_tp_1 = [\n",
    "        (0, 4),  # Bound untuk seed (misalnya dari 0 sampai 100)\n",
    "        (0.01, 0.1),  # Bound untuk inner_step_size (misalnya dari 0.01 sampai 0.1)\n",
    "        (1, 7),  # Bound untuk inner_epochs (misalnya dari 1 sampai 10)\n",
    "        (0.01, 0.2),  # Bound untuk outer_stepsize_reptile (misalnya dari 0.01 sampai 0.2)\n",
    "        (0.001, 0.05),  # Bound untuk outer_stepsize_maml (misalnya dari 0.001 sampai 0.05)\n",
    "        (1, 10),  # Bound untuk n_iterations (misalnya dari 1 sampai 50)\n",
    "        (0, 1)  # Bound untuk run type (misalnya dari 0 sampai 1) \"E-MAML\" if <= 0.5 else \"E-MAML_Synthetic_E-Reptile\"\n",
    "    ]\n",
    "\n",
    "bounds_tp_2 = [\n",
    "        (0, 8),  # Bound untuk seed (misalnya dari 0 sampai 100)\n",
    "        (0.001, 0.2),  # Bound untuk inner_step_size (misalnya dari 0.01 sampai 0.1)\n",
    "        (1, 14),  # Bound untuk inner_epochs (misalnya dari 1 sampai 10)\n",
    "        (0.001, 0.4),  # Bound untuk outer_stepsize_reptile (misalnya dari 0.01 sampai 0.2)\n",
    "        (0.0001, 0.1),  # Bound untuk outer_stepsize_maml (misalnya dari 0.001 sampai 0.05)\n",
    "        (1, 20),  # Bound untuk n_iterations (misalnya dari 1 sampai 50)\n",
    "        (0, 1)  # Bound untuk run type (misalnya dari 0 sampai 1) \"E-MAML\" if <= 0.5 else \"E-MAML_Synthetic_E-Reptile\"\n",
    "    ]\n",
    "\n",
    "\n",
    "bounds_test_functions = [bounds_tp_1, bounds_tp_2]\n",
    "\n",
    "# results = run_experiments(test_functions, optimization_algorithms, bounds)\n",
    "results = run_experiments(test_functions, optimization_algorithms, bounds_test_functions)\n",
    "\n",
    "# Menampilkan hasil dalam bentuk tabel\n",
    "def display_results_table(results):\n",
    "    print(\"\\nResults:\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"| Function           | Algorithm      | Best Solution                   | Best Fitness | Median Fitness | Worst Fitness | Mean Fitness | Std Fitness | Running Time (s) |\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    for func_result in results:\n",
    "        for result in func_result['results']:\n",
    "            print(f\"| {func_result['function']:<20} | {result['algorithm_name']:<15} | {result['best_solution']} | \"\n",
    "                  f\"{result['best_fitness']} | {result['median_fitness']:<15} | {result['worst_fitness']:<13} | \"\n",
    "                  f\"{result['mean_fitness']:<12} | {result['std_fitness']:<11} | {result['running_time']:<17.5f} |\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "def display_results_table_df(results):\n",
    "    # Membuat list untuk menampung data\n",
    "    data = []\n",
    "    \n",
    "    # Loop melalui setiap fungsi hasil\n",
    "    for func_result in results:\n",
    "        for result in func_result['results']:\n",
    "            # Menambahkan data dalam bentuk dictionary ke dalam list\n",
    "            data.append({\n",
    "                'Function': func_result['function'],\n",
    "                'Algorithm': result['algorithm_name'],\n",
    "                'Best Solution': result['best_solution'],\n",
    "                'Best Fitness': result['best_fitness'],\n",
    "                'Median Fitness': result['median_fitness'],\n",
    "                'Worst Fitness': result['worst_fitness'],\n",
    "                'Mean Fitness': result['mean_fitness'],\n",
    "                'Std Fitness': result['std_fitness'],\n",
    "                'Running Time (s)': result['running_time']\n",
    "            })\n",
    "    \n",
    "    # Membuat DataFrame dari data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Menampilkan DataFrame\n",
    "    print(\"\\nResults:\")\n",
    "    # display(df)\n",
    "    # display(df.style.hide_index())\n",
    "    display(df.style.hide(axis='index'))\n",
    "    \n",
    "    # return df\n",
    "\n",
    "# Function to save results to CSV, Excel, and JSON files\n",
    "def save_results_to_file(results, file_format, path_to_save=None):\n",
    "    for func_result in results:\n",
    "        df = pd.DataFrame(func_result['results'])\n",
    "        \n",
    "        # Define file path based on function name and file format\n",
    "        if path_to_save is None:\n",
    "            filename = f\"{func_result['function']}_{file_format}_results\"\n",
    "        else:\n",
    "            filename = f\"{path_to_save}/{func_result['function']}_{file_format}_results\"\n",
    "        \n",
    "        # Save in CSV, Excel, and JSON formats\n",
    "        df.to_csv(f\"{filename}.csv\", index=False)\n",
    "        df.to_excel(f\"{filename}.xlsx\", index=False)\n",
    "        df.to_json(f\"{filename}.json\", orient=\"records\", indent=4)\n",
    "\n",
    "# Function to create a chart and save it to a PDF file\n",
    "def save_chart_to_pdf(results,path_to_save=None):\n",
    "    if path_to_save == None:\n",
    "        pdf_pages = PdfPages(\"chart_fitness.pdf\")\n",
    "    else:\n",
    "        pdf_pages = PdfPages(path_to_save+\"/chart_fitness.pdf\")        \n",
    "        \n",
    "    # patterns = [ \"/\" , \"\\\\\" , \"|\" , \"-\" , \"+\" , \"x\", \"o\", \"O\", \".\", \"*\" ]\n",
    "    patterns = [ \"\\\\\" , \".\" , \"|\" , \"-\" , \"+\" , \"x\", \"o\", \"O\", \"/\", \"*\" ]\n",
    "\n",
    "    # ax1 = fig.add_subplot(111)\n",
    "    # for i in range(len(patterns)):\n",
    "    #     ax1.bar(i, 3, color='green', edgecolor='black', hatch=patterns[i])\n",
    "\n",
    "    nama_all_alg = [results[0]['results'][i]['algorithm_name'] for i in range(len(results[0]['results']))]\n",
    "    len_nama_all_alg = len(nama_all_alg)\n",
    "    # print(nama_all_alg)\n",
    "    \n",
    "    # untuk plotting hasil nilai fitness final:\n",
    "    # ----------------------------------\n",
    "    for func_result in results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        for j in range(len_nama_all_alg):\n",
    "            # print(j)\n",
    "            plt.bar(func_result['results'][j]['algorithm_name'], \\\n",
    "                    func_result['results'][j]['best_fitness'], \\\n",
    "                    label='Best Fitness', edgecolor='white', \\\n",
    "                    hatch=patterns[-j])\n",
    "        \n",
    "        \n",
    "        plt.title(f\"Best Fitness for {func_result['function']}\")\n",
    "        plt.xlabel(\"Algorithm\")\n",
    "        plt.ylabel(\"Fitness Value\")\n",
    "        plt.legend()\n",
    "        pdf_pages.savefig()\n",
    "        # pdf_pages.savefig(bbox_inches='tight', dpi=1000)\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    # untuk plotting proses pergerakan konvergensi dari hasil nilai fitness final:\n",
    "    # ----------------------------------\n",
    "    for func_result in results:\n",
    "        # fig, ax = plt.figure(figsize=(10, 6))\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        number_color = len_nama_all_alg\n",
    "        # cmap = plt.get_cmap('gnuplot')\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        colors = [cmap(i) for i in np.linspace(0, 1, number_color)]\n",
    "        \n",
    "        for j in range(len_nama_all_alg):\n",
    "            plt.plot(np.arange(1,num_iterations_all+1), func_result['results'][j]['max_gbest_each_iter'], color = colors[j], label=func_result['results'][j]['algorithm_name'].upper())      \n",
    "        \n",
    "        # plt.add_subplot(111).set_xticks(arange(1,3,0.5)) # You can actually compute the interval You need - and substitute here\n",
    "        # ax.set_xticks(arange(1,num_iterations_all+1,1)) # You can actually compute the interval You need - and substitute here\n",
    "        plt.title(f\"Convergence of Max. Fitness Value for {func_result['function']}\")\n",
    "        plt.xlabel(\"Number of Iteration\")\n",
    "        plt.ylabel(\"Fitness Value\")\n",
    "        plt.legend()\n",
    "        pdf_pages.savefig()\n",
    "        # pdf_pages.savefig(bbox_inches='tight', dpi=1000)\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        for j in range(len_nama_all_alg):\n",
    "            plt.plot(np.arange(1,num_iterations_all+1),func_result['results'][j]['mean_gbest_each_iter'],color = colors[j],label=func_result['results'][j]['algorithm_name'].upper())\n",
    "        \n",
    "        \n",
    "        plt.title(f\"Convergence of Mean Fitness Value for {func_result['function']}\")\n",
    "        plt.xlabel(\"Number of Iteration\")\n",
    "        plt.ylabel(\"Fitness Value\")\n",
    "        plt.legend()\n",
    "        pdf_pages.savefig()\n",
    "        # pdf_pages.savefig(bbox_inches='tight', dpi=1000)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    pdf_pages.close()\n",
    "    \n",
    "# Analisis statistik p-values antar algoritma\n",
    "# def statistical_analysis_with_visualization(results, suffix, folder_name):\n",
    "#     for i in range(len(results)):\n",
    "#         function_name = results[i]['function']\n",
    "#         algorithm_results = results[i]['results']\n",
    "        \n",
    "#         # Create a dictionary to hold fitness values\n",
    "#         fitness_data = {}\n",
    "        \n",
    "#         # Collect max_gbest_each_iter for each algorithm\n",
    "#         for algorithm in algorithm_results:\n",
    "#             algorithm_name = algorithm['algorithm_name']\n",
    "#             fitness_data[algorithm_name] = algorithm['max_gbest_each_iter']  # or use mean_gbest_each_iter\n",
    "\n",
    "#         # Perform pairwise statistical analysis between algorithms\n",
    "#         algorithm_names = list(fitness_data.keys())\n",
    "#         for j in range(len(algorithm_names)):\n",
    "#             for k in range(j + 1, len(algorithm_names)):\n",
    "#                 algorithm1 = algorithm_names[j]\n",
    "#                 algorithm2 = algorithm_names[k]\n",
    "                \n",
    "#                 fitness_values1 = fitness_data[algorithm1]\n",
    "#                 fitness_values2 = fitness_data[algorithm2]\n",
    "\n",
    "#                 # Check for enough data points and variability\n",
    "#                 if len(fitness_values1) < 2 or len(fitness_values2) < 2:\n",
    "#                     print(f\"Not enough data to perform t-test for {function_name} using {algorithm1} and {algorithm2}.\")\n",
    "#                     continue\n",
    "                \n",
    "#                 std1, std2 = np.std(fitness_values1, ddof=1), np.std(fitness_values2, ddof=1)\n",
    "\n",
    "#                 if std1 == 0 or std2 == 0:\n",
    "#                     print(f\"Insufficient variability in fitness values for {function_name} using {algorithm1} and {algorithm2}. Skipping t-test.\")\n",
    "#                     continue\n",
    "\n",
    "#                 # Perform t-test\n",
    "#                 t_stat, p_value = ttest_ind(fitness_values1, fitness_values2)\n",
    "#                 print(f\"\\nStatistical Analysis for {function_name} using {algorithm1} and {algorithm2}:\")\n",
    "#                 print(f\"P-value: {p_value}\")\n",
    "\n",
    "#                 # Define null hypothesis\n",
    "#                 H0 = \"There is no significant difference in the fitness values of the two algorithms.\"\n",
    "#                 H1 = \"There is a significant difference in the fitness values of the two algorithms.\"\n",
    "                \n",
    "#                 if p_value < 0.05:  # 95% confidence level\n",
    "#                     print(\"Reject H0:\", H1)\n",
    "#                 else:\n",
    "#                     print(\"Fail to reject H0:\", H0)\n",
    "\n",
    "#                 # Calculate means and standard deviations\n",
    "#                 mean1, mean2 = np.mean(fitness_values1), np.mean(fitness_values2)\n",
    "#                 std1, std2 = np.std(fitness_values1, ddof=1), np.std(fitness_values2, ddof=1)\n",
    "                \n",
    "#                 # Calculate confidence interval\n",
    "#                 conf_interval = 1.96 * np.sqrt((std1**2 / len(fitness_values1)) + (std2**2 / len(fitness_values2)))\n",
    "#                 mean_diff = mean1 - mean2\n",
    "#                 ci_lower = mean_diff - conf_interval\n",
    "#                 ci_upper = mean_diff + conf_interval\n",
    "\n",
    "#                 print(f\"95% Confidence Interval for the difference in means: ({ci_lower:.4f}, {ci_upper:.4f})\")\n",
    "\n",
    "#                 # Visualization\n",
    "#                 x = np.linspace(-1, 1, 1000)\n",
    "#                 y1 = norm.pdf(x, mean1, std1)\n",
    "#                 y2 = norm.pdf(x, mean2, std2)\n",
    "\n",
    "#                 plt.figure(figsize=(10, 6))\n",
    "#                 plt.plot(x, y1, label=f'{algorithm1} (Mean: {mean1:.4f})', color='blue')\n",
    "#                 plt.plot(x, y2, label=f'{algorithm2} (Mean: {mean2:.4f})', color='red')\n",
    "\n",
    "#                 # Shade the confidence interval\n",
    "#                 plt.fill_betweenx(y1, ci_lower, ci_upper, where=(x >= ci_lower) & (x <= ci_upper), color='lightblue', alpha=0.5, label='95% Confidence Interval')\n",
    "                \n",
    "#                 plt.title(f'Distribution of Fitness Values for {function_name}\\n{algorithm1} vs {algorithm2}')\n",
    "#                 plt.xlabel('Fitness Value')\n",
    "#                 plt.ylabel('Probability Density')\n",
    "#                 plt.legend()\n",
    "#                 plt.grid()\n",
    "                \n",
    "#                  # Save the plot as PDF and PNG\n",
    "#                 plt.savefig(f\"{folder_name}/{function_name}_{algorithm1}_{algorithm2}_{suffix}.pdf\")\n",
    "#                 plt.savefig(f\"{folder_name}/{function_name}_{algorithm1}_{algorithm2}_{suffix}.png\")\n",
    "                \n",
    "#                 plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "def statistical_analysis_with_visualization(results, suffix, folder_name):\n",
    "    for i in range(len(results)):\n",
    "        function_name = results[i]['function']\n",
    "        algorithm_results = results[i]['results']\n",
    "        \n",
    "        # Create a dictionary to hold fitness values\n",
    "        fitness_data = {}\n",
    "        \n",
    "        # Collect max_gbest_each_iter for each algorithm\n",
    "        noise=1e-8 # small noise add to a force t-test\n",
    "        for algorithm in algorithm_results:\n",
    "            algorithm_name = algorithm['algorithm_name']\n",
    "            # fitness_data[algorithm_name] = algorithm['max_gbest_each_iter']  # or use mean_gbest_each_iter\n",
    "            fitness_data[algorithm_name] = algorithm['max_gbest_each_iter'] + noise * np.random.randn(len(algorithm['max_gbest_each_iter']))\n",
    "\n",
    "        # Perform pairwise statistical analysis between algorithms\n",
    "        algorithm_names = list(fitness_data.keys())\n",
    "        for j in range(len(algorithm_names)):\n",
    "            for k in range(j + 1, len(algorithm_names)):\n",
    "                algorithm1 = algorithm_names[j]\n",
    "                algorithm2 = algorithm_names[k]\n",
    "                \n",
    "                fitness_values1 = fitness_data[algorithm1]\n",
    "                fitness_values2 = fitness_data[algorithm2]\n",
    "\n",
    "                # Check for enough data points and variability\n",
    "                if len(fitness_values1) < 2 or len(fitness_values2) < 2:\n",
    "                    print(f\"Not enough data to perform t-test for {function_name} using {algorithm1} and {algorithm2}.\")\n",
    "                    continue\n",
    "                \n",
    "                std1, std2 = np.std(fitness_values1, ddof=1), np.std(fitness_values2, ddof=1)\n",
    "                # Check for low variability (close to identical values)\n",
    "                if np.abs(np.mean(fitness_values1) - np.mean(fitness_values2)) < 1e-6 or std1 == 0 or std2 == 0:\n",
    "                    print(f\"Insufficient variability in fitness values for {function_name} using {algorithm1} and {algorithm2}. Skipping t-test.\")\n",
    "                    continue\n",
    "\n",
    "                # Perform t-test\n",
    "                t_stat, p_value = ttest_ind(fitness_values1, fitness_values2, equal_var=False)\n",
    "                print(f\"\\nStatistical Analysis for {function_name} using {algorithm1} and {algorithm2}:\")\n",
    "                print(f\"P-value: {p_value}\")\n",
    "\n",
    "                # Define null hypothesis\n",
    "                H0 = \"There is no significant difference in the fitness values of the two algorithms.\"\n",
    "                H1 = \"There is a significant difference in the fitness values of the two algorithms.\"\n",
    "                \n",
    "                if p_value < 0.05:  # 95% confidence level\n",
    "                    print(\"Reject H0:\", H1)\n",
    "                else:\n",
    "                    print(\"Fail to reject H0:\", H0)\n",
    "\n",
    "                # Calculate means and standard deviations\n",
    "                mean1, mean2 = np.mean(fitness_values1), np.mean(fitness_values2)\n",
    "                \n",
    "                # Calculate confidence interval\n",
    "                conf_interval = 1.96 * np.sqrt((std1**2 / len(fitness_values1)) + (std2**2 / len(fitness_values2)))\n",
    "                mean_diff = mean1 - mean2\n",
    "                ci_lower = mean_diff - conf_interval\n",
    "                ci_upper = mean_diff + conf_interval\n",
    "\n",
    "                print(f\"95% Confidence Interval for the difference in means: ({ci_lower:.4f}, {ci_upper:.4f})\")\n",
    "\n",
    "                # Visualization\n",
    "                x = np.linspace(min(mean1, mean2) - 3*max(std1, std2), max(mean1, mean2) + 3*max(std1, std2), 1000)\n",
    "                y1 = norm.pdf(x, mean1, std1)\n",
    "                y2 = norm.pdf(x, mean2, std2)\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(x, y1, label=f'{algorithm1} (Mean: {mean1:.4f})', color='blue')\n",
    "                plt.plot(x, y2, label=f'{algorithm2} (Mean: {mean2:.4f})', color='red')\n",
    "\n",
    "                # Shade the confidence interval\n",
    "                plt.fill_betweenx(y1, ci_lower, ci_upper, where=(x >= ci_lower) & (x <= ci_upper), color='lightblue', alpha=0.5, label='95% Confidence Interval')\n",
    "                \n",
    "                plt.title(f'Distribution of Fitness Values for {function_name}\\n{algorithm1} vs {algorithm2}')\n",
    "                plt.xlabel('Fitness Value')\n",
    "                plt.ylabel('Probability Density')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "                \n",
    "                 # Save the plot as PDF and PNG\n",
    "                plt.savefig(f\"{folder_name}/{function_name}_{algorithm1}_{algorithm2}_{suffix}.pdf\")\n",
    "                plt.savefig(f\"{folder_name}/{function_name}_{algorithm1}_{algorithm2}_{suffix}.png\")\n",
    "                \n",
    "                plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Menyimpan hasil ke file\n",
    "def save_results(results, folder_name):\n",
    "    save_results_to_file(results, 'final', folder_name)\n",
    "    save_chart_to_pdf(results, folder_name)\n",
    "\n",
    "# Memproses log fitness\n",
    "def process_fitness_log(results, alias_func_name):\n",
    "    counter_alias = 0\n",
    "    log_fitness = ''\n",
    "    for func_result in results:\n",
    "        temp_log_fitness = ''\n",
    "        temp_val_fitness = 0\n",
    "        temp_algo_name = ''\n",
    "        for idx, result in enumerate(func_result['results']):\n",
    "            if idx == 0:\n",
    "                temp_val_fitness = result['best_fitness'].item()\n",
    "                temp_algo_name = result['algorithm_name']\n",
    "            else:\n",
    "                # jika nilai fitness berupa nilai loss, maka gunakan comparasi if temp_val_fitness < result['best_fitness']:\n",
    "                # tetapi jika nilai fitness berupa nilai yang profit atau invers dari loss, misal akurasi, gunakan > \n",
    "                if temp_val_fitness < result['best_fitness']:\n",
    "                    temp_log_fitness = f\"{alias_func_name[counter_alias]}-{temp_algo_name}-{temp_val_fitness:.2f}\"\n",
    "                else:\n",
    "                    temp_val_fitness = result['best_fitness'].item()\n",
    "                    temp_algo_name = result['algorithm_name']\n",
    "                    temp_log_fitness = f\"{alias_func_name[counter_alias]}-{temp_algo_name}-{temp_val_fitness:.2f}\"\n",
    "        log_fitness += ('--' if counter_alias else '') + temp_log_fitness\n",
    "        counter_alias += 1\n",
    "    return log_fitness\n",
    "\n",
    "# Menyimpan hasil dalam folder\n",
    "def create_save_folder(device, log_fitness, num_iterations, pop_size):\n",
    "    info_param = f\"{device}-{log_fitness}-{len(optimization_algorithms)}alg-it-{num_iterations}-ps-{pop_size}\"\n",
    "    timestamp = datetime.now(pytz.timezone('Asia/Jakarta')).strftime('%d-%m-%Y-%H-%M-%S')\n",
    "    path = f\"./log results/{info_param}-{timestamp}\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path + '/'\n",
    "\n",
    "# Menginisialisasi variabel dan menjalankan proses\n",
    "log_fitness = process_fitness_log(results, alias_func_name)\n",
    "save_folder = create_save_folder(my_device, log_fitness, num_iterations_all, pop_size_all)\n",
    "\n",
    "# Tampilkan hasil dan simpan\n",
    "# display_results_table(results)\n",
    "display_results_table_df(results)\n",
    "statistical_analysis_with_visualization(results, 'final_stat_viz', save_folder)\n",
    "save_results(results, save_folder)\n",
    "\n",
    "print(\"Done..!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-MAML to EG-MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Meta-Learning: E-MAML\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [0 2]\n",
      "\n",
      "=======================\n",
      "dpinds = [1 4 3]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [1 3]\n",
      "\n",
      "=======================\n",
      "dpinds = [4 0 2]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [3 1]\n",
      "\n",
      "=======================\n",
      "dpinds = [2 4 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [4 2]\n",
      "\n",
      "=======================\n",
      "dpinds = [3 1 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [2 3]\n",
      "\n",
      "=======================\n",
      "dpinds = [4 1 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [2 4]\n",
      "\n",
      "=======================\n",
      "dpinds = [1 3 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [3 1]\n",
      "\n",
      "=======================\n",
      "dpinds = [4 2 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [0 4]\n",
      "\n",
      "=======================\n",
      "dpinds = [2 1 3]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [4 2]\n",
      "\n",
      "=======================\n",
      "dpinds = [3 1 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "mbinds = [1 4]\n",
      "\n",
      "=======================\n",
      "dpinds = [2 3 0]\n",
      "\n",
      "\n",
      "final loss on last model = 0.247\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd as ag\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "# declare params\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "hidden_layers = [100, 50, 25, 12, 6, 3]\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "def experiment(run, plot=True):\n",
    "    print('Type of Meta-Learning:', run)\n",
    "    seed = 0\n",
    "    inner_step_size = 0.02  # stepsize in inner SGD\n",
    "    inner_epochs = 1  # number of epochs of each inner SGD\n",
    "    outer_stepsize_reptile = 0.1  # stepsize of outer optimization, i.e., meta-optimization\n",
    "    outer_stepsize_maml = 0.01\n",
    "    # n_iterations = 30000  # number of outer updates; each iteration we sample one task and update on it\n",
    "    n_iterations = 10  # number of outer updates; each iteration we sample one task and update on it\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Define task distribution\n",
    "    n_data_all = 5\n",
    "    n_sample = n_data_all # minimum 1, maks = n_data_all\n",
    "    idx_x_all = np.arange(0,n_data_all)[:,None]\n",
    "    \n",
    "    # All of the x points data, dengan fitur input pepanjang n_input = 14\n",
    "    x_all = get_data_test(np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None])\n",
    "    # n_train = 10  # Size of training minibatches\n",
    "    n_train = 3  # Size of training minibatches, harus < n_data_all\n",
    "    \n",
    "    # info_params = f\"imax-{n_iterations}-ndata-{n_data_all}-nspl-{n_sample}-ntrain-{n_train}-s-{seed}-iss-{inner_step_size}-ie-{inner_epochs}-osr-{outer_stepsize_reptile}-osm-{outer_stepsize_maml}\"\n",
    "       \n",
    "    def get_mse_or_loss_val(get_idx_x_all_in):\n",
    "        x = to_torch(np.array([x_all[i] for i in get_idx_x_all_in]))\n",
    "        y = to_torch(np.array([y_all[i] for i in get_idx_x_all_in]))\n",
    "\n",
    "        # cara 1\n",
    "        model.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        individual_losses = (y_pred - y).pow(2).mean(dim=1)  # Loss for each sample along feature dimension\n",
    "        # print(\"Individual losses:\", individual_losses)\n",
    "\n",
    "        return individual_losses.data.numpy()\n",
    "    \n",
    "    def get_idx_x_all(x_all_in, x_all_to_search):\n",
    "        idx_result = []\n",
    "\n",
    "        # Iterate through each element in x_all_in\n",
    "        for x_in in x_all_in:\n",
    "            # Iterate through x_all_to_search to find a matching element\n",
    "            for i, x in enumerate(x_all_to_search):\n",
    "                # Use np.array_equal to compare arrays element-wise\n",
    "                if np.array_equal(x_in, x):\n",
    "                    idx_result.append(i)\n",
    "                    break  # Stop after the first match is found\n",
    "\n",
    "        return idx_result\n",
    "\n",
    "    def gen_task_eg_maml_base_idx_data():\n",
    "        # elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        # load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "        \n",
    "        elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda idx_x: np.array(\n",
    "            [test_single_data_return_pred(elm_model_n_hidden_layers, get_data_test(idx_x_single[0]))\n",
    "             for idx_x_single in idx_x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task_eg_maml_base_val_data():\n",
    "        # elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        # load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "        \n",
    "        elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task(): # sama dengan gen_task_eg_maml_base_val_data()\n",
    "        \n",
    "        elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "\n",
    "    # Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "    # ==========\n",
    "    # nn.Sequential: Mudah dan cocok untuk deep learning sederhana yang berurutan, \n",
    "    # tidak cocok jika arsitektur membutuhkan koneksi yang kompleks.\n",
    "    ## -------\n",
    "    # nn.Linear dalam Kelas nn.Module: Lebih fleksibel dan dapat digunakan untuk arsitektur kompleks, \n",
    "    # yang melibatkan banyak hidden layer, skip connections, atau jalur paralel.\n",
    "    \n",
    "    # Custom activation function NRReLU\n",
    "    def NRReLU(x):\n",
    "        return 1 / (torch.exp(-x) - torch.exp(x))\n",
    "\n",
    "    activations = [NRReLU, nn.Sigmoid(), nn.Tanh(), nn.ReLU()]  # Custom activations, including NRReLU\n",
    "    \n",
    "    # Define a function to create the model with configurable layers and activation functions\n",
    "    def define_model_type1(n_input, n_hidden_layers, n_output, activations=None):\n",
    "        layers = []\n",
    "        input_dim = n_input\n",
    "\n",
    "        # Ensure activations list matches the number of hidden layers, or use ReLU as default\n",
    "        if activations is None:\n",
    "            activations = [F.relu] * len(n_hidden_layers)  # Default to ReLU for all layers\n",
    "        elif len(activations) != len(n_hidden_layers):\n",
    "            raise ValueError(\"Length of activations must match number of hidden layers\")\n",
    "\n",
    "        # Add each hidden layer with the specified number of neurons and activation\n",
    "        for hidden_units, activation in zip(n_hidden_layers, activations):\n",
    "            layers.append(nn.Linear(input_dim, hidden_units))\n",
    "            input_dim = hidden_units  # Update input_dim for the next layer\n",
    "\n",
    "            # Add the activation layer as a callable function\n",
    "            layers.append(activation)  # Add the activation function directly\n",
    "\n",
    "        # Add the final output layer without activation\n",
    "        layers.append(nn.Linear(input_dim, n_output))\n",
    "\n",
    "        # Create the model with nn.Sequential\n",
    "        model = nn.Sequential(*layers)\n",
    "        return model\n",
    "    \n",
    "    def define_model_type2(n_input, n_hidden_layers, n_output):\n",
    "        #     model = nn.Sequential(\n",
    "        #         nn.Linear(1, 64),\n",
    "        #         nn.Tanh(),\n",
    "        #         nn.Linear(64, 64),\n",
    "        #         nn.Tanh(),\n",
    "        #         nn.Linear(64, 1),\n",
    "        #     )\n",
    "\n",
    "        layers = []\n",
    "        input_dim = n_input\n",
    "\n",
    "        # Add each hidden layer with alternating activation functions\n",
    "        for i, hidden_units in enumerate(n_hidden_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_units))\n",
    "\n",
    "            # Use ReLU for the first layer, Tanh for others\n",
    "            if i % 2 == 0:\n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                layers.append(nn.Tanh())\n",
    "\n",
    "            input_dim = hidden_units  # Update input_dim for the next layer\n",
    "\n",
    "        # Add the final output layer\n",
    "        layers.append(nn.Linear(input_dim, n_output))\n",
    "\n",
    "        # Create the model with nn.Sequential\n",
    "        model = nn.Sequential(*layers)\n",
    "        return model\n",
    "    \n",
    "    # Define sintesis model. Reptile dengan ELMRegressionForReptile - nn.Linear\n",
    "    model = ModelForSyntheticReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        \n",
    "    def save_model_reptile_checkpoint(model, filename):\n",
    "        # Dapatkan state_dict dari model\n",
    "        model_state = model.state_dict()\n",
    "\n",
    "        # Konversi tensor menjadi list untuk serialisasi JSON\n",
    "        model_state_serializable = {k: v.numpy().tolist() for k, v in model_state.items()}\n",
    "\n",
    "        # Simpan model ke file JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(model_state_serializable, f)\n",
    "\n",
    "    def load_model_reptile_checkpoint(model, filename):\n",
    "        # Muat model dari file JSON\n",
    "        with open(filename, 'r') as f:\n",
    "            model_state_serializable = json.load(f)\n",
    "\n",
    "        # Konversi kembali dari list ke tensor\n",
    "        model_state = {k: torch.tensor(np.array(v)) for k, v in model_state_serializable.items()}\n",
    "\n",
    "        # Memuat state_dict ke model\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()  # Set model ke mode evaluasi\n",
    "\n",
    "    def to_torch(x):\n",
    "        return ag.Variable(torch.Tensor(x))\n",
    "\n",
    "    # def train_on_batch(x, y):\n",
    "    #     x = to_torch(x)\n",
    "    #     y = to_torch(y)\n",
    "    #     model.zero_grad()\n",
    "    #     ypred = model(x)\n",
    "    #     loss = (ypred - y).pow(2).mean()\n",
    "    #     loss.backward()\n",
    "    #     for param in model.parameters():\n",
    "    #         param.data -= inner_step_size * param.grad.data\n",
    "    \n",
    "               \n",
    "    # using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "    def train_on_batch_eg_maml(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "            \n",
    "    def train_on_batch(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "        \n",
    "    # Cara Memuat model dari checkpoint\n",
    "    #     try:\n",
    "    #         load_model_reptile_checkpoint(model, filename_ckpt)\n",
    "    #         print(\"Model berhasil dimuat dari:\", filename_ckpt)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Terjadi kesalahan saat memuat model:\", e)\n",
    "\n",
    "    #     # Sekarang Anda bisa menggunakan model untuk melakukan prediksi atau melanjutkan pelatihan\n",
    "    #     # Contoh prediksi\n",
    "    #     x_test = to_torch([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]])  # Ganti dengan data yang sesuai\n",
    "    #     model.eval()  # Set model ke mode evaluasi\n",
    "    #     with torch.no_grad():\n",
    "    #         prediction = model(x_test)\n",
    "    #         print(\"Hasil prediksi:\", prediction.numpy())\n",
    "\n",
    "    def predict(x):\n",
    "        x = to_torch(x)\n",
    "        return model(x).data.numpy()\n",
    "    \n",
    "    def save_info_params(\n",
    "        n_iterations, n_data_all, n_sample, n_train, seed, inner_step_size,\n",
    "        inner_epochs, outer_stepsize_reptile, outer_stepsize_maml,\n",
    "        run, final_lossval, filename_last_Model\n",
    "    ):\n",
    "        # Construct the info_params dictionary\n",
    "        info_params = {\n",
    "            \"imax\": n_iterations,\n",
    "            \"ndata\": n_data_all,\n",
    "            \"nspl\": n_sample,\n",
    "            \"ntrain\": n_train,\n",
    "            \"s\": seed,\n",
    "            \"iss\": inner_step_size,\n",
    "            \"ie\": inner_epochs,\n",
    "            \"osr\": outer_stepsize_reptile,\n",
    "            \"run\": run,\n",
    "            \"osm\": outer_stepsize_maml,\n",
    "            \"final_lossval\": float(final_lossval)\n",
    "        }\n",
    "\n",
    "        # Construct the filename with all the specified information\n",
    "        filename = (\n",
    "            f\"model_reg_last/model_params_last_{run}_{final_lossval:.3f}_\"\n",
    "            f\"{filename_last_Model}.json\"\n",
    "        )\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "        # Save the info_params dictionary to the file as JSON\n",
    "        with open(filename, 'w') as json_file:\n",
    "            json.dump(info_params, json_file, indent=4)\n",
    "\n",
    "        # print(f\"Parameters saved to {filename}\")\n",
    "        # return filename\n",
    "\n",
    "    # Choose a fixed task and minibatch for visualization\n",
    "    f_plot = gen_task()\n",
    "    # xtrain_plot = x_all[rng.choice(len(x_all), size=n_train)]\n",
    "    xtrain_plot = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])\n",
    "\n",
    "    # plt.cla()\n",
    "    # Set figure and axis properties\n",
    "    # plt.figure()\n",
    "    \n",
    "    # Set gray background color\n",
    "    # plt.gca().set_facecolor('#f0f0f0')  # Light gray color for the background\n",
    "    # plt.gca().set_facecolor('lightgray')\n",
    "    \n",
    "    # Add grid lines\n",
    "    # plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    filename_first_n_last_Loss = datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    for iteration in range(n_iterations):\n",
    "        weights_before = deepcopy(model.state_dict())\n",
    "\n",
    "        # Generate task\n",
    "        f = gen_task()\n",
    "        y_all = f(x_all)\n",
    "\n",
    "        # Do SGD on this task\n",
    "        inds = rng.permutation(len(x_all))\n",
    "        train_ind = inds[:-1 * n_train]\n",
    "        val_ind = inds[-1 * n_train:]       # Val contains 1/5th of the gt model (com. model)\n",
    "\n",
    "        for _ in range(inner_epochs):\n",
    "            for start in range(0, len(train_ind), n_train):\n",
    "                mbinds = train_ind[start:start + n_train]\n",
    "                print('mbinds =', mbinds)\n",
    "                print()\n",
    "                # print('x_all[mbinds] =', x_all[mbinds])\n",
    "                # print()\n",
    "                # print('y_all[mbinds] =', y_all[mbinds])\n",
    "                x_all_mbinds = np.array([x_all[i] for i in mbinds])\n",
    "                y_all_mbinds = np.array([y_all[i] for i in mbinds])\n",
    "                # train_on_batch(x_all[mbinds], y_all[mbinds])\n",
    "                train_on_batch(x_all_mbinds, y_all_mbinds)\n",
    "                \n",
    "                print('=======================')\n",
    "\n",
    "        if run == 'E-MAML':\n",
    "            outer_step_size = outer_stepsize_maml * (1 - iteration / n_iterations)  # linear schedule\n",
    "            for start in range(0, len(val_ind), n_train):\n",
    "                dpinds = val_ind[start:start + n_train]\n",
    "                print('dpinds =', dpinds)\n",
    "                print()\n",
    "                \n",
    "                # x = to_torch(x_all[dpinds])\n",
    "                x = to_torch(np.array([x_all[i] for i in dpinds]))\n",
    "                \n",
    "                # y = to_torch(y_all[dpinds])\n",
    "                y = to_torch(np.array([y_all[i] for i in dpinds]))\n",
    "\n",
    "                # Compute the grads\n",
    "                model.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = (y_pred - y).pow(2).mean()\n",
    "                loss.backward()\n",
    "                \n",
    "\n",
    "                # Reload the model\n",
    "                model.load_state_dict(weights_before)\n",
    "\n",
    "                # SGD on the params\n",
    "                for param in model.parameters():\n",
    "                    param.data -= outer_step_size * param.grad.data\n",
    "            # print(weights_before)\n",
    "        else:\n",
    "            # Interpolate between current weights and trained weights from this task\n",
    "            # I.e. (weights_before - weights_after) is the meta-gradient\n",
    "            weights_after = model.state_dict()\n",
    "            outerstepsize = outer_stepsize_reptile * (1 - iteration / n_iterations)  # linear schedule\n",
    "            model.load_state_dict({name: weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize\n",
    "                                   for name in weights_before})\n",
    "\n",
    "        # Periodically plot the results on a particular task and minibatch\n",
    "        # if (plot and ((iteration == 0) or ((iteration + 1) % 1000 == 0))):\n",
    "        if (plot and ((iteration == 0) or ((iteration + 1) % n_iterations == 0))):\n",
    "            plt.cla()\n",
    "            # plt.cla()\n",
    "            \n",
    "            # Set gray background color\n",
    "            # plt.gca().set_facecolor('#f0f0f0')  # Light gray color for the background\n",
    "            \n",
    "            # Set gray background color\n",
    "            # plt.gca().set_facecolor('#f0f0f0')  # Light gray color for the background\n",
    "            plt.gca().set_facecolor('lightgray')\n",
    "\n",
    "            # Add grid lines\n",
    "            plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "            \n",
    "            f = f_plot\n",
    "            weights_before = deepcopy(model.state_dict())  # save snapshot before evaluation\n",
    "            \n",
    "            # plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0, 0, 1))\n",
    "            get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "            get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "            plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"pred after 0\", color=(0, 0, 1))\n",
    "            \n",
    "            for inneriter in range(32):\n",
    "                train_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "                if (inneriter + 1) % 8 == 0:\n",
    "                    frac = (inneriter + 1) / 32\n",
    "                    # plt.plot(x_all, predict(x_all), label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "                    \n",
    "                    get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "                    get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "                    plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "            \n",
    "            # plt.plot(x_all, f(x_all), label=\"true\", color=(0, 1, 0))\n",
    "            # plt.plot(x_all, f(x_all), label=\"ground truth from sin(x)\", color=(0, 1, 0))\n",
    "            \n",
    "            get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "            get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "            plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"ground truth from comb. model\", color=(0, 1, 0))\n",
    "            \n",
    "            \n",
    "            lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "            # plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "            \n",
    "            # print(\"xtrain_plot: \",xtrain_plot)\n",
    "            \n",
    "            get_idx_x_all_to_2d_plot = get_idx_x_all(xtrain_plot, x_all) # agar dapat diplot pd 2D\n",
    "            get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all_to_2d_plot)\n",
    "            \n",
    "            # print(\"idx xtrain_plot: \",get_idx_x_all_to_2d_plot)\n",
    "        \n",
    "            \n",
    "            plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, \"x\", label=\"train\", color=\"k\")\n",
    "            \n",
    "            plt.ylim(-4, 4)\n",
    "            plt.xlim(0, 4)  # Set x-axis limits\n",
    "            plt.xticks(range(5))  # Set x-ticks to show 0, 1, 2, 3, 4\n",
    "            plt.xlabel(\"index of data\")  # Label for x-axis\n",
    "            plt.ylabel(\"loss value\")     # Label for y-axis\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            \n",
    "            plt.savefig(f\"loss_e_maml/plot_{run}_{lossval:.3f}_iter_{iteration}_{filename_first_n_last_Loss}.png\")\n",
    "            plt.savefig(f\"loss_e_maml/plot_{run}_{lossval:.3f}_iter_{iteration}_{filename_first_n_last_Loss}.pdf\")\n",
    "\n",
    "            \n",
    "            plt.pause(0.01)\n",
    "            model.load_state_dict(weights_before)  # restore from snapshot\n",
    "            print(f\"-----------------------------\")\n",
    "            print(f\"iteration               {iteration + 1}\")\n",
    "            print(f\"loss on plotted curve   {lossval:.3f}\")  # would be better to average loss over a set of examples, but this is optimized for brevity\n",
    "\n",
    "    \n",
    "    print()\n",
    "    final_lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "    print(f\"final loss on last model = {final_lossval:.3f}\") \n",
    "    filename_last_Model = datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    \n",
    "    # Save last loss\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml_{filename_last_Model_n_Loss}.png\")\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml{filename_last_Model_n_Loss}.pdf\")\n",
    "    \n",
    "    # Save the plot as PNG and PDF only after plotting\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml_{filename_last_Model_n_Loss}.png\")\n",
    "    # plt.savefig(f\"loss_e_maml/plot_e_maml_{filename_last_Model_n_Loss}.pdf\")\n",
    "\n",
    "    # Optionally, show the plot if you want to display it interactively\n",
    "    # plt.show()  # Use this only if you want to display the plot interactively\n",
    "    \n",
    "    \n",
    "    # Save last model checkpoint\n",
    "    path_last_Model = f'model_reg_last/model_last_{run}_{final_lossval:.3f}_{filename_last_Model}.json'\n",
    "    save_model_reptile_checkpoint(model, path_last_Model)\n",
    "    save_info_params(\n",
    "        n_iterations, n_data_all, n_sample, n_train, seed, inner_step_size,\n",
    "        inner_epochs, outer_stepsize_reptile, outer_stepsize_maml,\n",
    "        run, final_lossval, filename_last_Model)\n",
    "        \n",
    "experiment('E-MAML', False)\n",
    "# experiment('E-MAML', True)\n",
    "# experiment('E-MAML_Synthetic_E-Reptile', False)\n",
    "# experiment('E-MAML_Synthetic_E-Reptile', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Data - hasil model E-MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil dimuat dari: model_reg_last/model_last_E-MAML_0.247_31-10-2024-20-45-23.json\n",
      "Data Uji: \n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "panjang fitur input =  14\n",
      "\n",
      "Hasil Regresi: [-0.13187271 -0.00842541  0.20748296  0.04862366  0.00715363  0.09904483\n",
      " -0.21184166 -0.16326526 -0.11075503  0.37917936 -0.21138455 -0.04121123\n",
      "  0.00060908 -0.07439522 -0.07923727  0.14346708 -0.05763385 -0.3218375\n",
      " -0.06915761  0.00447744 -0.03410883  0.0658434   0.17517105 -0.03276164\n",
      "  0.04481705  0.15281963 -0.16166544 -0.00438098  0.14958285  0.12029406\n",
      "  0.00839265  0.20408103 -0.23486535 -0.1463198  -0.26493752  0.07310615\n",
      " -0.1249454   0.09390464 -0.1737231   0.10664489  0.06358783  0.22081359\n",
      "  0.08833277  0.05443309]\n",
      "Panjang dim Hasil Regresi: 44\n",
      "\n",
      "Top Values: [0.37917936 0.22081359]\n",
      "Top Indices: [ 9 41]\n",
      "Top Column Names: ['daging (sapi)', 'wijen']\n",
      "\n",
      "Hasil nilai loss: 0.12017130106687546\n"
     ]
    }
   ],
   "source": [
    "# Cara Memuat model dari checkpoint\n",
    "# declare params\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "path_last_Model = 'model_reg_last/model_last_E-MAML_0.247_31-10-2024-20-45-23.json'\n",
    "# path_last_Model = 'model_reg_last/model_last_E-MAML_Synthetic_E-Reptile_0.248_31-10-2024-20-44-27.json'\n",
    "\n",
    "\n",
    "# model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "model = ModelForSyntheticReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "\n",
    "try:\n",
    "    load_model_reptile_checkpoint(model, path_last_Model)\n",
    "    print(\"Model berhasil dimuat dari:\", path_last_Model)\n",
    "except Exception as e:\n",
    "    print(\"Terjadi kesalahan saat memuat model:\", e)\n",
    "\n",
    "# Sekarang Anda bisa menggunakan model untuk melakukan prediksi atau melanjutkan pelatihan\n",
    "# Contoh prediksi\n",
    "# test_data = to_torch([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]])  # Ganti dengan data yang sesuai\n",
    "\n",
    "id_test_data = 0\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "test_data = get_data_test(id_test_data)\n",
    "print(\"Data Uji: \")  # Tampilkan data uji\n",
    "print(test_data)\n",
    "print('panjang fitur input = ',len(test_data))\n",
    "print()\n",
    "\n",
    "y_true_test_data = get_y_gt(id_test_data)\n",
    "\n",
    "# model.eval()  # Set model ke mode evaluasi\n",
    "# with torch.no_grad():\n",
    "#     prediction = model(x_test)\n",
    "#     print(\"Hasil prediksi:\", prediction.numpy())\n",
    "    \n",
    "\n",
    "hasil_pred = test_single_data_return_pred(model, test_data)\n",
    "print(f\"Hasil Regresi: {hasil_pred}\") \n",
    "print(f\"Panjang dim Hasil Regresi: {len(hasil_pred)}\") \n",
    "\n",
    "print()\n",
    "topk = 2\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(hasil_pred, topk)\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n",
    "\n",
    "print()\n",
    "\n",
    "# test_single_data_return_loss(elm_model, test_data, y)\n",
    "# X_tensor = torch.FloatTensor(X)\n",
    "# nilai_loss = test_single_data_return_loss(elm_model, torch.FloatTensor(test_data), np.array(y_true_test_data))\n",
    "nilai_loss = test_single_data_return_loss(model, test_data, y_true_test_data)\n",
    "print(f\"Hasil nilai loss: {nilai_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG2CAYAAAB20iz+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABvX0lEQVR4nO3de1iUdf7/8edwPp8REAZBRAQPIAKKneygrZVg+62t7fCttrZtq+1gds7KNjPTDtu3w+Z22Ha3Nisr0Q5rW5kViKCiqIiKIgcROc0Aw3m4f3/wc4w0GxDmZuZ+P67L64qRGV7dr5nh7f255751iqIoCCGEEEI4ICe1AwghhBBCDBcZdIQQQgjhsGTQEUIIIYTDkkFHCCGEEA5LBh0hhBBCOCwZdIQQQgjhsGTQEUIIIYTDkkFHCCGEEA5LBh0hhBBCOCwZdIQQQgjhsOx20Fm6dCk6nY677rpL7ShCCCGEGKHsctApKChg5cqVTJkyRe0oQgghhBjB7G7QaW1t5eqrr+Zvf/sbgYGBascRQgghxAjmonaAgbrtttu4+OKLueCCC3jyySdP+b2dnZ10dnZavu7t7aWxsZHg4GB0Ot1wRxVCCCHEEFAUhZaWFkaPHo2T08D20djVoPPee++xdetWCgoKrPr+pUuXsnjx4mFOJYQQQghbqKysJCoqakD3sZtBp7KykjvvvJP169fj4eFh1X0efPBBFixYYPnaaDQSHR3Nf//7X7y9vYcrqhhB8vPzmT59utoxhI1I39oifWuHyWTiggsuwNfXd8D3tZtBZ8uWLRw9epRp06ZZbjObzWzcuJGXXnqJzs5OnJ2d+93H3d0dd3f3Ex7L29sbHx+fYc8s1DdhwgTpWkOkb22RvrVnMIed2M2gc/7551NcXNzvthtuuIEJEyZw//33nzDkCCGEEELYzaDj6+vLpEmT+t3m7e1NcHDwCbcLccyhQ4eIjIxUO4awEelbW6RvYQ27+3i5EEIIIYS17GaPzsls2LBB7QhihEtNTVU7grAh6VtbpG9hDdmjIxzavn371I4gbEj61hbpW1hDBh3h0FpaWtSOIGxI+tYW6VtYQwYd4dDkfEnaIn1ri/QtrCGDjnBoSUlJakcQNiR9a4v0Lawhg45waNZeLkQ4BulbW6RvYQ0ZdIQQQgjhsGTQEQ5Nr9erHUHYkPStLdK3sIYMOsKhubq6qh1B2JD0rS3St7CGDDrCoR04cEDtCMKGpG9tkb6FNWTQEUIIIYTDkkFHOLSUlBS1Iwgbkr61RfoW1pBBRzi08vJytSMIG5K+tUX6FtaQQUc4NIPBoHYEYUPSt7ZI38IaMugIh+bh4aF2BGFD0re2SN/CGjLoCIeWnJysdgRhQ9K3tkjfwhoy6AiHlp+fr3YEYUPSt7ZI38IaMugIIYQQwmHJoCMcWmRkpNoRhA1J39oifQtryKAjHJqXl5faEYQNSd/aIn0La8igIxzavn371I4gbEj61hbpW1hDBh0hhBBCOCwZdIRDmzx5stoRhA1J39oifQtryKAjHFp1dbXaEYQNSd/aIn0La8igIxxaY2Oj2hGEDUnf2iJ9C2vIoCMcmpubm9oRhA1J39oifQtryKAjHFpaWpraEYQNSd/aIn0La8igIxxabm6u2hGEDUnf2iJ9C2vIoCOEEEIIhyWDjnBo4eHhakcQNiR9a4v0Lawhg45waP7+/mpHEDYkfWuL9C2sIYOOcGilpaVqRxA2JH1ri/QtrCGDjhBCCCEclgw6wqFNnDhR7QjChqRvbZG+hTVk0BEOrba2Vu0Iwoakb22RvoU1ZNARDq2+vl7tCMKGpG9tkb6FNexm0Hn11VeZMmUKfn5++Pn5kZmZyeeff652LDHCubi4qB1B2JD0rS3St7CG3Qw6UVFRPP300xQWFlJYWMh5551HdnY2u3btUjuaGMEyMjLUjiBsSPrWFulbWMNuBp158+Zx0UUXMX78eMaPH8+SJUvw8fFh06ZNakcTI5g8P7RF+tYW6VtYwy73+5nNZj744ANMJhOZmZk/+32dnZ10dnZavm5ubrZFPDGC9Pb2qh1B2JD0rS3St+Pr7taxcaMPq1cHDPox7GrQKS4uJjMzk46ODnx8fPj4449JSkr62e9funQpixcvPuH2/Px8PD09SU9PZ9euXbS1teHn50dcXBzbtm0DICYmBkVROHToEADTpk2jtLSU1tZWvL29SUxMpLCwEIDo6GicnZ05ePAgACkpKRw8eBCj0YinpyeTJ09m8+bNQN8SnIeHB/v37wdgypQpVFZW0tTUhLu7O6mpqeTl5QEQERGBr68ve/fuBWDSpEnU1NTQ0NCAi4sLGRkZ5OXloSgKYWFhBAYGsmfPHgASExNpaGjg6NGjODk5MWPGDDZv3kxPTw8hISGEhYVZlv0SEhIwGo0cOXIEgJkzZ1JYWEhXVxdBQUFERkZSXFwMQHx8PG1tbVRXVwMwffp0tm/fTkdHBwEBAcTExFBUVATA2LFj6e7uprKyEoD09HR2796NyWTC19eX+Ph4tm7dCsCYMWMALNs7NTWVffv20dLSgre3N0lJSRQUFACg1+txdXXlwIEDlu1dXl6OwWDAw8OD5ORk8vPzgb41/Lq6Ovbt2wfA5MmTqa6uprGxETc3N9LS0iwXBgwPD8ff399yErKJEydSW1tLfX29ZXtv2rSJ3t5eRo0aRXBwMCUlJQBMmDCBpqYmamtr0el0ZGZmWrZ3cHAwERER7Ny5E4Dx48fT0tJCTU0NAJmZmWzdupXOzk4CAwPR6/Xs2LEDgHHjxtHR0UFVVRXQt6u+uLiY9vZ2/P39iY2NtWzv2NhYzGYzFRUVQN+VnUtKSjCZTPj4+JCQkMCWLVss21un01FeXg7A1KlTKSsro7m5GS8vLyZOnNhve7u5uVFWVgZAcnIyFRUVNDU14eHhQUpKiuVf1qNHj8bb29uyvX/8nHV1dSU9Pb3f9g4ICLA8Z5OSkqirq6Ourg5nZ2emT59Ofn4+ZrOZ0NBQQkND2b17t2V7GwyGfs/ZgoICTCYTpaWl/bZ3fHw8JpOJw4cPAzBjxgyKioro6OggMDCQ6Ohotm/fDkBcXBxdXV39nrPyHjFy3yN6enrIzc09rfeIyMhIvLy85D1iBL1H/PBDLhUVIWzfnsL33+sxGl2Bwe+o0CmKogz63jbW1dVFRUUFBoOB1atX8/rrr/Ptt9/+7LBzsj06er2evLw8fHx8bBVbqKipqYnAwEC1Ywgbkb61Rfp2LHV1Lnz6qT9r1gSyf7+H5fbQ0G7mzKnknXfiMBqN+Pn5Dehx7WqPjpubG+PGjQP6JtGCggL+8pe/8Nprr530+93d3XF3d7dlRDHClJSUMHPmTLVjCBuRvrVF+rZ/nZ06vvnGl5ycQH74wYfeXh0Abm69nH9+M1lZBmbMaKWjo5V33hncz7CrQeenFEXpt8dGCCGEECObosCOHZ7k5ATw+ecBtLQ4W/4uObmN7OwmLrzQiJ/f0ByDZTeDzkMPPcTcuXPR6/W0tLTw3nvvsWHDBr744gu1o4kRbMKECWpHEDYkfWuL9G1fjhxxYd26ANasCaS8/PhqS3h4F/PmGcjKMhAT0zXkP9duBp3a2lquvfZaampq8Pf3Z8qUKXzxxRfMnj1b7WhiBGtqaiIoKEjtGMJGpG9tkb5HvvZ2HV995UdOTiCbNnmjKH1LU56evVxwgZGsLAMZGSachvFkN3Yz6LzxxhtqRxB2qLa2lri4OLVjCBuRvrVF+h6ZFAW2bvUiJyeA//zHH5Pp+NJUWpqJrKwm5sxpxtvbNqcHsJtBR4jB0Ol0akcQNiR9a4v0PbJUV7uSkxPA2rUBVFYeX5qKjOwiO7uJSy4xoNd32zyXDDrCoZ3qhJLC8Ujf2iJ9q6+tzYn16/3IyQmgoOD4aVu8vMxceGEzWVlNpKa2DevS1C+RQUc4tM2bN8v1cDRE+tYW6Vsdvb1QUOBNTk4AX37pR3t739KUTqeQkWEiO7uJ889vxstrZJymTwYd4dB6enrUjiBsSPrWFunbtioq3Fizpm9pqqbGzXL7mDGdZGcbuOQSAxERtl+a+iUy6AiHFhwcrHYEYUPSt7ZI38OvpcWJ//zHn5ycALZt87bc7utr5sILjWRnN5Gc3M5IPlxKBh3h0CIiItSOIGxI+tYW6Xt4mM2Qn+/DJ58E8PXXfnR29h1g4+SkkJnZyvz5Tcya1YKHx8hYmvolMugIh7Zz5045RbyGSN/aIn0PrQMH3FmzJoB16wI4etTVcntcXAfZ2QYuvtjAqFH2t1wog44QQgihUUajM59/3rc0VVzsZbnd37+Hiy7qW5pKSuoY0UtTv0QGHeHQxo8fr3YEYUPSt7ZI34PT0wM//ODLmjUBbNjgS3d339KUs7PCWWe1kJ1t4OyzW3Bzs4+lqV8ig45waC0tLYSEhKgdQ9iI9K0t0vfAlJa6k5MTyKef+tPQcHxpKiGhnexsAxddZCA42KxiwuEhg45waDU1NcTGxqodQ9iI9K0t0vcva2x05rPPAsjJCaCkxNNye1BQDxddZCA728CECR0qJhx+MugIIYQQDqS7W8fGjT6sWRPId9/50tPTd4CNi0svs2b1LU2dcUYLrq6/8EAOQgYd4dDkFPHaIn1ri/R9nKJASYkHa9YE8tln/hgMx3+9T5zYRna2gblzjQQEON7S1C+RQUc4tK1btzJt2jS1Ywgbkb61RfqG+noX1q3zZ82aQPbv97DcHhrazSWXGMjKMjBuXKeKCdUng45waJ2d2n6Ba430rS1a7buzU8eGDb6sWRNIbq4PZnPf0pSbWy/nnddMdraBGTNacZHf8IAMOsLBBQYGqh1B2JD0rS1a6ltRoLjYkzVrAvj88wBaWpwtf5ec3EZWVhO/+pURP79eFVOOTDLoCIem1+vVjiBsSPrWFi30feSIC+vWBbBmTSDl5e6W28PDu5g3r29pKiamS8WEI58MOsKh7dixQ04RryHSt7Y4at/t7Tq++sqPnJxANm3yRlH6lqY8PHq54IJmsrObyMgw4eSkclA7IYOOEEIIoTJFgW3bvFizJoD//Mcfk+n40tS0aSays5uYM6cZb29ZmhooGXSEQxs3bpzaEYQNSd/a4gh9V1e7kpMTwNq1AVRWHl+aiozsIivLwLx5Tej13SomtH8y6AiH1tHh2Gf8FP1J39pir323tTnx5Zd+rFkTQEGBj+V2Ly8zc+b0LU2lprbJ0tQQkUFHOLSqqiqio6PVjiFsRPrWFnvqu7cXCgu9WbMmgC+/9KO9vW9pSqdTyMgwkZ1t4PzzjXh5OcaFNEcSGXSEEEKIYVJR4WZZmjp82M1y+5gxnf9/acpARIQsTQ0nGXSEQ8vIyFA7grAh6VtbRmrfLS1OrF/vz5o1AWzb5m253dfXzIUXGsnObiI5uR2dTsWQGiKDjnBoxcXFTJ06Ve0Ywkakb20ZSX2bzZCf78MnnwTw9dd+dHb2HWDj5KSQmdnK/PlNzJrVgoeHLE3Zmgw6wqG1t7erHUHYkPStLSOh7wMH3MjJCWTt2gCOHj1+OfC4uA6ysgxccomBUaN6VEwoZNARDs3f31/tCMKGpG9tUatvo9GZL77oW5oqLvb6UZ4e5s41Mn9+E0lJHbI0NULIoCMcWmxsrNoRhA1J39piy757eiA314dPPglkwwZfurv7lqacnRXOOquFrCwD55zTgpubLE2NNDLoCIdWVFTkkKeIFycnfWuLLfreu9edNWsC+fRTfxoaji9NJSS0k5Vl4KKLDISEmIc1gzg9MugIIYQQP9LY6Mznn/uzZk0gJSWeltuDgnq46CID2dkGJkywz5MVapEMOsKhyVKGtkjf2jKUfXd369i40Yc1awL57jtfenr6DrBxcell1qy+pakzz2zB1fUXHkiMODLoCIdmNssuZS2RvrXldPtWFCgp8SAnJ4DPPgugqen4r8SJE9v+/9KUkYAAeV7ZMxl0hEOrqKggKipK7RjCRqRvbRls3/X1Lnz6qT+ffBLI/v0elttDQ7u55BIDWVkGxo3rHMqoQkUy6AghhHB4nZ06NmzwJScngB9+8MVs7luacnPr5bzzmsnONjBjRisu8lvR4UilwqGlpaWpHUHYkPStLb/Ut6JAcbGnZWmqpcXZ8nfJyW1kZTXxq18Z8fPrHe6oQkV2M+gsXbqUjz76iD179uDp6cnMmTNZtmwZCQkJakcTI1hJSQnJyclqxxA2In1ry8/1feSIC+vWBZCTE8DBg8eXpsLCusnKamLePAOxsV22jCpUZDeDzrfffsttt91Geno6PT09PPzww8yZM4fdu3fj7e39yw8gNMlkMqkdQdiQ9K0tP+67vV3H11/7sWZNIJs2eaMofUtTHh69XHBBM1lZTWRkmHB2/rlHE47KbgadL774ot/Xb731FqNGjWLLli2cffbZKqUSI52Pj4/aEYQNSd/a4u3tw9atXuTkBPDFF/6YTMenmGnTTGRnNzF7djM+PrI0pWV2M+j8lNFoBCAoKEjlJGIkk6VNbZG+taGrS8c//hHM6tW/pqrq+NJUZGQXWVkG5s1rQq/vVjGhGEnsctBRFIUFCxZw5plnMmnSpJ/9vs7OTjo7j39EsLm52RbxxAiyZcsWuSSAhkjfjq+iwo2FC/WWMxZ7eZmZM6dvaWratDacnFQOKEYcuxx0br/9dnbs2MH3339/yu9bunQpixcvPuH2/Px8PD09SU9PZ9euXbS1teHn50dcXBzbtm0DICYmBkVROHToEADTpk2jtLSU1tZWvL29SUxMpLCwEIDo6GicnZ05ePAgACkpKRw8eBCj0YinpyeTJ09m8+bNAERFReHh4cH+/fsBmDJlCpWVlTQ1NeHu7k5qaip5eXkARERE4Ovry969ewGYNGkSNTU1NDQ04OLiQkZGBnl5eSiKQlhYGIGBgezZsweAxMREGhoaOHr0KE5OTsyYMYPNmzfT09NDSEgIYWFh7Nq1C+j7V7DRaOTIkSMAzJw5k8LCQrq6uggKCiIyMpLi4mIA4uPjaWtro7q6GoDp06ezfft2Ojo6CAgIICYmhqKiIgDGjh1Ld3c3lZWVAKSnp7N7925MJhO+vr7Ex8ezdetWAMaMGQNg2d6pqans27ePlpYWvL29SUpKoqCgAAC9Xo+rqysHDhywbO/y8nIMBgMeHh4kJyeTn58PQFtbG3V1dezbtw+AyZMnU11dTWNjI25ubqSlpZGbmwtAeHg4/v7+lJaWAjBx4kRqa2upr6+3bO9NmzbR29vLqFGjCA4OpqSkBIAJEybQ1NREbW0tOp2OzMxMy/YODg4mIiKCnTt3AjB+/HhaWlqoqakBIDMzk61bt9LZ2UlgYCB6vZ4dO3YAMG7cODo6OqiqqgIgIyOD4uJi2tvb8ff3JzY21rK9Y2NjMZvNVFRUAH2fSCkpKcFkMuHj40NCQgJbtmyxbG+dTkd5eTkAU6dOpaysjObmZry8vJg4cWK/7e3m5kZZWRkAycnJVFRU0NTUhIeHBykpKWzatAmA0aNH4+3tbdneP37Ourq6kp6e3m97BwQEWJ6zSUlJ1NXVUVdXh7OzM9OnTyc/Px+z2UxoaCihoaHs3r3bsr0NBkO/52xBQQGNjY2Ulpb2297x8fGYTCYOHz4MwIwZMygqKqKjo4PAwECio6PZvn07AHFxcXR1dfV7zsp7xMh5j9i4MYznnhtPR4cb/v7dXHjhV5x55mGCgtxJSkpi06aBv0dERkbi5eUl7xEj/D2ivb2dwdIpimJXl1r905/+xCeffMLGjRt/8fTfJ9ujo9frycvLk7V8jaiuriYyMlLtGMJGpG/H1NGh45lnIvjgg75DFVJTTSxbVonZfEj61ojW1lYyMzMxGo34+fkN6L52s0dHURT+9Kc/8fHHH7NhwwarrnHi7u6Ou7u7DdKJkUqn06kdQdiQ9O14Dh7sW6rau9cTnU7hppvquPXWo7i4wOHD0rf4ZXazmnnbbbfxr3/9i3fffRdfX1+OHDnCkSNHTmt3lnB8x3a7Cm2Qvh3L2rX+XHFFHHv3ehIU1MNf/1rOHXcctZy9WPoW1rCbPTqvvvoqALNmzep3+1tvvcX1119v+0BCCCGGRXu7jqVLR/Pxx4EAZGS08vTTVYSG9qicTNgjuxl07OxQIjFCTJ06Ve0Iwoakb/tXVubOwoV69u/3QKdT+OMfj3LzzXUnPdGf9C2sYTdLV0IMxrFPAQhtkL7t2yefBHDllXHs3+9BSEg3r79ezh//ePIhB6RvYR272aMjxGDIuZO0Rfq2T21tTixZEkFOTt9SVWZmK089VUlIiPmU95O+hTVk0BEOzcvLS+0Iwoakb/uzd2/fUtXBgx44OSncfvtRbryxzqoT/0nfwhoy6AiHNnHiRLUjCBuSvu2HosDq1YE8/XQEnZ1OjBrVzTPPVDJtWpvVjyF9C2vIMTrCoR07c6fQBunbPrS2OnH//VEsXhxJZ6cTZ57Zwocf7h/QkAPSt7CO7NERQghhMyUlHixcqKeiwh1nZ4U776zluuvq5RpVYtjIoCMcml6vVzuCsCHpe+RSFFi1Kohnngmnu9uJiIgunnmmkpSUwZ/0VfoW1pBBRzg0Nzc3tSMIG5K+R6aWFiceeyySL7/0B2DWrGaefLIaf/9Tf6rql0jfwhqys1A4NDnPhrZI3yPPzp2eXH75OL780h8Xl17uu6+GF1+sOO0hB6RvYR3ZoyOEEGLIKQq8804wzz4bRk+PE5GRXaxYUcmkSXJ9QmFbMugIh5acnKx2BGFD0vfIYDQ6s2hRJN984wfABRcYWby4Gj+/3iH9OdK3sIYsXQmHVlFRoXYEYUPSt/q2b/fk8svj+OYbP1xde3noocM891zlkA85IH0L68geHeHQmpqa1I4gbEj6Vk9vL/zjH8H85S/h9PTo0Os7WbGikqSkjmH7mdK3sIYMOsKheXh4qB1B2JD0rY6mJmceeSSSjRv7lqp+9SsDjz12GB+fod+L82PSt7CGDDrCoaWkpKgdQdiQ9G17W7d6cd99emprXXFz6+WBB2q47LImdLrh/9nSt7CGHKMjHNqmTZvUjiBsSPq2nd5eeP31EH73u1hqa12Jienk3XfLuPxy2ww5IH0L68geHSGEEAPS0ODMQw9FkZvrC8AllxhYtOgwXl7Du1QlxGDIoCMc2ujRo9WOIGxI+h5+BQXe3H9/FHV1rnh49H2qav58g8324vyY9C2sIYOOcGje3t5qRxA2JH0PH7MZVq4M5a9/HUVvr464uA5WrKhk3LhO1TJJ38IacoyOcGj79u1TO4KwIel7eNTXu/CHP8Twyith9PbqmD+/iXffLVN1yAHpW1hH9ugIIYT4WXl53jzwgJ7GRhc8Pc0sWlTDvHkGtWMJYTUZdIRDmzRpktoRhA1J30OnpwdefXUUf/tbKIqiIz6+gxUrKhg7tkvtaBbSt7CGLF0Jh1ZTU6N2BGFD0vfQqK114aabYlm5chSKouOyyxp5992yETXkgPQtrCN7dIRDa2hoUDuCsCHp+/R9/70PDz0URVOTC15eZh5//DBz5xrVjnVS0rewhgw6wqG5urqqHUHYkPQ9eN3d8NJLYbz5ZigAiYntLF9eyZgxI2svzo9J38IaMugIh5aenq52BGFD0vfgHDniyr33RlFU1Pdx7SuvbGDhwiO4uysqJzs16VtYQ47REQ4tNzdX7QjChqTvgduwwZfLLoujqMgbX18zzz1XwcMP14z4IQekb2Ed2aMjhBAa1N2t44UXwvjHP0IAmDSpjWeeqUSv71Y5mRBDSwYd4dDCw8PVjiBsSPq2TlWVK/fdp6e42AuAa66pZ8GCWlxdR/5enB+TvoU1ZNARDi0gIEDtCMKGpO9f9tVXvixaFEVLizO+vmaefLKK885rUTvWoEjfwhpyjI5waHv27FE7grAh6fvndXXpWLo0grvuGkNLizNTprTx4Yf77XbIAelbWEf26AghhIOrrHRj4UI9u3d7AnDDDXX86U+1yKezhRbIoCMcWlJSktoRhA1J3yf64gs/Hn88EpPJmYCAHpYsqeLss1vVjjUkpG9hDVm6Eg6trq5O7QjChqTv4zo7dfz5zxHce280JpMzqakmPvhgv8MMOSB9C+vIoCMcmrwRaov03ae83I2rrx7L++8Ho9Mp/P73R3njjYOEh/eoHW1ISd/CGrJ0JRyas7Oz2hGEDUnfsG6dP088MZr2dmeCgnpYurSKmTMdZy/Oj0nfwhp2tUdn48aNzJs3j9GjR6PT6fjkk0/UjiRGuOnTp6sdQdiQlvtub9fx2GOjefBBPe3tzqSnt/Lhh/sddsgBbfctrGdXg47JZCI5OZmXXnpJ7SjCTuTn56sdQdiQVvsuK3Pnqqvi+OijIHQ6hT/+8Sh/+1s5oaGOtVT1U1rtWwyMXS1dzZ07l7lz56odQ9gRs9msdgRhQ1rs+5NPAnjqqdG0tzsREtLN009XMX26Se1YNqHFvsXA2dWgM1CdnZ10dnZavm5ublYxjVBDaGio2hGEDWmp77Y2J5YsiSAnJxCAGTNaWbq0kpAQ7fzy11LfYvAcetBZunQpixcvPuH2/Px8PD09SU9PZ9euXbS1teHn50dcXBzbtm0DICYmBkVROHToEADTpk2jtLSU1tZWvL29SUxMpLCwEIDo6GicnZ05ePAgACkpKRw8eBCj0YinpyeTJ09m8+bNAERFReHh4cH+/fsBmDJlCpWVlTQ1NeHu7k5qaip5eXkARERE4Ovry969ewGYNGkSNTU1NDQ04OLiQkZGBnl5eSiKQlhYGIGBgZYzhSYmJtLQ0MDRo0dxcnJixowZbN68mZ6eHkJCQggLC2PXrl0AJCQkYDQaOXLkCAAzZ86ksLCQrq4ugoKCiIyMpLi4GID4+Hja2tqorq4G+tbIt2/fTkdHBwEBAcTExFBUVATA2LFj6e7uprKyEoD09HR2796NyWTC19eX+Ph4tm7dCsCYMWMALNs7NTWVffv20dLSgre3N0lJSRQUFACg1+txdXXlwIEDlu1dXl6OwWDAw8OD5ORkyy7twMBA6urq2LdvHwCTJ0+murqaxsZG3NzcSEtLs1wBOTw8HH9/f0pLSwGYOHEitbW11NfXW7b3pk2b6O3tZdSoUQQHB1NSUgLAhAkTaGpqora2Fp1OR2ZmpmV7BwcHExERwc6dOwEYP348LS0t1NTUAJCZmcnWrVvp7OwkMDAQvV7Pjh07ABg3bhwdHR1UVVUBkJGRQXFxMe3t7fj7+xMbG2vZ3rGxsZjNZioqKgBIS0ujpKQEk8mEj48PCQkJbNmyxbK9dTod5eXlAEydOpWysjKam5vx8vJi4sSJ/ba3m5sbZWVlACQnJ1NRUUFTUxMeHh6kpKSwadMmAEaPHo23t7dle//4Oevq6kp6enq/7R0QEGB5ziYlJVFXV0ddXR3Ozs5Mnz6d/Px8zGYzoaGhhIaGsnv3bsv2NhgM/Z6zBQUFtLW10dvb2297x8fHYzKZOHz4MAAzZsygqKiIjo4OAgMDiY6OZvv27QDExcXR1dXV7zk7Et8jPvxwDytXXsCRI4E4OSnMm1fIr361DTe3SZSWauc9wmg0kpube1rvEZGRkXh5ecl7xAh/j2hvb2ewdIqi2NdV3P4/nU7Hxx9/zPz583/2e062R0ev15OXl4ePj48NUgq15ebmMnPmTLVjCBtx9L4VBT76KJClSyPo7HRi1Khuli2rJC2tTe1oqnD0vsVxra2tZGZmYjQa8fPzG9B9HXqPjru7O+7u7mrHEEKI02YyObF48Wg+/zwAgDPOaOGpp6oICtLOUpUQg+HQg44QEyZMUDuCsCFH7XvPHg8WLtRz6JA7zs4Kd9xRy/XX1+NkV5+bHXqO2rcYWnY16LS2tlrWrQEOHjxIUVERQUFBREdHq5hMjFQGg4GgoCC1YwgbcbS+FQXefz+IZ54Jp6vLifDwLpYvryQlZfDHKzgSR+tbDA+7+vdAYWEhU6dOZerUqQAsWLCAqVOn8uijj6qcTIxUxw6eFNrgSH23tDixcKGeJ58cTVeXE7NmNfPhh2Uy5PyII/Utho9d7dGZNWsWdnrstBBCWG3XLg8WLoymqsoNF5de7r67lmuvbUCnUzuZEPbHrgYdIQZKPpGhLfbet6LAO+8E8+yzYfT0OBEZ2bdUNXmy7MU5GXvvW9iGXS1dCTFQx87zILTBnvs2Gp24665oli2LoKfHiQsuMPL++/tlyDkFe+5b2I7s0REOrbu7W+0Iwobste8dOzy59149hw+74eray8KFR/jtbxtlqeoX2GvfwrZk0BEOLTg4WO0Iwobsre/eXvjnP4N54YVwenp06PWdLF9eycSJHWpHswv21rdQhww6wqFFRESoHUHYkD31bTA48/DDkWzc2HeW1wsvNPL449X4+PSqnMx+2FPfQj1yjI5waMeuHSO0wV763rbNi8suG8fGjX64ufWyaFE1y5dXypAzQPbSt1DXoPfo7N+/n7KyMs4++2w8PT1RFAWdLCgLIcTP6u2FN98M4aWXwjCbdcTEdLJiRSUJCbJUJcRwGfAenYaGBi644ALGjx/PRRddZLm66k033cQ999wz5AGFOB3x8fFqRxA2NJL7bmhw5tZbx/CXv4RjNuu45BIDq1aVyZBzGkZy32LkGPCgc/fdd+Pi4kJFRQVeXl6W26+44gq++OKLIQ0nxOkymUxqRxA2NFL7Lijw4vLLx/HDD754ePTyxBNVPPVUFV5eslR1OkZq32JkGfCgs379epYtW0ZUVFS/2+Pj4zl06NCQBRNiKBw+fFjtCMKGRlrfZjO8+mooN90US12dK2PHdvDvf5dx6aUG+ej4EBhpfYuRacDH6JhMpn57co6pr6/H3d19SEIJIYS9q6934YEHosjP9wFg/vwmHnzwMF5echkbIWxpwHt0zj77bP7xj39YvtbpdPT29rJ8+XLOPffcIQ0nxOmaMWOG2hGEDY2Uvjdt8uayy+LIz/fB07OXp56q4s9/rpYhZ4iNlL7FyDbgPTrLly9n1qxZFBYW0tXVxX333ceuXbtobGzkhx9+GI6MQgxaUVERqampascQNqJ23z098Ne/jmLlylAURUd8fAcrVlQwdmyXapkcmdp9C/sw4D06SUlJ7Nixg4yMDGbPno3JZOLXv/4127ZtIy4ubjgyCjFoHR3yiRYtUbPvo0dduOmmWF57bRSKouN//qeRd98tkyFnGMnrW1hjUOfRCQ8PZ/HixUOdRYghFxgYqHYEYUNq9f399z489FAUTU0ueHmZeeyxw1x0kVGVLFoir29hjQEPOhs3bjzl35999tmDDiPEUIuOjlY7grAhW/fd0wMvvRTGG2+EAjBhQjsrVlQyZozsxbEFeX0Lawx40Jk1a9YJt/34jMhms/m0AgkxlLZv387MmTPVjiFsxJZ9Hzniyn33RbFtmzcAV1zRwL33HsHdXQ44thV5fQtrDHjQaWpq6vd1d3c327ZtY9GiRSxZsmTIggkhxEj17be+PPxwJEajCz4+ZhYvrmbOnGa1YwkhTmLAg46/v/8Jt82ePRt3d3fuvvtutmzZMiTBhBgKcoC8tgx3393dOv7ylzDefjsEgIkT21i+vBK9vntYf644OXl9C2sM+qKePxUaGkppaelQPZwQQ6KrS46V0JLh7Lu62pX77tOzY0ffCVOvuaaeu++uxc1NlqrUIq9vYY0BDzo7duzo97WiKNTU1PD000+TnJw8ZMGEGAqVlZXo9Xq1YwgbGa6+v/rKl0WLomhpccbX18yTT1Zx3nktQ/5zxMDI61tYY8CDTkpKCjqdDkXp/6+YGTNm8Oabbw5ZMCGEUFtXl47nngvjnXf6lqqmTOlbqho9WpaqhLAXAx50Dh482O9rJycnQkND8fDwGLJQQgyV9PR0tSMIGxrKvisr3Vi4UM/u3Z4AXH99HXfcUYur65D9CHGa5PUtrDHgQWfMmDHDkUOIYbFr1y5SUlLUjiFsZKj6/s9//Hj88UhaW50JCOhhyZIqzj679fQDiiElr29hDasGnRdffNHqB7zjjjsGHUaIodbW1qZ2BGFDp9t3Z6eO5cvDWbUqGICpU00880wl4eE9QxFPDDF5fQtrWDXoPP/881Y9mE6nk0FHjCh+fn5qRxA2dDp9l5f3LVWVlvYtVd10Ux233VaLy5B9NlUMNXl9C2tY9RL+6XE5QtgLOc+Gtgy2708/9eeJJ0bT1uZMUFAPTz1VxRlnyFLVSCevb2GNAV+9XAh7sm3bNrUjCBsaaN/t7Toef3w0Dzygp63NmfT0Vj74YL8MOXZCXt/CGoPaKVtVVUVOTg4VFRUnnLDpueeeG5JgQggxnA4ccOeee/Ts3++BTqfwhz/UccstR3F2VjuZEGIoDXjQ+eqrr8jKyiI2NpbS0lImTZpEeXk5iqKQmpo6HBmFGLSYmBi1IwgbsrbvNWsCWLJkNO3tTgQHd7NsWRXTp5uGN5wYcvL6FtYY8NLVgw8+yD333MPOnTvx8PBg9erVVFZWcs4553D55ZcPR0YhBu2nJ7YUju2X+m5r0/Hww5E88kgU7e1OzJjRyocf7pchx07J61tYY8CDTklJCddddx0ALi4utLe34+PjwxNPPMGyZcuGPKAQp+PQoUNqRxA2dKq+9+1z58or48jJCcTJSeH222v561/LCQkx2zChGEry+hbWGPCg4+3tTWdnJwCjR4+mrKzM8nf19fVDl0wIIYaAosDq1YH89rdxHDzowahR3bz++kH+8Ic6OR5HCA0Y8DE6M2bM4IcffiApKYmLL76Ye+65h+LiYj766CNmzJgxHBmFGLRp06apHUHY0E/7NpmceOKJ0Xz2WQAAZ5zRwlNPVREUJHtxHIG8voU1BrxH57nnnmP69OkAPP7448yePZtVq1YxZswY3njjjSEPKMTpKC0tVTuCsKEf971njwdXXhnHZ58F4OyscNddR3jllUMy5DgQeX0Lawx4j87YsWMt/+3l5cUrr7wypIF+ySuvvMLy5cupqalh4sSJvPDCC5x11lk2zSDsR2urnA9FS1pbW1EUeP/9IJ55JpyuLifCw7tYvrySlJR2teOJISavb2GNAe/RueGGG/jqq69UOdp91apV3HXXXTz88MNs27aNs846i7lz51JRUWHzLMI+eHt7qx1B2JBOF8C99+p58snRdHU5MWtWMx98UCZDjoOS17ewxoAHnYaGBi6++GKioqK45557KCoqGoZYJ/fcc89x4403ctNNN5GYmMgLL7yAXq/n1VdftVkGYV8SExPVjiBsZNcuDxYvzuY///HHxUVh4cIaXnyxgoAAWapyVPL6FtYY8KCTk5PDkSNHeOyxx9iyZQvTpk0jKSmJp556ivLy8mGI2Kerq4stW7YwZ86cfrfPmTOH3Nzck96ns7OT5ubmfn+EthQWFqodQQyjjg4dn3/uzy23jOGqq+KornZn9Ogu3n77ANdd14BOp3ZCMZzk9S2sMahLQAQEBHDzzTdz8803U1VVxb///W/efPNNHn30UXp6eoY6I9D30XWz2UxYWFi/28PCwjhy5MhJ77N06VIWL158wu35+fl4enqSnp7Orl27aGtrw8/Pj7i4OMu1U2JiYlAUxXKehmnTplFaWkprayve3t4kJiZaXmTR0dE4OztbLn6akpLCwYMHMRqNeHp6MnnyZDZv3gxAVFQUHh4e7N+/H4ApU6ZQWVlJU1MT7u7upKamkpeXB0BERAS+vr7s3bsXgEmTJlFTU0NDQwMuLi5kZGSQl5eHoiiEhYURGBjInj17gL5/6TQ0NHD06FGcnJyYMWMGmzdvpqenh5CQEMLCwti1axcACQkJGI1Gy3acOXMmhYWFdHV1ERQURGRkJMXFxQDEx8fT1tZGdXU1ANOnT2f79u10dHQQEBBATEyMZS/f2LFj6e7uprKyEoD09HR2796NyWTC19eX+Ph4tm7dCsCYMWOA4+fFSE1NZd++fbS0tODt7U1SUhIFBQUA6PV6XF1dOXDggGV7l5eXYzAY8PDwIDk5mfz8fADa2tqoq6tj3759AEyePJnq6moaGxtxc3MjLS3NMiiHh4fj7+9vOcBx4sSJ1NbWUl9fb9nemzZtore3l1GjRhEcHExJSQkAEyZMoKmpidraWnQ6HZmZmZbtHRwcTEREBDt37gRg/PjxtLS0UFNTA0BmZiZbt26ls7OTwMBA9Ho9O3bsAGDcuHF0dHRQVVUFQEZGBsXFxbS3t+Pv709sbKxle8fGxmI2my1LuWlpaZSUlGAymfDx8SEhIYEtW7ZYtrdOp7P842Tq1KmUlZXR3NyMl5cXEydO7Le93dzcLKeSSE5OpqKigqamJjw8PEhJSWHTpk1A3yknvL29Ldv7x89ZV1dX0tPT+23vgIAAy3M2KSmJuro66urqcHZ2Zvr06eTn52M2mwkNDSU0NJTdu3ejKGA2Z5CTE8DGjRG0t7tbXtuTJu1h4cK9xMSEkJu70/KcNZlMHD58GOj75GhRUREdHR0EBgYSHR3N9u3bgb6LRHZ1dfV7zsp7xMh9j2hubiY3N/e03iMiIyPx8vKS94gR/h7R3j745WedchoH23R3d/Ppp5/yr3/9i08//ZSgoCDLk3uoHT58mMjISHJzc8nMzLTcvmTJEv75z39aXrw/1tnZaTnnD0BzczN6vZ68vDx8fHyGJacYWaqqqoiKilI7hhgCNTWurF0bQE5OAIcOHR9uIiK6yMoykJVlwMnpgPStIfL61o7W1lYyMzMxGo34+fkN6L6D2qPzzTff8O6777J69WrMZjO//vWvWbt2Leedd95gHs4qISEhODs7n7D35ujRoyfs5TnG3d0dd3f3k/6d0AZnOSOcXWtr0/HVV/6sWRPA5s3eKErfWpSnp5nZs5vJzjaQlmbC6f8vwtfUSN9aIq9vYY0BDzpRUVE0NDRw4YUX8tprrzFv3jw8PDyGI1s/bm5uTJs2jS+//JJLL73UcvuXX35Jdnb2sP98YZ8OHjxIRESE2jHEAPT2wpYtXuTkBLJ+vR9tbcd/mWVktJKVZWD27Ga8vHpPuK/0rS3St7DGgAedRx99lMsvv5zAwMDhyHNKCxYs4NprryUtLY3MzExWrlxJRUUFt9xyi82zCCGGVmWlK2vXBpKTE0B1tZvldr2+k6wsA/PmGYiM7FYxoRDCHg140Ln55puHI4dVrrjiChoaGnjiiSeoqalh0qRJfPbZZ5aD1IT4qZSUFLUjiFMwmZxYv96PNWsC2bLl+DlRvL3N/OpXRrKyDEyd2mb1p6ekb22RvoU1BnWMjppuvfVWbr31VrVjCDtx8OBBJk6cqHYM8SNmM2ze7E1OTiD//a8fHR19B9jodAqZmX1LU+ed14yn58A/JyF9a4v0Laxhd4OOEANhNBrVjiD+v/JyN3JyAli7NoAjR44vTcXEdJKd3cQllxgIDz+901NI39oifQtryKAjHJqnp6faETStudmJL77wJycnkO3bvSy3+/qaueiivo+ET57cPmQn9pO+tUX6FtaQQUc4tMmTJ6sdQXN6eiAvz4ecnAC+/tqPrq6+pSlnZ4WZM1vJzm5i1qwW3N2H/np50re2SN/CGgO+BMTbb7/Np59+avn6vvvuIyAggJkzZ1rOWCnESHHsbLNi+O3f785zz4UxZ04Ct94awxdfBNDV5cS4cR0sXFjDf/9byiuvHOLCC5uHZcgB6VtrpG9hjQHv0XnqqacsF9HMy8vjpZde4oUXXmDdunXcfffdfPTRR0MeUggxMhkMznz2mT85OQHs2nV8aSogoIeLLjKSnd1EYmKHXHNKCKGaAQ86lZWVjBs3DoBPPvmEyy67jJtvvpkzzjiDWbNmDXU+IU6LnB5+6HV3w/ff+5KTE8CGDb709PTtGHZxUTjrrBays5s4++xWXF2HZ6/NqUjf2iJ9C2sMeNDx8fGhoaGB6Oho1q9fz9133w2Ah4fHaV10S4jhYIuzdmvFnj0erFkTwGefBdDYePytIzGxnezsJubONRIUZFYxofStNdK3sMaAB53Zs2dz0003MXXqVPbu3cvFF18MwK5du4iJiRnqfEKclv379zNq1Ci1Y9ithgZnPv2070KapaXHP+ESHNzNxRcbycpqIiGh8xSPYFvSt7ZI38IaAx50Xn75ZR555BEqKytZvXo1wcHBAGzZsoXf/va3Qx5QCGFbXV06vv22b2nqu+98MZv7DrBxde3l3HP7lqZmzmzFRT6zKYSwAzpFUWy/kK6S5uZm/P39ycvLw8fHR+04wgZaW1ulaysoCuza5cmaNQF8/rk/RuPxKWby5Daysw386ldG/P3VXZr6JdK3tkjf2tHa2kpmZiZGoxE/P78B3XfA/yb74osv8PHx4cwzzwT69vD87W9/IykpiZdfflmVi30K8XMqKytJTExUO8aIdfSoC+vW9S1NlZUdP95h1Khu5s3rO6Hf2LEjZ2nql0jf2iJ9C2sMeNC59957WbZsGQDFxcXcc889LFiwgK+//poFCxbw1ltvDXlIIQarqalJ7QgjTkeHjm++8WPNmgDy8nzo7e1bmnJ37+W885qZP9/A9OmtODurHHQQpG9tkb6FNQY86Bw8eJCkpCQAVq9ezSWXXMJTTz3F1q1bueiii4Y8oBCnw93dXe0II4KiwPbtnqxZE8h//uNPS8vxKWbqVBPZ2QbmzDHi69urYsrTJ31ri/QtrDHgQcfNzY22tjYA/vvf//K///u/AAQFBdHc3Dy06YQ4TampqWpHUFVNjStr1/YtTR06dPyXQkREl2VpasyYLhUTDi2t96010rewxoAHnTPPPJMFCxZwxhlnsHnzZlatWgXA3r175eRNYsTJy8tj5syZasewqbY2HV995c+aNQFs3uyNovQtTXl69jJ7tpHsbANpaSacBnwBmJFPi31rmfQtrDHgQeell17i1ltv5cMPP+TVV18lMjISgM8//5xf/epXQx5QCPHLenthyxYvcnICWb/ej7a240tT6emtZGUZmD27GW9v+16aEkKIgRrwoBMdHc26detOuP35558fkkBCDKWIiAi1IwyrykpX1q4NJCcngOpqN8vtUVFdZGU1kZVlIDKyW8WEtuXofYv+pG9hjUGd8stsNvPJJ59QUlKCTqcjMTGR7OxsnO3xYxrCofn6+qodYciZTE6sX+/HmjWBbNnibbnd29vMhRcaycoykJrapskLaTpi3+LnSd/CGgMedPbv389FF11EdXU1CQkJKIrC3r170ev1fPrpp8TFxQ1HTiEGZe/evYSEhKgd47T19kJ+vjc5OYF89ZUf7e19B9jodAozZpjIymri/POb8fTUzPk/T8pR+hbWkb6FNQY86Nxxxx3ExcWxadMmgoKCAGhoaOCaa67hjjvu4NNPPx3ykEJoVXm5Gzk5AaxdG8CRI8eXpmJiOsnObuKSS4yEh2tnaUoIIQZqwIPOt99+22/IAQgODubpp5/mjDPOGNJwQpyuSZMmqR1hwJqbnfjiC39ycgLZvt3Lcruvr5m5cw1kZxuYPLldk0tTv8Qe+xaDJ30Lawx40HF3d6elpeWE21tbW3FzczvJPYRQT01NzYCvi6KGnh7Iy/MhJyeAr7/2o6urb2nKyUnhjDNaycpq4txzW3B31/bS1C+xl77F0JC+hTUGPOhccskl3HzzzbzxxhtkZGQAkJ+fzy233EJWVtaQBxTidDQ0NKgd4ZT273cnJyeAdesCqKtztdw+blwH2dlNXHyxkdDQHhUT2peR3rcYWtK3sMaAB50XX3yR6667jszMTFxd+96Ye3p6yMrK4i9/+cuQBxTidLi4DOqDhcPKYHDms8/8yckJYNeu40tTAQE9XHSRkezsJhITO2RpahBGYt9i+Ejfwho6RVEGtS9837597NmzB0VRSEpKYty4cUOdbcg1Nzfj7+9PXl4ePj4+ascRGtLdDd9/70tOTgAbNvjS09O3NOXionDWWS1kZzdx9tmtuLrK0pQQQvxUa2srmZmZGI3GAS9XDnocjo+PJz4+frB3F8Im8vLyyMzMVO3nl5Z68MknAXz2WQCNjcdfbomJ7WRlGbjoIgNBQWbV8jkatfsWtiV9C2tYNegsWLDA6gd87rnnBh1GiKE2yB2Wp6WhwZlPP+27kGZpqafl9qCgHi65xEBWVhMJCZ02z6UFavQt1CN9C2tYNehs27bNqgfTyUEFYoQJCwuzyc/p6tLx7bd9S1PffeeL2dz3WnB17WXWrBaysw3MnNmCq+svPJA4LbbqW4wM0rewhlWDzjfffDPcOYQYFoGBgcP22IoCu3Z5smZNAJ9/7o/RePzlNHlyG1lZBubONeLvL0tTtjKcfYuRR/oW1pBD1oVD27NnDzNnzhzSxzx61IV16/qWpsrKPCy3jxrVzSWX9J3Qb+xYWZpSw3D0LUYu6VtYQwYdIazQ0aHjm2/8WLMmgLw8H3p7+5am3N17Oe+8ZrKzDcyY0Ypc11YIIUYWGXSEQ0tMTBz0fRUFtm/3ZM2aQP7zH39aWo5PMVOnmsjKMnDhhUZ8fXuHIqoYAqfTt7A/0rewhgw6wqE1NDQMeB2/psaVtWv7lqYOHXK33B4R0cW8eQaysgyMGdM11FHFEBhM38J+Sd/CGjLoCId29OhRq05m2dam46uv/FmzJoDNm71RlL6lKU/PXmbPNpKVZSA93YST03AnFqfD2r6FY5C+hTVk0BEOzekUk0lvL2zZ4kVOTiDr1/vR1nZ8aSo9vZWsLAOzZzfj7S1LU/biVH0LxyN9C2vYzaCzZMkSPv30U4qKinBzc8NgMKgdSdiBGTNmnHBbZaUra9cGkpMTQHW1m+X2qKgusrKamDfPQFRUty1jiiFysr6F45K+hTXsZtDp6uri8ssvJzMzkzfeeEPtOMJObN68mYyMDEwmJ9av92PNmkC2bPG2/L23t5kLL+xbmkpNbZMLadq5Y30LbZC+hTXsZtBZvHgxAH//+9/VDSLsRm8vFBeH8fHHUXz1lR/t7X27uXU6hRkzTGRlNXH++c14espp5B1FT0+P2hGEDUnfwhp2M+gMRmdnJ52dx0/c1tzcrGIaYUuFhV4sWhRFVdUky20xMZ1kZzdxySVGwsNlacoRhYSEqB1B2JD0Lazh0IPO0qVLLXuCfiw/Px9PT0/S09PZtWsXbW1t+Pn5ERcXZ7muV0xMDIqicOjQIQCmTZtGaWkpra2teHt7k5iYSGFhIQDR0dE4Oztz8OBBAFJSUjh48CBGoxFPT08mT57M5s2bAYiKisLDw4P9+/cDMGXKFCorK2lqasLd3Z3U1FTy8vIAiIiIwNfXl7179wIwadIkampqaGhowMXFhYyMDPLy8lAUhbCwMAIDA9mzZw/Qd36JhoYGjh49ipOTEzNmzGDz5s309PQQEhJCWFgYu3btAiAhIQGj0ciRI0cAmDlzJoWFhXR1dREUFERkZCTFxcVA31Xr29raqK6uBmD69Ols376djo4OAgICiImJoaioCICxY8fS3d1NZWUlAOnp6ezevRuTyYSvry/x8fFs3boVgDFjxgBYtndqair79u2jpaUFb29vkpKSKCgoAECv1+Pq6sqBAwcs27u8vByDwYCbmyeFhRfwyithKIoTXl5dnH9+HUlJBcTGHmXKlMlUV1dz4EAjbm5upKWlkZubC0B4eDj+/v6UlpYCMHHiRGpra6mvr7ds702bNtHb28uoUaMIDg6mpKQEgAkTJtDU1ERtbS06nY7MzEzL9g4ODiYiIoKdO3cCMH78eFpaWqipqQEgMzOTrVu30tnZSWBgIHq9nh07dgAwbtw4Ojo6qKqqAiAjI4Pi4mLa29vx9/cnNjbWsr1jY2Mxm81UVFQAkJaWRklJCSaTCR8fHxISEtiyZYtle+t0OsrLywGYOnUqZWVlNDc34+XlxcSJE/ttbzc3N8rKygBITk6moqKCpqYmPDw8SElJYdOmTQCMHj0ab29v9u3bd8Jz1tXVlfT09H7bOyAgwPKcTUpKoq6ujrq6OpydnZk+fTr5+fmYzWZCQ0MJDQ1l9+7dlu1tMBj6PWcLCgpoa2tDUZR+2zs+Ph6TycThw4eBvuM6ioqK6OjoIDAwkOjoaLZv3w5AXFwcXV1d/Z6z8h4xct8jmpubyc3NHdB7hIeHB8nJyeTn5wMQGRmJl5eX5Tk7eXLfe0Rjo7xHjKT3iPb2dgZLp6h4+dfHH3/8pIPIjxUUFJCWlmb5+u9//zt33XWXVQcjn2yPjl6vJy8vDx8fn0HnFiNTfb0zDzygJz+/r9u+pamPOe88WcPXitzcXLkkgIZI39rR2tpKZmYmRqMRPz+/Ad1X1T06t99+O1deeeUpvycmJmbQj+/u7o67u/svf6Owe5s2efPAA1E0NLji6dnLQw8dZv58A7m5soYvhBBapuqgExISImus4rSYzfDXv47itddCURQd48Z1sGJFJXFxfXvyEhISVE4obEn61hbpW1jDbo7RqaiooLGxkYqKCsxms2XNcdy4cbIMpVFHj7rwwANRFBT09f8//9PI/ffX9PsUldFoJDg4WK2Iwsakb22RvoU17GbQefTRR3n77bctX0+dOhWAb775hlmzZqmUSqjlhx98eOihKBobXfDyMvPoo4e5+GLjCd935MgRxo4dq0JCoQbpW1ukb2ENuxl0/v73v8s5dAQ9PfDyy2G8/nooAAkJ7axYUUlMjFxkUwghxInsZtAR4sgRF+6/X8/WrX1nNv7Nbxq4774juLv//AcH5RMZ2iJ9a4v0LawhV0QTdmHjRh8uv3wcW7d64+1tZvnyChYtqjnlkANYzmMitEH61hbpW1hD9uiIEa27G158MYy//71vqSoxsZ1nn61Er7duqaqrS5a0tET61hbpW1hDBh0xYh0+7Mq99+rZscMLgKuuauCee47g5mb9OS6DgoKGK54YgaRvbZG+hTVk0BEj0ldf+bJoURQtLc74+pp54olqLrhg4Ncqi4yMHIZ0YqSSvrVF+hbWkGN0xIjS3a1j2bJw7rprDC0tzkye3Mb77+8f1JADWK6/I7RB+tYW6VtYQ/boiBGjsrJvqWrXrr6lquuuq+fOO2txdVXtcmxCCCHsnAw6YkRYv96Pxx6LpLXVGX//Hp58sppZs1pO+3Hj4+OHIJ2wF9K3tkjfwhoy6AhVdXbqWL48nFWr+k7jnpJi4plnqoiI6B6Sx29raxuSxxH2QfrWFulbWEOO0RGqOXTIjWuuGWsZcn73uzrefPPgkA05ANXV1UP2WGLkk761RfoW1pA9OkIVn33mz+LFo2lrcyYwsIennqrizDNb1Y4lhBDCwcigI2yqo0PH009HsHp13/kvpk0zsWxZJWFhPcPy86ZPnz4sjytGJulbW6RvYQ1ZuhI2c+CAG1ddFcfq1UHodAo333yU118/OGxDDsD27duH7bHFyCN9a4v0Lawhe3SETeTkBPDkk6Npb3ciKKiHp5+uJDPTNOw/t6OjY9h/hhg5pG9tkb6FNWTQEcOqrU3HU0+NZs2aQACmT29l6dIqQkOHby/OjwUEBNjk54iRQfrWFulbWEMGHTFs9u93Z+FCPWVlHjg5Kdxyy1FuvrkOZ2fbZYiJibHdDxOqk761RfoW1pBjdMSQUxT46KNAfvvbOMrKPAgN7eb118v54x9tO+QAFBUV2fYHClVJ39oifQtryB4dMaRMJif+/OfRfPppAAAzZ7bw1FNVBAeb1Q0mhBBCk2TQEUOmtNSDhQv1lJe74+yscPvttfzud/U4qbjfcOzYser9cGFz0re2SN/CGjLoiNOmKPDBB4EsWxZBV5cTo0Z1s3x5Jamp6p+evbt76M6yLEY+6VtbpG9hDTlGR5yW1lYn7r1Xz5//HElXlxNnn93Mhx/uHxFDDkBlZaXaEYQNSd/aIn0La8geHTFou3f3LVVVVrrj4qJw551H+N//bVB1qUoIIYT4MRl0xIApCvz730GsWBFOd7cTERFdLF9eSXJyu9rRTpCenq52BGFD0re2SN/CGvJvbzEgzc1OLFigZ+nS0XR3O3Huuc188EHZiBxyAHbv3q12BGFD0re2SN/CGrJHR1ituNiTe+/VU13thotLL/fcU8vVVzeg06md7OeZTMN/mQkxckjf2iJ9C2vIoCN+kaLAP/8ZzPPPh9HT40RkZBcrVlQyadLI3IvzY76+vmpHEDYkfWuL9C2sIYOOOCWj0ZlHHolkwwY/AGbPNrJ4cTW+vr0qJ7NOfHy82hGEDUnf2iJ9C2vIMTriZxUVeXLZZXFs2OCHq2svDz98mGefrbSbIQdg69atakcQNiR9a4v0Lawhe3TECXp74e9/D+HFF8Mwm3VER3eyYkUliYkdakcTQgghBkQGHdFPY6MzDz8cxfff9619z51r4LHHDuPtbT97cX5szJgxakcQNiR9a4v0Lawhg46wKCz04v779Rw96oq7ey8PPFDD//xP04j+VJUQQghxKnKMjqC3F1auDOXGG2M5etSVmJhO3nmnjMsus/8h59ChQ2pHEDYkfWuL9C2sIXt0NK6+3pmHHtKTl+cDwLx5TTzySA1eXva5VCWEEEL8mAw6Gpaf780DD0RRX++Kh0ffp6rmzzeoHWtIpaamqh1B2JD0rS3St7CGLF1pkNkMr7wyit//Pob6elfGjevgvffKHG7IAdi3b5/aEYQNSd/aIn0La9jFoFNeXs6NN95IbGwsnp6exMXF8dhjj9HV1aV2NLtTV+fC738fw6uvjkJRdPz61428+24ZcXGdakcbFi0tLWpHEDYkfWuL9C2sYRdLV3v27KG3t5fXXnuNcePGsXPnTn7/+99jMplYsWKF2vHsRm6uDw8+GEVjowuenmYeffQwl1xiVDvWsPL29lY7grAh6VtbpG9hDZ2iKIraIQZj+fLlvPrqqxw4cMDq+zQ3N+Pv709eXh4+Pj7DmG5k6enpW6p6/fVQFEXH+PHtrFhRSWys4+8R6+7uxtXVVe0Ywkakb22RvrWjtbWVzMxMjEYjfn5+A7qvXSxdnYzRaCQoKOiU39PZ2Ulzc3O/P1pz5IgLN94Yy9/+1rdU9ZvfNPDOOwc0MeQAFBQUqB1B2JD0rS3St7CGXSxd/VRZWRn/93//x7PPPnvK71u6dCmLFy8+4fb8/Hw8PT1JT09n165dtLW14efnR1xcHNu2bQMgJiYGRVEs52mYNm0apaWltLa24u3tTWJiIoWFhQBER0fj7OzMwYMHAUhJSeHgwYMYjUY8PT2ZPHkymzdvBiAqKgoPDw/2798PwJQpU6isrKSpqQl3d3dSU1PJy8sDICIiAl9fX/bu3QvApEmTqKmpoaGhARcXFzIyMsjLy0NRFMLCwggMDGTPnj0AJCYmsn69C88/PxWTyQNvbzPXXLOR1NR9VFSEEBYWxq5duwBISEjAaDRy5MgRAGbOnElhYSFdXV0EBQURGRlJcXEx0HcRvba2NqqrqwGYPn0627dvp6Ojg4CAAGJiYigqKgJg7NixdHd3U1lZCUB6ejq7d+/GZDLh6+tLfHy85Vo1x85wemx7p6amsm/fPlpaWvD29iYpKcnypqbX63F1dbXszUtJSaG8vByDwYCHhwfJycnk5+cD0NbWRl1dneWgxcmTJ1NdXU1jYyNubm6kpaWRm5sLQHh4OP7+/pSWlgIwceJEamtrqa+vt2zvTZs20dvby6hRowgODqakpASACRMm0NTURG1tLTqdjszMTDZv3kxPTw/BwcFERESwc+dOAMaPH09LSws1NTUAZGZmsnXrVjo7OwkMDESv17Njxw4Axo0bR0dHB1VVVQBkZGRQXFxMe3s7/v7+xMbGWrZ3bGwsZrOZiooKANLS0igpKcFkMuHj40NCQgJbtmyxbG+dTkd5eTkAU6dOpaysjObmZry8vJg4cWK/7e3m5kZZWRkAycnJVFRU0NTUhIeHBykpKWzatAmA0aNH4+3tbdneP37Ourq6kp6e3m97BwQEWJ6zSUlJ1NXVUVdXh7OzM9OnTyc/Px+z2UxoaCihoaHs3r3bsr0NBkO/52xBQQGNjY2Ulpb2297x8fGYTCYOHz4MwIwZMygqKqKjo4PAwECio6PZvn07AHFxcXR1dfV7zjrye0RDQwNHjx7FycmJGTNmWJ6zISH28R7R3NxMbm7uab1HREZG4uXlJe8RI/w9or29ncFSdenq8ccfP+kg8mMFBQWkpaVZvj58+DDnnHMO55xzDq+//vop79vZ2Uln5/GDbJubm9Hr9Q6/dNXdDf/3f2G89VYoAImJfUtV0dHa2IvzY5WVlej1erVjCBuRvrVF+taO01m6UnWPzu23386VV155yu+JiYmx/Pfhw4c599xzyczMZOXKlb/4+O7u7ri7u59uTLty+LAr996rZ8cOLwCuuqqBe+45gpubXR6Kddpk/V5bpG9tkb6FNVQddEJCQggJCbHqe6urqzn33HOZNm0ab731Fk5Odnt40bD5+mtfFi2KpLnZBV9fM088Uc0FF2jvuKQfO3DgAOHh4WrHEDYifWuL9C2sYRfH6Bw+fJhZs2YRHR3NihUrqKurs/ydPMmhu1vHc8+F8a9/9Q2Nkya1sXx5JVFR3SonE0IIIdRlF4PO+vXr2b9/P/v37ycqKqrf39npp+OHTFVV31LVzp19S1XXXlvP3XfX4uqq7e1yTEpKitoRhA1J39oifQtr2MX6z/XXX4+iKCf9o2VffunHb34zjp07vfDz6+HFFw9x331HZMj5kWOfGBDaIH1ri/QtrGEXe3REf52dOlasCOe994IBSE7uW6qKiJClqp8yGAxqRxA2JH1ri/QtrCGDjp2pqHBj4UI9JSWeANxwQx1/+lMt8uGDk/Pw8FA7grAh6VtbpG9hDRl07Mjnn/uzePFoTCZnAgN7WLKkirPOalU71oiWnJysdgRhQ9K3tkjfwhp2cYyO1nV06Fi8eDT33afHZHImNdXEBx/slyHHCsfOfiq0QfrWFulbWEP26IxwBw/2LVXt3euJTqfw+9/X8cc/HsVFmhNCCCF+kfy6HMHWrvXnz38eTXu7M0FBPSxdWsnMmSa1Y9mVyMhItSMIG5K+tUX6FtaQQWcEamvTsXTpaD75JBCAjIxWnn66itDQHpWT2R8vLy+1Iwgbkr61RfoW1pBjdEaY/fvdueqqOD75JBCdTuHWW2tZubJchpxBOnaFXKEN0re2SN/CGrJHZ4RQFPjkkwCeemo0HR1OhIR0s2xZFRkZslQlhBBCDJYMOiNAW5sTf/7zaNatCwBg5swWnnqqiuBgs7rBHMDkyZPVjiBsSPrWFulbWEOWrlRWWurOFVfEsW5dAE5OCnfcUcurrx6SIWeIVFdXqx1B2JD0rS3St7CG7NFRiaLAhx8G8vTTEXR1OTFqVDfPPFPJtGltakdzKI2NjWpHEDYkfWuL9C2sIYOOClpbnXjiidF8/nkAAGed1cKSJVUEBspenKHm5uamdgRhQ9K3tkjfwhoy6NhYSYkHCxfqqahwx8Wlb6nquuvqcZJFxGGRlpamdgRhQ9K3tkjfwhry69VGFAX+/e8grr56LBUV7kREdPHWWwe44QYZcoZTbm6u2hGEDUnf2iJ9C2vIHh0baG524vHHI/nyS38AZs1q5sknq/H3l6UqIYQQYjjJoDPMdu70ZOFCPdXVbri49LJgQS3XXNOATqd2Mm0IDw9XO4KwIelbW6RvYQ0ZdIaJosC//hXMc8+F0dPjRGRkFytWVDJpUrva0TTF399f7QjChqRvbZG+hTXk6JBhYDQ6c8cd0TzzTAQ9PU7Mnm3k/ff3y5CjgtLSUrUjCBuSvrVF+hbWkD06Q6yoyJP77tNTU+OGq2sv9957hCuvbJSlKiGEEEIFMugMkd5eePvtEF58MYyeHh3R0Z2sWFFJYmKH2tE0beLEiWpHEDYkfWuL9C2sIUtXQ6CpyZnbbx/Dc8+F09OjY+5cA6tWlcmQMwLU1taqHUHYkPStLdK3sIYMOqdpyxYvLrtsHN9954u7ey+PPlrNsmVV+Pj0qh1NAPX19WpHEDYkfWuL9C2sIUtXg9TbC2+8EcrLL4/CbNYRE9PJihUVJCR0qh1N/IiLizzFtUT61hbpW1hDniWD0NDgzIMP6snL8wFg3rwmHnmkBi8v2Ysz0mRkZKgdQdiQ9K0t0rewhixdDdDmzd5cdtk48vJ88PDo5YknqliypFqGnBFq06ZNakcQNiR9a4v0Lawhe3SsZDbDypWh/PWvo+jt1REX18GKFZWMGydLVSNZb68MoFoifWuL9C2sIYOOFerqXHjwwSjy8/uWqi69tIkHHzyMp6eicjLxS0aNGqV2BGFD0re2SN/CGjLo/ILcXG8efFBPY6MLnp5mFi2qYd48g9qxhJWCg4PVjiBsSPrWFulbWEOO0fkZPT3w4oujuOWWGBobXRg/vp333iuTIcfOlJSUqB1B2JD0rS3St7CG7NE5iSNHXLj/fj1bt3oDcPnljdx3Xw0eHrJUJYQQQtgTGXR+4rvvfHjooSgMBhe8vc089thh5s41qh1LDNKECRPUjiBsSPrWFulbWEOWrv6/7m547rkwbr01BoPBhcTEdt5/v0yGHDvX1NSkdgRhQ9K3tkjfwhoy6AA1Na7ccMNY3norFIDf/raBf/7zANHRXSonE6dLroWjLdK3tkjfwhqaX7r65htfHnkkkuZmF3x9zSxeXM3s2c1qxxJDRKfTqR1B2JD0rS3St7CG3ezRycrKIjo6Gg8PDyIiIrj22ms5fPjwoB+vu1vHM8+Ec8cdY2hudmHSpDZWrdovQ46DyczMVDuCsCHpW1ukb2ENuxl0zj33XN5//31KS0tZvXo1ZWVlXHbZZYN6rOpqV667LpZ//jMEgGuuqecf/ziIXt89lJHFCLB582a1Iwgbkr61RfoW1rCbpau7777b8t9jxozhgQceYP78+XR3d+Pq6jqgx7r++rG0tnrh59fDk09Wc+65LUMdV4wQPT09akcQNiR9a4v0LaxhN4POjzU2NvLOO+8wc+bMAQ85AK2tzkyZ0sby5ZWMHi17cRyZnDlVW6RvbZG+hTXsZukK4P7778fb25vg4GAqKipYs2bNKb+/s7OT5ubmfn8Arr66nr///YAMORoQERGhdgRhQ9K3tkjfwho6RVFUO93v448/zuLFi0/5PQUFBaSlpQFQX19PY2Mjhw4dYvHixfj7+7Nu3bqfPfL+5x7/hRdewNPTk/T0dHbt2kVbWxt+fn7ExcWxbds2AGJiYlAUhUOHDgEwbdo0SktLaW1txdvbm8TERAoLCwGIjo7G2dmZgwcPApCSksLBgwcxGo14enoyefJky1pyVFQUHh4e7N+/H4ApU6ZQWVlJU1MT7u7upKamkpeXB/S9iH19fdm7dy8AkyZNoqamhoaGBlxcXMjIyCAvLw9FUQgLCyMwMJA9e/YAkJiYSENDA0ePHsXJyYkZM2awefNmenp6CAkJISwsjF27dgGQkJCA0WjkyJEjAMycOZPCwkK6uroICgoiMjKS4uJiAOLj42lra6O6uhqA6dOns337djo6OggICCAmJoaioiIAxo4dS3d3N5WVlQCkp6eze/duTCYTvr6+xMfHs3XrVqBvORKwbO/U1FT27dtHS0sL3t7eJCUlUVBQAIBer8fV1ZUDBw5Ytnd5eTkGgwEPDw+Sk5PJz88HoK2tjeTkZPbt2wfA5MmTqa6uprGxETc3N9LS0sjNzQUgPDwcf39/SktLAZg4cSK1tbXU19dbtvemTZvo7e1l1KhRBAcHW05BP2HCBJqamqitrUWn05GZmWnZ3sHBwURERLBz504Axo8fT0tLCzU1NUDfAZVbt26ls7OTwMBA9Ho9O3bsAGDcuHF0dHRQVVUFQEZGBsXFxbS3t+Pv709sbKxle8fGxmI2m6moqAAgLS2NkpISTCYTPj4+JCQksGXLFsv21ul0lJeXAzB16lTKyspobm7Gy8uLiRMn9tvebm5ulJWVAZCcnExFRQVNTU14eHiQkpLCpk2bABg9ejTe3t6W7f3j56yrqyvp6en9tndAQIDlOZuUlERdXR11dXU4Ozszffp08vPzMZvNhIaGEhoayu7duy3b22Aw9HvOFhQUUFtbS3x8fL/tHR8fj8lksnx4YcaMGRQVFdHR0UFgYCDR0dFs374dgLi4OLq6uvo9Z+U9YuS+R2zYsAE/P7/Teo+IjIzEy8tL3iNG+HtEe3s7d911F0ajET8/PwZC1UGnvr6e+vr6U35PTEwMHh4eJ9xeVVWFXq8nNzf3Z4+87+zspLOz0/J1c3Mzer2evLw8fHx8Ti+8sAu5ubnMnDlT7RjCRqRvbZG+taO1tZXMzMxBDTqqHqMTEhJCSEjIoO57bD778SDzU+7u7ri7uw/q8YVjGD9+vNoRhA1J39oifQtr2MUxOps3b+all16iqKiIQ4cO8c0333DVVVcRFxcn51EQp9TSIp+o0xLpW1ukb2ENuxh0PD09+eijjzj//PNJSEjgd7/7HZMmTeLbb7+VPTbilI6tcQttkL61RfoW1rCLj5dPnjyZr7/+Wu0YQgghhLAzdrFHR4jBkqVNbZG+tUX6FtaQQUc4tGMfTRXaIH1ri/QtrCGDjnBop/pUnnA80re2SN/CGjLoCIcWGBiodgRhQ9K3tkjfwhoy6AiHptfr1Y4gbEj61hbpW1hDBh3h0I6dJl1og/StLdK3sIYMOkIIIYRwWHZxHh0hBmvcuHFqRxA2JH07lmOX+vm5SzKOHz+e3t5eW0YSw+TYxbl/7iLdp0MGHeHQOjo61I4gbEj6dhyKomA2m0/5i8/Pz+9nhyBhXxRFQVEUnJ2dh3zYkUFHOLSqqiqio6PVjiFsRPp2DIqi0NPTg6enJ8HBwbi4uJz0l19bWxteXl4qJBRD6VjfDQ0NtLe3/2zfgyWDjhBCiBFFURScnJwICwvDw8PjZ7+vu7tbrnfoQFxcXKioqBjyx5WDkYVDy8jIUDuCsCHp27H80r/qvb29bZRE2MKxvod6OVIGHeHQiouL1Y4gbEj61pb29na1Iwg7IIOOcGjyRqgt0re2yCeuhDVk0BEOzd/fX+0Iwoakb21xdnZWO4KwAzLoCIcWGxurdgRhQ9K3tozEA5FvvvlmfvOb36gdw+Lbb79l/vz5REVFERwcTGpqKg888ADV1dVqR7MZGXSEQysqKlI7grAh6Vtb2tra1I4wor3++utcfPHFhIWF8e6777J161ZefPFFjEYjL7744qAft6urawhTDj8ZdIQQQox4igImkzp/hvJDQN999x1nnXUWAQEBxMbGsmjRInp6eix///HHH5Oenk5QUBBRUVFcfPHFmEwmADZu3MhZZ51FSEgIERERnHfeeT/7ceyqqioWLlzIrbfeymuvvcbZZ5/NmDFjOPPMM3n11Vd58MEHAXjyySeZPn16v/u+9NJLTJgwwfL1sb1Uy5cvZ+zYsUyZMoVHH32Uc84554Sfm5GRwZ///GfL1//4xz+YOnUqgYGBpKSk8Nprrw1+4w2SnEdHODRZytAW6dtxtbVBaKjnT2796dfDo66unaH4JHt1dTWXXnop11xzDa+//jqlpaXcfvvtuLu788gjj1BTU8N1113HkiVLyMrKoqWlhR9++MFyQr0rrriCG264gbfffpuuri4KCwt/9iP4H3/8MV1dXdx9990n/fuAgIABZd+wYQN+fn6sW7fO8vHvFStWcODAAcaOHQvA7t272blzJ++88w4Ab775Jk8++STPPfccKSkpFBUVcfvtt+Pt7c0111wzoJ9/OmTQEQ7NbDarHUHYkPQtRrKVK1cSFRXF888/j06nIyEhgZqaGhYtWsRDDz3EkSNH6OnpITs723KG70mTJgHQ2NiI0Whk7ty5lsHix3tdfmr//v34+fkRERExJNm9vLx45ZVXcHNzs9w2efJkVq1aZdk79N577zFt2jTi4+MBePrpp3n66aeZP38+ADExMezZs4c33nhDBh0hhkpFRQVRUVFqxxA2In07Li+vvj0rP9ba2oqPj49NfvZQKC0tJSMjo99emMzMTFpbW6murmbKlCmce+65pKenc8EFF3D++edz6aWXEhgYSFBQENdccw1ZWVmcd955nHfeefz617/+2UFGUZQhvYzCpEmT+g05AFdccQX/+Mc/ePDBB1EUhQ8++IDbb78dgLq6OqqqqvjjH//IbbfdZrlPT0+PzT8dKYOOEEKIEU+n44TlI0U58baR7GTDx7FlIJ1Oh7OzM+vWrWPTpk3897//5a9//SuLFy/m22+/JSYmhpUrV3Lrrbfy5Zdf8uGHH7J48WLWrVt30jOCx8fHYzQaqampOeVeHSenEw/V7e7uPuG2k11T7De/+Q2LFi1i27ZtdHR0UFVVxWWXXQYcP8fRyy+/THp6er/72fq0AHIwsnBoaWlpakcQNiR9a4u9XdBzwoQJ5Ofn97vEwaZNm/D19WX06NFA38CTmZnJokWLyMvLw9XVlZycHMv3p6SkcO+99/LNN9+QlJTEqlWrTvqzLr30Utzc3Hj++edP+vcGgwGAkJAQamtr+2XasWOHVf8/UVFRnHnmmaxatYr33nuPc889l7CwMADCwsIYPXo0Bw8eJC4urt+fmJgYqx5/qMgeHeHQSkpKSE5OVjuGsBHpW1s6OjpG5LDT3NzM9u3b+90WFBTEzTffzMsvv8yCBQu45ZZb2Lt3L0uWLOFPf/oTTk5ObN68mQ0bNnD++eczatQoCgoKqK+vJyEhgfLyct58800uvvhiIiIi2Lt3L/v37+eqq646aYaoqCiWLVvGggULaG5u5uqrr2bMmDFUV1fzzjvv4OPjw9NPP83ZZ5/N3XffzXPPPcell17K+vXrWb9+Pb6+vlb9v15xxRUsWbKE7u5uli1b1u/vHn74YRYuXIifnx9z5syhs7OTrVu3YjAYuOOOOwa3cQdBBh3h0I59LFNog/StLSP1EhAbN24kMzOz323XXHMNK1eu5OOPP+ahhx5i+vTpBAYGct111/HAAw8A4Ofnx/fff8/LL79Mc3Mz0dHRLF26lAsvvJDa2lpKS0v517/+RWNjI+Hh4fzhD3/gpptu+tkcf/jDH4iPj+eFF17gyiuvpL29nTFjxjB37lz+9Kc/AX17mV544QWWL19uOXD4zjvv5M0337Tq//XXv/4199xzD87OzsybN6/f391www14eXnx/PPP8/DDD+Pt7c3EiRP7HbNjCzplqC8TOoI1Nzfj7+9PXl6eTQ5gE+rbsWMHU6ZMUTuGsBHp2zH09vaiKApjxow55dmP29raRuQeHTE4nZ2dHDp0CJ1Od8KxQ62trWRmZmI0GvHz8xvQ48oxOsKhJSQkqB1B2JD0rS0eHh5qRxB2QAYd4dC2bNmidgRhQ9K3tsglIIQ1ZNARQgghhMOSQUc4tDFjxqgdQdiQ9K0tPz2BnRAnI4OOcGhDeWZQMfJJ345FQ5+VEQxf3zLoCIdWXl6udgRhQ9K3Y9DpdCiKQkdHxym/r6ury0aJhC10dHQM+aUrQM6jI4QQYoQ59ouurq4O6Pt01cl++XV3d9PZ2WnTbGLoHRtqj/Utg44QAzB16lS1Iwgbkr4dh7OzM2azmdra2p/9xTcc//oX6ji2bDUc18GSpSvh0MrKytSOIGxI+nYcOp0OFxcXnJ2d0el0J/1TVlb2s38nf+zrj7OzMy4uLsMyuMoeHeHQmpub1Y4gbEj6djzHfhGeTENDg5wkUvwiu9uj09nZSUpKCjqdjqKiIrXjiBFOTg+vLdK3tkjfwhp2N+jcd999lsvZC/FLJk6cqHYEYUPSt7ZI38IadjXofP7556xfv54VK1aoHUXYiYKCArUjCBuSvrVF+hbWsJtjdGpra/n973/PJ598YvXuys7Ozn4fPTQajQCYTKZhyShGnvb2dlpbW9WOIWxE+tYW6Vs7jv3eHsxJBe1i0FEUheuvv55bbrmFtLQ0q08KtnTpUhYvXnzC7RdccMEQJxRCCCHEcGtoaMDf339A99EpKp5j+/HHHz/pIPJjBQUF5ObmsmrVKjZu3IizszPl5eXExsaybds2UlJSfva+P92jYzAYGDNmDBUVFQPeUML+NDc3o9frqaysxM/PT+04YphJ39oifWuL0WgkOjqapqYmAgICBnRfVQed+vp66uvrT/k9MTExXHnllaxdu7bfRwzNZjPOzs5cffXVvP3221b9vObmZvz9/TEajfLC0ADpW1ukb22RvrXldPpWdekqJCSEkJCQX/y+F198kSeffNLy9eHDh7nwwgtZtWoV06dPH86IQgghhLBjdnGMTnR0dL+vfXx8AIiLiyMqKkqNSEIIIYSwA3b18fLT5e7uzmOPPYa7u7vaUYQNSN/aIn1ri/StLafTt6rH6AghhBBCDCdN7dERQgghhLbIoCOEEEIIhyWDjhBCCCEclgw6QgghhHBYmhl0XnnlFWJjY/Hw8GDatGl89913akcSw2Tjxo3MmzeP0aNHo9Pp+OSTT9SOJIbJ0qVLSU9Px9fXl1GjRjF//nxKS0vVjiWGyauvvsqUKVPw8/PDz8+PzMxMPv/8c7VjCRtZunQpOp2Ou+66a0D308Sgs2rVKu666y4efvhhtm3bxllnncXcuXOpqKhQO5oYBiaTieTkZF566SW1o4hh9u2333LbbbexadMmvvzyS3p6epgzZ45cuNdBRUVF8fTTT1NYWEhhYSHnnXce2dnZ7Nq1S+1oYpgVFBSwcuVKpkyZMuD7auLj5dOnTyc1NZVXX33VcltiYiLz589n6dKlKiYTw02n0/Hxxx8zf/58taMIG6irq2PUqFF8++23nH322WrHETYQFBTE8uXLufHGG9WOIoZJa2srqampvPLKKzz55JOkpKTwwgsvWH1/h9+j09XVxZYtW5gzZ06/2+fMmUNubq5KqYQQw8FoNAJ9v/yEYzObzbz33nuYTCYyMzPVjiOG0W233cbFF1/MBRdcMKj728UlIE5HfX09ZrOZsLCwfreHhYVx5MgRlVIJIYaaoigsWLCAM888k0mTJqkdRwyT4uJiMjMz6ejowMfHh48//pikpCS1Y4lh8t5777F161YKCgoG/RgOP+gc8+Mrn0Pfm+JPbxNC2K/bb7+dHTt28P3336sdRQyjhIQEioqKMBgMrF69muuuu45vv/1Whh0HVFlZyZ133sn69evx8PAY9OM4/KATEhKCs7PzCXtvjh49esJeHiGEffrTn/5ETk4OGzdulAv9Ojg3NzfGjRsHQFpaGgUFBfzlL3/htddeUzmZGGpbtmzh6NGjTJs2zXKb2Wxm48aNvPTSS3R2duLs7PyLj+Pwx+i4ubkxbdo0vvzyy363f/nll8ycOVOlVEKIoaAoCrfffjsfffQRX3/9NbGxsWpHEjamKAqdnZ1qxxDD4Pzzz6e4uJiioiLLn7S0NK6++mqKioqsGnJAA3t0ABYsWMC1115LWloamZmZrFy5koqKCm655Ra1o4lh0Nrayv79+y1fHzx4kKKiIoKCgoiOjlYxmRhqt912G++++y5r1qzB19fXsufW398fT09PldOJofbQQw8xd+5c9Ho9LS0tvPfee2zYsIEvvvhC7WhiGPj6+p5wvJ23tzfBwcEDOg5PE4POFVdcQUNDA0888QQ1NTVMmjSJzz77jDFjxqgdTQyDwsJCzj33XMvXCxYsAOC6667j73//u0qpxHA4dsqIWbNm9bv9rbfe4vrrr7d9IDGsamtrufbaa6mpqcHf358pU6bwxRdfMHv2bLWjiRFME+fREUIIIYQ2OfwxOkIIIYTQLhl0hBBCCOGwZNARQgghhMOSQUcIIYQQDksGHSGEEEI4LBl0hBBCCOGwZNARQgghhMOSQUcIcVKzZs3irrvuOq3HKC8vR6fTUVRUNCSZBqutrY3/+Z//wc/PD51Oh8FgsOp+jz/+OCkpKcOaTQgxvDRxZmQhxMB99NFHuLq6qh1jSLz99tt899135ObmEhISgr+//7D9LJ1Ox8cff8z8+fOH7WcIIawng44Q4qSCgoLUjjBkysrKSExMHND1cYQQjkGWroQQJ/XTpauYmBieeuopfve73+Hr60t0dDQrV67sd5/NmzczdepUPDw8SEtLY9u2bSc87u7du7nooovw8fEhLCyMa6+9lvr6egA2bNiAm5sb3333neX7n332WUJCQqipqfnZrKtXr2bixIm4u7sTExPDs88+2+//49lnn2Xjxo3odLoTrov1Y08//TRhYWH4+vpy44030tHR0e/vCwoKmD17tmWv0DnnnMPWrVv7bSOASy+9FJ1OZ/m6rKyM7OxswsLC8PHxIT09nf/+978/m0MIMYQUIYQ4iXPOOUe58847LV+PGTNGCQoKUl5++WVl3759ytKlSxUnJyelpKREURRFaW1tVUJDQ5UrrrhC2blzp7J27Vpl7NixCqBs27ZNURRFOXz4sBISEqI8+OCDSklJibJ161Zl9uzZyrnnnmv5Offee68yZswYxWAwKEVFRYq7u7vy0Ucf/WzOwsJCxcnJSXniiSeU0tJS5a233lI8PT2Vt956S1EURWloaFB+//vfK5mZmUpNTY3S0NBw0sdZtWqV4ubmpvztb39T9uzZozz88MOKr6+vkpycbPmer776SvnnP/+p7N69W9m9e7dy4403KmFhYUpzc7OiKIpy9OhRBVDeeustpaamRjl69KiiKIpSVFSk/PWvf1V27Nih7N27V3n44YcVDw8P5dChQwOtRQgxQDLoCCFO6mSDzjXXXGP5ure3Vxk1apTy6quvKoqiKK+99poSFBSkmEwmy/e8+uqr/QadRYsWKXPmzOn3cyorKxVAKS0tVRRFUTo7O5WpU6cqv/nNb5SJEycqN9100ylzXnXVVcrs2bP73XbvvfcqSUlJlq/vvPNO5Zxzzjnl42RmZiq33HJLv9umT5/eb9D5qZ6eHsXX11dZu3at5TZA+fjjj0/5sxRFUZKSkpT/+7//+8XvE0KcHlm6EkJYbcqUKZb/1ul0hIeHc/ToUQBKSkpITk7Gy8vL8j2ZmZn97r9lyxa++eYbfHx8LH8mTJgA9C3vALi5ufGvf/2L1atX097ezgsvvHDKTCUlJZxxxhn9bjvjjDPYt28fZrPZ6v+3kpKSE/L+9OujR49yyy23MH78ePz9/fH396e1tZWKiopTPrbJZOK+++4jKSmJgIAAfHx82LNnzy/eTwhx+uRgZCGE1X76KSydTkdvby8AiqL84v17e3uZN28ey5YtO+HvIiIiLP+dm5sLQGNjI42NjXh7e//sYyqKgk6nO+G24XD99ddTV1fHCy+8wJgxY3B3dyczM5Ourq5T3u/ee+/lP//5DytWrGDcuHF4enpy2WWX/eL9hBCnT/boCCGGRFJSEtu3b6e9vd1y26ZNm/p9T2pqKrt27SImJoZx48b1+3NsmCkrK+Puu+/mb3/7GzNmzOB///d/LcPUz/3c77//vt9tubm5jB8/HmdnZ6vzJyYmnpD3p19/99133HHHHVx00UWWg5+PHUh9jKur6wl7kr777juuv/56Lr30UiZPnkx4eDjl5eVWZxNCDJ4MOkKIIXHVVVfh5OTEjTfeyO7du/nss89YsWJFv++57bbbaGxs5Le//S2bN2/mwIEDrF+/nt/97neYzWbMZjPXXnstc+bM4YYbbuCtt95i586d/T5F9VP33HMPX331FX/+85/Zu3cvb7/9Ni+99BILFy4cUP4777yTN998kzfffJO9e/fy2GOPsWvXrn7fM27cOP75z39SUlJCfn4+V199NZ6env2+JyYmhq+++oojR47Q1NRkud9HH31EUVER27dv56qrrjrl8CaEGDoy6AghhoSPjw9r165l9+7dTJ06lYcffviEJarRo0fzww8/YDabufDCC5k0aRJ33nkn/v7+ODk5sWTJEsrLyy0fWw8PD+f111/nkUce+dmzK6empvL+++/z3nvvMWnSJB599FGeeOIJrr/++gHlv+KKK3j00Ue5//77mTZtGocOHeKPf/xjv+958803aWpqYurUqVx77bXccccdjBo1qt/3PPvss3z55Zfo9XqmTp0KwPPPP09gYCAzZ85k3rx5XHjhhaSmpg4onxBicHTKcC1mCyGEEEKoTPboCCGEEMJhyaAjhBBCCIclg44QQgghHJYMOkIIIYRwWDLoCCGEEMJhyaAjhBBCCIclg44QQgghHJYMOkIIIYRwWDLoCCGEEMJhyaAjhBBCCIclg44QQgghHJYMOkIIIYRwWP8PC15234DG3BoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.cla()  # Clear the current axes\n",
    "\n",
    "# Set figure and axis properties\n",
    "# plt.figure()\n",
    "plt.xlim(0, 4)  # Set x-axis limits\n",
    "plt.xticks(range(5))  # Set x-ticks to show 0, 1, 2, 3, 4\n",
    "plt.ylim(-4, 4)  # Set y-axis limits\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"index of data\")  # Label for x-axis\n",
    "plt.ylabel(\"loss value\")      # Label for y-axis\n",
    "\n",
    "# Plotting data (assuming you have your data ready)\n",
    "# plt.plot(data_x, data_y)  # Replace with your actual plot command\n",
    "\n",
    "# Example data (replace these with your actual data)\n",
    "data_x = [0, 1, 2, 3, 4]  # x-axis values\n",
    "data_y = [-3, -1, 0, 2, 3]  # y-axis values\n",
    "\n",
    "# Plotting the data\n",
    "plt.plot(data_x, data_y, label='Loss Curve', color='blue')  # Replace with your actual plot command\n",
    "\n",
    "\n",
    "# Set background color to gray\n",
    "plt.gca().set_facecolor('lightgray')\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('loss_e_maml/loss_plot.png')  # Save as PNG\n",
    "plt.savefig('loss_e_maml/loss_plot.pdf')  # Save as PDF\n",
    "\n",
    "plt.pause(0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Meta-Learning: Synthetic_E-Reptile\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6k/t6s5_7b156500_twxhplmtjr0000gn/T/ipykernel_34336/3147284780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;31m# experiment('E-MAML', True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;31m# experiment('Synthetic_E-Reptile', False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Synthetic_E-Reptile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/6k/t6s5_7b156500_twxhplmtjr0000gn/T/ipykernel_34336/3147284780.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(run, plot)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;31m# Choose a fixed task and minibatch for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mf_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0;31m# xtrain_plot = x_all[rng.choice(len(x_all), size=n_train)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mxtrain_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6k/t6s5_7b156500_twxhplmtjr0000gn/T/ipykernel_34336/3147284780.py\u001b[0m in \u001b[0;36mgen_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sama dengan gen_task_eg_maml_base_val_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0melm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mELMRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mload_model_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'model_reg/model_Comb-KMT-Tiny-Reg.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melm_model\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ganti dengan nama sheet yang sesuai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd as ag\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "# declare params\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "def experiment(run, plot=True):\n",
    "    print('Type of Meta-Learning:', run)\n",
    "    seed = 0\n",
    "    inner_step_size = 0.02  # stepsize in inner SGD\n",
    "    inner_epochs = 1  # number of epochs of each inner SGD\n",
    "    outer_stepsize_reptile = 0.1  # stepsize of outer optimization, i.e., meta-optimization\n",
    "    outer_stepsize_maml = 0.01\n",
    "    # n_iterations = 30000  # number of outer updates; each iteration we sample one task and update on it\n",
    "    n_iterations = 10  # number of outer updates; each iteration we sample one task and update on it\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Define task distribution\n",
    "    n_data_all = 5\n",
    "    n_sample = n_data_all # minimum 1, maks = n_data_all\n",
    "    idx_x_all = np.arange(0,n_data_all)[:,None]\n",
    "    \n",
    "    # All of the x points data, dengan fitur input pepanjang n_input = 14\n",
    "    x_all = get_data_test(np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None])\n",
    "    # n_train = 10  # Size of training minibatches\n",
    "    n_train = 3  # Size of training minibatches, harus < n_data_all\n",
    "       \n",
    "    def get_mse_or_loss_val(get_idx_x_all_in):\n",
    "        x = to_torch(np.array([x_all[i] for i in get_idx_x_all_in]))\n",
    "        y = to_torch(np.array([y_all[i] for i in get_idx_x_all_in]))\n",
    "\n",
    "        # cara 1\n",
    "        model.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        individual_losses = (y_pred - y).pow(2).mean(dim=1)  # Loss for each sample along feature dimension\n",
    "        # print(\"Individual losses:\", individual_losses)\n",
    "\n",
    "        return individual_losses.data.numpy()\n",
    "    \n",
    "#     def get_idx_x_all(x_all_in, x_all_to_search):\n",
    "#         # Data utama yang akan dicari indeksnya\n",
    "#         # x_all = [\n",
    "#         #     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "#         #     [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "#         #     [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "#         #     [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "#         #     [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
    "#         # ]\n",
    "\n",
    "#         # List untuk menyimpan indeks hasil\n",
    "#         idx_result = []\n",
    "\n",
    "#         # Cari indeks dari setiap elemen di x_all_in dalam x_all\n",
    "#         for x_in in x_all_in:\n",
    "#             if x_in in x_all_to_search:\n",
    "#                 idx_result.append(x_all_to_search.index(x_in))\n",
    "\n",
    "#         return idx_result\n",
    "    \n",
    "    def get_idx_x_all(x_all_in, x_all_to_search):\n",
    "        idx_result = []\n",
    "\n",
    "        # Iterate through each element in x_all_in\n",
    "        for x_in in x_all_in:\n",
    "            # Iterate through x_all_to_search to find a matching element\n",
    "            for i, x in enumerate(x_all_to_search):\n",
    "                # Use np.array_equal to compare arrays element-wise\n",
    "                if np.array_equal(x_in, x):\n",
    "                    idx_result.append(i)\n",
    "                    break  # Stop after the first match is found\n",
    "\n",
    "        return idx_result\n",
    "\n",
    "    def gen_task_eg_maml_base_idx_data():\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda idx_x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, get_data_test(idx_x_single[0]))\n",
    "             for idx_x_single in idx_x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task_eg_maml_base_val_data():\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task(): # sama dengan gen_task_eg_maml_base_val_data()\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "\n",
    "    # Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "    #     model = nn.Sequential(\n",
    "    #         nn.Linear(1, 64),\n",
    "    #         nn.Tanh(),\n",
    "    #         nn.Linear(64, 64),\n",
    "    #         nn.Tanh(),\n",
    "    #         nn.Linear(64, 1),\n",
    "    #     )\n",
    "    \n",
    "    # Define sintesis model. Reptile dengan ELMRegressionForReptile\n",
    "    model = ModelForSyntheticReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        \n",
    "    def save_model_reptile_checkpoint(model, filename):\n",
    "        # Dapatkan state_dict dari model\n",
    "        model_state = model.state_dict()\n",
    "\n",
    "        # Konversi tensor menjadi list untuk serialisasi JSON\n",
    "        model_state_serializable = {k: v.numpy().tolist() for k, v in model_state.items()}\n",
    "\n",
    "        # Simpan model ke file JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(model_state_serializable, f)\n",
    "\n",
    "    def load_model_reptile_checkpoint(model, filename):\n",
    "        # Muat model dari file JSON\n",
    "        with open(filename, 'r') as f:\n",
    "            model_state_serializable = json.load(f)\n",
    "\n",
    "        # Konversi kembali dari list ke tensor\n",
    "        model_state = {k: torch.tensor(np.array(v)) for k, v in model_state_serializable.items()}\n",
    "\n",
    "        # Memuat state_dict ke model\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()  # Set model ke mode evaluasi\n",
    "\n",
    "    def to_torch(x):\n",
    "        return ag.Variable(torch.Tensor(x))\n",
    "\n",
    "    # def train_on_batch(x, y):\n",
    "    #     x = to_torch(x)\n",
    "    #     y = to_torch(y)\n",
    "    #     model.zero_grad()\n",
    "    #     ypred = model(x)\n",
    "    #     loss = (ypred - y).pow(2).mean()\n",
    "    #     loss.backward()\n",
    "    #     for param in model.parameters():\n",
    "    #         param.data -= inner_step_size * param.grad.data\n",
    "    \n",
    "               \n",
    "    # using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "    def train_on_batch_eg_maml(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "            \n",
    "    def train_on_batch(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "        \n",
    "    # Cara Memuat model dari checkpoint\n",
    "    #     try:\n",
    "    #         load_model_reptile_checkpoint(model, filename_ckpt)\n",
    "    #         print(\"Model berhasil dimuat dari:\", filename_ckpt)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Terjadi kesalahan saat memuat model:\", e)\n",
    "\n",
    "    #     # Sekarang Anda bisa menggunakan model untuk melakukan prediksi atau melanjutkan pelatihan\n",
    "    #     # Contoh prediksi\n",
    "    #     x_test = to_torch([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]])  # Ganti dengan data yang sesuai\n",
    "    #     model.eval()  # Set model ke mode evaluasi\n",
    "    #     with torch.no_grad():\n",
    "    #         prediction = model(x_test)\n",
    "    #         print(\"Hasil prediksi:\", prediction.numpy())\n",
    "\n",
    "    def predict(x):\n",
    "        x = to_torch(x)\n",
    "        return model(x).data.numpy()\n",
    "\n",
    "    # Choose a fixed task and minibatch for visualization\n",
    "    f_plot = gen_task()\n",
    "    # xtrain_plot = x_all[rng.choice(len(x_all), size=n_train)]\n",
    "    xtrain_plot = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])\n",
    "\n",
    "    # Training loop\n",
    "    for iteration in range(n_iterations):\n",
    "        weights_before = deepcopy(model.state_dict())\n",
    "\n",
    "        # Generate task\n",
    "        f = gen_task()\n",
    "        y_all = f(x_all)\n",
    "\n",
    "        # Do SGD on this task\n",
    "        inds = rng.permutation(len(x_all))\n",
    "        train_ind = inds[:-1 * n_train]\n",
    "        val_ind = inds[-1 * n_train:]       # Val contains 1/5th of the gt model (com. model)\n",
    "\n",
    "        for _ in range(inner_epochs):\n",
    "            for start in range(0, len(train_ind), n_train):\n",
    "                mbinds = train_ind[start:start + n_train]\n",
    "                print('mbinds =', mbinds)\n",
    "                print()\n",
    "                # print('x_all[mbinds] =', x_all[mbinds])\n",
    "                # print()\n",
    "                # print('y_all[mbinds] =', y_all[mbinds])\n",
    "                x_all_mbinds = np.array([x_all[i] for i in mbinds])\n",
    "                y_all_mbinds = np.array([y_all[i] for i in mbinds])\n",
    "                # train_on_batch(x_all[mbinds], y_all[mbinds])\n",
    "                train_on_batch(x_all_mbinds, y_all_mbinds)\n",
    "                \n",
    "                print('=======================')\n",
    "\n",
    "        if run == 'E-MAML':\n",
    "            outer_step_size = outer_stepsize_maml * (1 - iteration / n_iterations)  # linear schedule\n",
    "            for start in range(0, len(val_ind), n_train):\n",
    "                dpinds = val_ind[start:start + n_train]\n",
    "                print('dpinds =', dpinds)\n",
    "                print()\n",
    "                \n",
    "                # x = to_torch(x_all[dpinds])\n",
    "                x = to_torch(np.array([x_all[i] for i in dpinds]))\n",
    "                \n",
    "                # y = to_torch(y_all[dpinds])\n",
    "                y = to_torch(np.array([y_all[i] for i in dpinds]))\n",
    "\n",
    "                # Compute the grads\n",
    "                model.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = (y_pred - y).pow(2).mean()\n",
    "                loss.backward()\n",
    "                \n",
    "\n",
    "                # Reload the model\n",
    "                model.load_state_dict(weights_before)\n",
    "\n",
    "                # SGD on the params\n",
    "                for param in model.parameters():\n",
    "                    param.data -= outer_step_size * param.grad.data\n",
    "            # print(weights_before)\n",
    "        else:\n",
    "            # Interpolate between current weights and trained weights from this task\n",
    "            # I.e. (weights_before - weights_after) is the meta-gradient\n",
    "            weights_after = model.state_dict()\n",
    "            outerstepsize = outer_stepsize_reptile * (1 - iteration / n_iterations)  # linear schedule\n",
    "            model.load_state_dict({name: weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize\n",
    "                                   for name in weights_before})\n",
    "\n",
    "        # Periodically plot the results on a particular task and minibatch\n",
    "        # if (plot and ((iteration == 0) or ((iteration + 1) % 1000 == 0))):\n",
    "        if (plot and ((iteration == 0) or ((iteration + 1) % n_iterations == 0))):\n",
    "            plt.cla()\n",
    "            f = f_plot\n",
    "            weights_before = deepcopy(model.state_dict())  # save snapshot before evaluation\n",
    "            \n",
    "            # plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0, 0, 1))\n",
    "            get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "            get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "            plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"pred after 0\", color=(0, 0, 1))\n",
    "            \n",
    "            for inneriter in range(32):\n",
    "                train_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "                if (inneriter + 1) % 8 == 0:\n",
    "                    frac = (inneriter + 1) / 32\n",
    "                    # plt.plot(x_all, predict(x_all), label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "                    \n",
    "                    get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "                    get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "                    plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "            \n",
    "            # plt.plot(x_all, f(x_all), label=\"true\", color=(0, 1, 0))\n",
    "            # plt.plot(x_all, f(x_all), label=\"ground truth from sin(x)\", color=(0, 1, 0))\n",
    "            \n",
    "            get_idx_x_all_to_2d_plot = get_idx_x_all(x_all, x_all) # agar dapat diplot pd 2D\n",
    "            get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all(x_all, x_all))\n",
    "            plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, label=\"ground truth from comb. model\", color=(0, 1, 0))\n",
    "            \n",
    "            \n",
    "            lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "            # plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "            \n",
    "            print(\"xtrain_plot: \",xtrain_plot)\n",
    "            \n",
    "            get_idx_x_all_to_2d_plot = get_idx_x_all(xtrain_plot, x_all) # agar dapat diplot pd 2D\n",
    "            get_mse_or_loss_val_to_2d_plot = get_mse_or_loss_val(get_idx_x_all_to_2d_plot)\n",
    "            \n",
    "            print(\"idx xtrain_plot: \",get_idx_x_all_to_2d_plot)\n",
    "        \n",
    "            \n",
    "            plt.plot(get_idx_x_all_to_2d_plot, get_mse_or_loss_val_to_2d_plot, \"x\", label=\"train\", color=\"k\")\n",
    "            \n",
    "            plt.ylim(-4, 4)\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.pause(0.01)\n",
    "            model.load_state_dict(weights_before)  # restore from snapshot\n",
    "            print(f\"-----------------------------\")\n",
    "            print(f\"iteration               {iteration + 1}\")\n",
    "            print(f\"loss on plotted curve   {lossval:.3f}\")  # would be better to average loss over a set of examples, but this is optimized for brevity\n",
    "\n",
    "\n",
    "# experiment('E-MAML', False)\n",
    "# experiment('E-MAML', True)\n",
    "# experiment('Synthetic_E-Reptile', False)\n",
    "experiment('Synthetic_E-Reptile', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubah gen_task dengan fungsi y_test_dengan sheet combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 5\n",
    "n_data_all = 5\n",
    "x_all = get_data_test(np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug MAML to EG-MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# n_input = 14\n",
    "# n_hidden1 = 100\n",
    "# n_hidden2 = 50\n",
    "# n_hidden3 = 25\n",
    "# n_output = 44\n",
    "\n",
    "# # ubah define Reptile dengan ELMRegression model\n",
    "# # elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "# # load_model_elm_reptile_from_json(f'model_reg/model_reptile_Comb-KMT-Tiny-Reg.json')  # Ganti dengan nama sheet yang sesuai\n",
    "# # hasil_pred = test_single_data_return_pred(elm_model, test_data)\n",
    "\n",
    "# # Example usage\n",
    "# # elm_model_reptile = ELMRegressionForReptile(n_input=14, n_hidden1=100, n_hidden2=50, n_hidden3=25, n_output=44)\n",
    "# elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(elm_model_reptile.parameters(), lr=0.01)\n",
    "\n",
    "# # Dummy data for demonstration\n",
    "# X_dummy = torch.randn(10, 14)\n",
    "# y_dummy = torch.randn(10, 44)\n",
    "\n",
    "# # Forward and backward pass\n",
    "# elm_model_reptile.train()\n",
    "# optimizer.zero_grad()\n",
    "# hasil_pred = elm_model_reptile(X_dummy)\n",
    "# loss = criterion(hasil_pred, y_dummy)\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in elm_model_reptile.parameters():\n",
    "#     print(param.data)\n",
    "#     print(param.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "x_try = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])\n",
    "print(x_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_torch(x_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(x):\n",
    "    return ag.Variable(torch.Tensor(x))\n",
    "\n",
    "# y_gt = y\n",
    "# using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "# def train_on_batch_eg_maml(x, y):\n",
    "#     x = to_torch(x)\n",
    "#     y = to_torch(y)\n",
    "    \n",
    "#     elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(elm_model_reptile.parameters(), lr=0.01)\n",
    "    \n",
    "#     # elm_model_reptile.zero_grad()\n",
    "#     # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "#     # loss = (ypred - y).pow(2).mean()\n",
    "#     # loss.backward()\n",
    "    \n",
    "#     # Forward and backward pass\n",
    "#     elm_model_reptile.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     ypred = elm_model_reptile(x)\n",
    "#     loss = criterion(ypred, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     for param in elm_model_reptile.parameters():\n",
    "#         param.data -= inner_step_size * param.grad.data\n",
    "\n",
    "# using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "def train_on_batch_eg_maml(x, y):\n",
    "    x = to_torch(x)\n",
    "    y = to_torch(y)\n",
    "\n",
    "    model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # elm_model_reptile.zero_grad()\n",
    "    # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "    # loss = (ypred - y).pow(2).mean()\n",
    "    # loss.backward()\n",
    "\n",
    "    # Forward and backward pass\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    ypred = model(x)\n",
    "    loss = criterion(ypred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.data -= inner_step_size * param.grad.data\n",
    "\n",
    "    # Save model checkpoint\n",
    "    filename_ckpt = f'model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "    save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "\n",
    "    # return loss.item()  # Optionally return the loss for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_step_size = 0.02  # stepsize in inner SGD\n",
    "train_on_batch_eg_maml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [0 2]\n",
      "\n",
      "=======================\n",
      "dpinds = [1 4 3]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [1 3]\n",
      "\n",
      "=======================\n",
      "dpinds = [4 0 2]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [3 1]\n",
      "\n",
      "=======================\n",
      "dpinds = [2 4 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [4 2]\n",
      "\n",
      "=======================\n",
      "dpinds = [3 1 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [2 3]\n",
      "\n",
      "=======================\n",
      "dpinds = [4 1 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [2 4]\n",
      "\n",
      "=======================\n",
      "dpinds = [1 3 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [3 1]\n",
      "\n",
      "=======================\n",
      "dpinds = [4 2 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [0 4]\n",
      "\n",
      "=======================\n",
      "dpinds = [2 1 3]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [4 2]\n",
      "\n",
      "=======================\n",
      "dpinds = [3 1 0]\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "mbinds = [1 4]\n",
      "\n",
      "=======================\n",
      "dpinds = [2 3 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/JosephKJ/PyTorch-MAML-and-Reptile\n",
    "# python main.py --run=MAML\n",
    "# python main.py --run=Reptile\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd as ag\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "# declare params\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "def experiment(run, plot=True):\n",
    "    seed = 0\n",
    "    inner_step_size = 0.02  # stepsize in inner SGD\n",
    "    inner_epochs = 1  # number of epochs of each inner SGD\n",
    "    outer_stepsize_reptile = 0.1  # stepsize of outer optimization, i.e., meta-optimization\n",
    "    outer_stepsize_maml = 0.01\n",
    "    # n_iterations = 30000  # number of outer updates; each iteration we sample one task and update on it\n",
    "    n_iterations = 10  # number of outer updates; each iteration we sample one task and update on it\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Define task distribution\n",
    "    n_data_all = 5\n",
    "    n_sample = n_data_all # minimum 1, maks = n_data_all\n",
    "    idx_x_all = np.arange(0,n_data_all)[:,None]\n",
    "    \n",
    "    # All of the x points data, dengan fitur input pepanjang n_input = 14\n",
    "    x_all = get_data_test(np.linspace(0, n_data_all-1, np.amin((n_sample, n_data_all), axis=0))[:, None])\n",
    "    # n_train = 10  # Size of training minibatches\n",
    "    n_train = 3  # Size of training minibatches, harus < n_data_all\n",
    "\n",
    "    # def gen_task(): # ubah dengan fungsi y_test_dengan sheet combine\n",
    "    #     \"Generate classification problem\"\n",
    "    #     # \"dfsfsds\"\n",
    "    #     phase = rng.uniform(low=0, high=2 * np.pi)\n",
    "    #     ampl = rng.uniform(0.1, 5)\n",
    "    #     f_randomsine = lambda x: np.sin(x + phase) * ampl\n",
    "    #     return f_randomsine\n",
    "    \n",
    "    def gen_task_eg_maml_base_idx_data():\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda idx_x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, get_data_test(idx_x_single[0]))\n",
    "             for idx_x_single in idx_x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task_eg_maml_base_val_data():\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "    \n",
    "    def gen_task(): # sama dengan gen_task_eg_maml_base_val_data()\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "\n",
    "        # f_randoms_eg_maml akan memproses semua data dalam idx_x_all\n",
    "        f_randoms_eg_maml = lambda x: np.array(\n",
    "            [test_single_data_return_pred(elm_model, x_single) for x_single in x]\n",
    "        )\n",
    "\n",
    "        return f_randoms_eg_maml\n",
    "\n",
    "    # Define model. Reptile paper uses ReLU, but Tanh gives slightly better results\n",
    "#     model = nn.Sequential(\n",
    "#         nn.Linear(1, 64),\n",
    "#         nn.Tanh(),\n",
    "#         nn.Linear(64, 64),\n",
    "#         nn.Tanh(),\n",
    "#         nn.Linear(64, 1),\n",
    "#     )\n",
    "    \n",
    "    # # ubah define Reptile dengan ELMRegression model\n",
    "    # elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "    # load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "    # hasil_pred = test_single_data_return_pred(elm_model, test_data)\n",
    "    \n",
    "    # Define model. Reptile dengan ELMRegressionForReptile\n",
    "    model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "#     # Forward and backward pass\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     ypred = model(x)\n",
    "#     loss = criterion(ypred, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "    # def save_model_reptile_checkpoint(model, filename):\n",
    "    #     model_state = model.state_dict()\n",
    "    #     with open(filename, 'w') as f:\n",
    "    #         json.dump(model_state, f)\n",
    "\n",
    "    # def load_model_reptile_checkpoint(model, filename):\n",
    "    #     with open(filename, 'r') as f:\n",
    "    #         model_state = json.load(f)\n",
    "    #     model.load_state_dict(model_state)\n",
    "        \n",
    "    def save_model_reptile_checkpoint(model, filename):\n",
    "        # Dapatkan state_dict dari model\n",
    "        model_state = model.state_dict()\n",
    "\n",
    "        # Konversi tensor menjadi list untuk serialisasi JSON\n",
    "        model_state_serializable = {k: v.numpy().tolist() for k, v in model_state.items()}\n",
    "\n",
    "        # Simpan model ke file JSON\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(model_state_serializable, f)\n",
    "\n",
    "    def load_model_reptile_checkpoint(model, filename):\n",
    "        # Muat model dari file JSON\n",
    "        with open(filename, 'r') as f:\n",
    "            model_state_serializable = json.load(f)\n",
    "\n",
    "        # Konversi kembali dari list ke tensor\n",
    "        model_state = {k: torch.tensor(np.array(v)) for k, v in model_state_serializable.items()}\n",
    "\n",
    "        # Memuat state_dict ke model\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()  # Set model ke mode evaluasi\n",
    "\n",
    "    def to_torch(x):\n",
    "        return ag.Variable(torch.Tensor(x))\n",
    "\n",
    "    # def train_on_batch(x, y):\n",
    "    #     x = to_torch(x)\n",
    "    #     y = to_torch(y)\n",
    "    #     model.zero_grad()\n",
    "    #     ypred = model(x)\n",
    "    #     loss = (ypred - y).pow(2).mean()\n",
    "    #     loss.backward()\n",
    "    #     for param in model.parameters():\n",
    "    #         param.data -= inner_step_size * param.grad.data\n",
    "    \n",
    "               \n",
    "    # using ELMRegressionForReptile support param.data dan param.grad.data\n",
    "    def train_on_batch_eg_maml(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "            \n",
    "    def train_on_batch(x, y):\n",
    "        x = to_torch(x)\n",
    "        y = to_torch(y)\n",
    "\n",
    "        model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        # elm_model_reptile.zero_grad()\n",
    "        # ypred = test_single_data_return_pred(elm_model_reptile,x)\n",
    "        # loss = (ypred - y).pow(2).mean()\n",
    "        # loss.backward()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ypred = model(x)\n",
    "        loss = criterion(ypred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for param in model.parameters():\n",
    "            param.data -= inner_step_size * param.grad.data\n",
    "            \n",
    "        # Save model checkpoint\n",
    "        filename_ckpt = f'model_reg_ckpt/model_reptile_checkpoint_{datetime.today().astimezone(pytz.timezone(\"Asia/Jakarta\")).strftime(\"%d-%m-%Y-%H-%M-%S\")}.json'\n",
    "        save_model_reptile_checkpoint(model, filename_ckpt)\n",
    "        \n",
    "        # return loss.item()  # Optionally return the loss for monitoring\n",
    "        \n",
    "    # Cara Memuat model dari checkpoint\n",
    "    #     try:\n",
    "    #         load_model_reptile_checkpoint(model, filename_ckpt)\n",
    "    #         print(\"Model berhasil dimuat dari:\", filename_ckpt)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Terjadi kesalahan saat memuat model:\", e)\n",
    "\n",
    "    #     # Sekarang Anda bisa menggunakan model untuk melakukan prediksi atau melanjutkan pelatihan\n",
    "    #     # Contoh prediksi\n",
    "    #     x_test = to_torch([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]])  # Ganti dengan data yang sesuai\n",
    "    #     model.eval()  # Set model ke mode evaluasi\n",
    "    #     with torch.no_grad():\n",
    "    #         prediction = model(x_test)\n",
    "    #         print(\"Hasil prediksi:\", prediction.numpy())\n",
    "\n",
    "    def predict(x):\n",
    "        x = to_torch(x)\n",
    "        return model(x).data.numpy()\n",
    "\n",
    "    # Choose a fixed task and minibatch for visualization\n",
    "    f_plot = gen_task()\n",
    "    # xtrain_plot = x_all[rng.choice(len(x_all), size=n_train)]\n",
    "    xtrain_plot = np.array([x_all[i] for i in rng.choice(len(x_all), size=n_train)])\n",
    "\n",
    "    # Training loop\n",
    "    for iteration in range(n_iterations):\n",
    "        weights_before = deepcopy(model.state_dict())\n",
    "\n",
    "        # Generate task\n",
    "        f = gen_task()\n",
    "        y_all = f(x_all)\n",
    "\n",
    "        # Do SGD on this task\n",
    "        inds = rng.permutation(len(x_all))\n",
    "        train_ind = inds[:-1 * n_train]\n",
    "        val_ind = inds[-1 * n_train:]       # Val contains 1/5th of the sine wave\n",
    "\n",
    "        for _ in range(inner_epochs):\n",
    "            for start in range(0, len(train_ind), n_train):\n",
    "                mbinds = train_ind[start:start + n_train]\n",
    "                print('mbinds =', mbinds)\n",
    "                print()\n",
    "                # print('x_all[mbinds] =', x_all[mbinds])\n",
    "                # print()\n",
    "                # print('y_all[mbinds] =', y_all[mbinds])\n",
    "                x_all_mbinds = np.array([x_all[i] for i in mbinds])\n",
    "                y_all_mbinds = np.array([y_all[i] for i in mbinds])\n",
    "                # train_on_batch(x_all[mbinds], y_all[mbinds])\n",
    "                train_on_batch(x_all_mbinds, y_all_mbinds)\n",
    "                \n",
    "                print('=======================')\n",
    "\n",
    "        if run == 'MAML':\n",
    "            outer_step_size = outer_stepsize_maml * (1 - iteration / n_iterations)  # linear schedule\n",
    "            for start in range(0, len(val_ind), n_train):\n",
    "                dpinds = val_ind[start:start + n_train]\n",
    "                print('dpinds =', dpinds)\n",
    "                print()\n",
    "                \n",
    "                # x = to_torch(x_all[dpinds])\n",
    "                x = to_torch(np.array([x_all[i] for i in dpinds]))\n",
    "                \n",
    "                # y = to_torch(y_all[dpinds])\n",
    "                y = to_torch(np.array([y_all[i] for i in dpinds]))\n",
    "\n",
    "                # Compute the grads\n",
    "                model.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = (y_pred - y).pow(2).mean()\n",
    "                loss.backward()\n",
    "                \n",
    "\n",
    "                # Reload the model\n",
    "                model.load_state_dict(weights_before)\n",
    "\n",
    "                # SGD on the params\n",
    "                for param in model.parameters():\n",
    "                    param.data -= outer_step_size * param.grad.data\n",
    "            # print(weights_before)\n",
    "        else:\n",
    "            # Interpolate between current weights and trained weights from this task\n",
    "            # I.e. (weights_before - weights_after) is the meta-gradient\n",
    "            weights_after = model.state_dict()\n",
    "            outerstepsize = outer_stepsize_reptile * (1 - iteration / n_iterations)  # linear schedule\n",
    "            model.load_state_dict({name: weights_before[name] + (weights_after[name] - weights_before[name]) * outerstepsize\n",
    "                                   for name in weights_before})\n",
    "\n",
    "        # Periodically plot the results on a particular task and minibatch\n",
    "        # if (plot and ((iteration == 0) or ((iteration + 1) % 1000 == 0))):\n",
    "#         if (plot and ((iteration == 0) or ((iteration + 1) % n_iterations == 0))):\n",
    "#             plt.cla()\n",
    "#             f = f_plot\n",
    "#             weights_before = deepcopy(model.state_dict())  # save snapshot before evaluation\n",
    "            \n",
    "#             plt.plot(x_all, predict(x_all), label=\"pred after 0\", color=(0, 0, 1))\n",
    "#             for inneriter in range(32):\n",
    "#                 train_on_batch(xtrain_plot, f(xtrain_plot))\n",
    "#                 if (inneriter + 1) % 8 == 0:\n",
    "#                     frac = (inneriter + 1) / 32\n",
    "#                     plt.plot(x_all, predict(x_all), label=\"pred after %i\" % (inneriter + 1), color=(frac, 0, 1 - frac))\n",
    "#             # plt.plot(x_all, f(x_all), label=\"true\", color=(0, 1, 0))\n",
    "#             plt.plot(x_all, f(x_all), label=\"ground truth from sin(x)\", color=(0, 1, 0))\n",
    "#             lossval = np.square(predict(x_all) - f(x_all)).mean()\n",
    "#             plt.plot(xtrain_plot, f(xtrain_plot), \"x\", label=\"train\", color=\"k\")\n",
    "#             plt.ylim(-4, 4)\n",
    "#             plt.legend(loc=\"lower right\")\n",
    "#             plt.pause(0.01)\n",
    "#             model.load_state_dict(weights_before)  # restore from snapshot\n",
    "#             print(f\"-----------------------------\")\n",
    "#             print(f\"iteration               {iteration + 1}\")\n",
    "#             print(f\"loss on plotted curve   {lossval:.3f}\")  # would be better to average loss over a set of examples, but this is optimized for brevity\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='MAML and Reptile Sine wave regression example.')\n",
    "    parser.add_argument('--run', dest='run', default='Reptile') # MAML, Reptile\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    experiment(args.run)\n",
    "\n",
    "# experiment('MAML', False)\n",
    "experiment('MAML', True)\n",
    "# experiment('Reptile', False)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sheet 'CombinedSheet' created/overwritten in dataset/regression_data2.xlsx\n",
      "Processing sheet: Sheet1\n",
      "Model saved as JSON at model_reg/model_Sheet1.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet1.json\n",
      "Loss plot saved as PNG and PDF for Sheet1\n",
      "Final Loss for Sheet1: 0.32231268286705017\n",
      "Processing sheet: Sheet2\n",
      "Model saved as JSON at model_reg/model_Sheet2.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet2.json\n",
      "Loss plot saved as PNG and PDF for Sheet2\n",
      "Final Loss for Sheet2: 0.30444157123565674\n",
      "Processing sheet: Sheet3\n",
      "Model saved as JSON at model_reg/model_Sheet3.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet3.json\n",
      "Loss plot saved as PNG and PDF for Sheet3\n",
      "Final Loss for Sheet3: 0.3324933648109436\n",
      "Processing sheet: Sheet4\n",
      "Model saved as JSON at model_reg/model_Sheet4.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet4.json\n",
      "Loss plot saved as PNG and PDF for Sheet4\n",
      "Final Loss for Sheet4: 0.34479546546936035\n",
      "Processing sheet: CombinedSheet\n",
      "Model saved as JSON at model_reg/model_CombinedSheet.json\n",
      "Loss per epoch saved as JSON at loss/loss_CombinedSheet.json\n",
      "Loss plot saved as PNG and PDF for CombinedSheet\n",
      "Final Loss for CombinedSheet: 0.806570291519165\n",
      "\n",
      "Processing sheet: Sheet1\n",
      "Model saved as JSON at model_reg/model_maml_Sheet1.json\n",
      "Processing sheet: Sheet2\n",
      "Model saved as JSON at model_reg/model_maml_Sheet2.json\n",
      "Processing sheet: Sheet3\n",
      "Model saved as JSON at model_reg/model_maml_Sheet3.json\n",
      "Processing sheet: Sheet4\n",
      "Model saved as JSON at model_reg/model_maml_Sheet4.json\n",
      "Processing sheet: CombinedSheet\n",
      "Model saved as JSON at model_reg/model_maml_CombinedSheet.json\n",
      "\n",
      "Testing model for sheet: Sheet1\n",
      "Model loaded from model_reg/model_Sheet1.json\n",
      "Testing Loss: 0.3212607800960541\n",
      "Testing model for sheet: Sheet2\n",
      "Model loaded from model_reg/model_Sheet2.json\n",
      "Testing Loss: 0.30357110500335693\n",
      "Testing model for sheet: Sheet3\n",
      "Model loaded from model_reg/model_Sheet3.json\n",
      "Testing Loss: 0.3316383361816406\n",
      "Testing model for sheet: Sheet4\n",
      "Model loaded from model_reg/model_Sheet4.json\n",
      "Testing Loss: 0.3456975221633911\n",
      "Testing model for sheet: CombinedSheet\n",
      "Model loaded from model_reg/model_CombinedSheet.json\n",
      "Testing Loss: 0.8066562414169312\n",
      "\n",
      "Testing model for sheet: Sheet1\n",
      "MAML Model loaded from model_reg/model_maml_Sheet1.json\n",
      "Testing Loss: 1.0037790536880493\n",
      "Testing model for sheet: Sheet2\n",
      "MAML Model loaded from model_reg/model_maml_Sheet2.json\n",
      "Testing Loss: 1.0136722326278687\n",
      "Testing model for sheet: Sheet3\n",
      "MAML Model loaded from model_reg/model_maml_Sheet3.json\n",
      "Testing Loss: 1.0127657651901245\n",
      "Testing model for sheet: Sheet4\n",
      "MAML Model loaded from model_reg/model_maml_Sheet4.json\n",
      "Testing Loss: 1.0108728408813477\n",
      "Testing model for sheet: CombinedSheet\n",
      "MAML Model loaded from model_reg/model_maml_CombinedSheet.json\n",
      "Testing Loss: 1.0186240673065186\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Definisi model ELM dengan 2 hidden layers untuk regresi\n",
    "class ELMRegression(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_output):\n",
    "        super(ELMRegression, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)  # Lapisan hidden pertama\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)  # Lapisan hidden kedua\n",
    "        self.output = nn.Linear(n_hidden2, n_output)  # Lapisan output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden1(x))  # Aktivasi sigmoid pada hidden layer 1\n",
    "        x = torch.sigmoid(self.hidden2(x))  # Aktivasi sigmoid pada hidden layer 2\n",
    "        return self.output(x)  # Output lapisan terakhir tanpa aktivasi untuk regresi\n",
    "\n",
    "# Fungsi untuk menyimpan model dalam format JSON\n",
    "def save_model_json(model, file_path):\n",
    "    model_params = {\n",
    "        \"hidden1_weights\": model.hidden1.weight.detach().numpy().tolist(),\n",
    "        \"hidden1_bias\": model.hidden1.bias.detach().numpy().tolist(),\n",
    "        \"hidden2_weights\": model.hidden2.weight.detach().numpy().tolist(),\n",
    "        \"hidden2_bias\": model.hidden2.bias.detach().numpy().tolist(),\n",
    "        \"output_weights\": model.output.weight.detach().numpy().tolist(),\n",
    "        \"output_bias\": model.output.bias.detach().numpy().tolist()\n",
    "    }\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(model_params, json_file)\n",
    "    print(f\"Model saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mencatat dan menyimpan loss dalam format JSON berdasarkan epoch\n",
    "def save_loss_json(loss_per_epoch, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(loss_per_epoch, json_file)\n",
    "    print(f\"Loss per epoch saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mem-plot dan menyimpan hasil training (loss per epoch)\n",
    "def plot_loss(loss_per_epoch, sheet_name):\n",
    "    epochs = list(range(1, len(loss_per_epoch) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss_per_epoch, label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss per Epoch for {sheet_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Menyimpan plot dalam format PNG dan PDF\n",
    "    plt.savefig(f'loss/loss_plot_{sheet_name}.png')\n",
    "    plt.savefig(f'loss/loss_plot_{sheet_name}.pdf')\n",
    "    print(f\"Loss plot saved as PNG and PDF for {sheet_name}\")\n",
    "\n",
    "    plt.close()  # Menutup plot setelah selesai\n",
    "\n",
    "# Fungsi untuk memuat data dan melakukan ELM regresi\n",
    "def perform_elm_regression(file_path, sheets, epochs=100):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :15].values  # 15 fitur input\n",
    "        y = df.iloc[:, 15:].values  # 10 target output\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        n_input = 15\n",
    "        n_hidden1 = 100  # Ukuran hidden layer pertama\n",
    "        n_hidden2 = 50   # Ukuran hidden layer kedua\n",
    "        n_output = 10\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        criterion = nn.MSELoss()  # Loss untuk regresi\n",
    "        optimizer = torch.optim.Adam(elm_model.parameters(), lr=0.01)\n",
    "\n",
    "        # List untuk menyimpan loss di setiap epoch\n",
    "        loss_per_epoch = []\n",
    "\n",
    "        # Training model dengan data yang ada\n",
    "        elm_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = elm_model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Simpan loss untuk setiap epoch\n",
    "            loss_per_epoch.append(loss.item())\n",
    "\n",
    "            # print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        # Menyimpan model dan loss\n",
    "        save_model_json(elm_model, f'model_reg/model_{sheet_name}.json')\n",
    "        save_loss_json(loss_per_epoch, f'loss/loss_{sheet_name}.json')\n",
    "\n",
    "        # Plot loss dan simpan dalam format PNG dan PDF\n",
    "        plot_loss(loss_per_epoch, sheet_name)\n",
    "\n",
    "        print(f\"Final Loss for {sheet_name}: {loss_per_epoch[-1]}\")\n",
    "\n",
    "# Fungsi untuk menggabungkan semua sheet menjadi satu\n",
    "def create_combined_sheet(file_path, sheets, combined_sheet_name=\"CombinedSheet\"):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name=combined_sheet_name, index=False)\n",
    "    \n",
    "    print(f\"Combined sheet '{combined_sheet_name}' created/overwritten in {file_path}\")\n",
    "\n",
    "# Fungsi untuk meload model dari file JSON\n",
    "def load_model_json(file_path, model):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        model_params = json.load(json_file)\n",
    "    \n",
    "    # Set parameter model dari file JSON\n",
    "    model.hidden1.weight.data = torch.FloatTensor(model_params['hidden1_weights'])\n",
    "    model.hidden1.bias.data = torch.FloatTensor(model_params['hidden1_bias'])\n",
    "    model.hidden2.weight.data = torch.FloatTensor(model_params['hidden2_weights'])\n",
    "    model.hidden2.bias.data = torch.FloatTensor(model_params['hidden2_bias'])\n",
    "    model.output.weight.data = torch.FloatTensor(model_params['output_weights'])\n",
    "    model.output.bias.data = torch.FloatTensor(model_params['output_bias'])\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "    \n",
    "# Fungsi untuk meload model MAML dari file JSON\n",
    "def load_model_maml_json(file_path, model):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        model_params = json.load(json_file)\n",
    "    \n",
    "    # Set parameter model dari file JSON\n",
    "    model.hidden1.weight.data = torch.FloatTensor(model_params['hidden1_weights'])\n",
    "    model.hidden1.bias.data = torch.FloatTensor(model_params['hidden1_bias'])\n",
    "    model.hidden2.weight.data = torch.FloatTensor(model_params['hidden2_weights'])\n",
    "    model.hidden2.bias.data = torch.FloatTensor(model_params['hidden2_bias'])\n",
    "    model.output.weight.data = torch.FloatTensor(model_params['output_weights'])\n",
    "    model.output.bias.data = torch.FloatTensor(model_params['output_bias'])\n",
    "    print(f\"MAML Model loaded from {file_path}\")\n",
    "\n",
    "# Fungsi untuk menguji model\n",
    "def test_model(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_tensor)\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        print(f'Testing Loss: {mse_loss.item()}')\n",
    "        \n",
    "\n",
    "# Fungsi untuk meload model dari semua sheet dan lakukan testing\n",
    "def load_and_test_all_models(file_path, sheets):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Testing model for sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :15].values  # 15 fitur input\n",
    "        y = df.iloc[:, 15:].values  # 10 target output\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "\n",
    "        n_input = 15\n",
    "        n_hidden1 = 100\n",
    "        n_hidden2 = 50\n",
    "        n_output = 10\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        # Memuat model dari file JSON\n",
    "        load_model_json(f'model_reg/model_{sheet_name}.json', elm_model)\n",
    "\n",
    "        # Melakukan testing dan menghitung loss\n",
    "        test_model(elm_model, X_tensor, y)\n",
    "        \n",
    "\n",
    "# Fungsi untuk MAML\n",
    "def maml_training(file_path, sheets, inner_epochs=5, outer_epochs=10, inner_lr=0.01, outer_lr=0.001):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    n_input = 15\n",
    "    n_hidden1 = 100\n",
    "    n_hidden2 = 50\n",
    "    n_output = 10\n",
    "\n",
    "    models = {}\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :15].values  # 15 fitur input\n",
    "        y = df.iloc[:, 15:].values  # 10 target output\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        # Inisialisasi model\n",
    "        model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=inner_lr)\n",
    "\n",
    "        for outer_epoch in range(outer_epochs):\n",
    "            # Salin model untuk setiap tugas (sheet)\n",
    "            model_copy = deepcopy(model)\n",
    "\n",
    "            # Inner loop: pelatihan pada satu tugas\n",
    "            for inner_epoch in range(inner_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model_copy(X_tensor)\n",
    "                loss = nn.MSELoss()(outputs, y_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Simpan model untuk tugas saat ini\n",
    "            models[sheet_name] = deepcopy(model_copy)\n",
    "\n",
    "            # Menghitung gradien untuk update outer model\n",
    "            for param in model.parameters():\n",
    "                param.grad = None  # Reset gradien\n",
    "            outputs = model(X_tensor)\n",
    "            outer_loss = nn.MSELoss()(outputs, y_tensor)\n",
    "            outer_loss.backward()\n",
    "\n",
    "            # Update parameter model utama\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Simpan model akhir\n",
    "        save_model_json(model, f'model_reg/model_maml_{sheet_name}.json')\n",
    "        \n",
    "# Fungsi untuk meload model dari semua sheet dan lakukan testing\n",
    "def load_and_test_all_maml_models(file_path, sheets):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Testing model for sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :15].values  # 15 fitur input\n",
    "        y = df.iloc[:, 15:].values  # 10 target output\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "\n",
    "        n_input = 15\n",
    "        n_hidden1 = 100\n",
    "        n_hidden2 = 50\n",
    "        n_output = 10\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        # Memuat model MAML dari file JSON\n",
    "        load_model_maml_json(f'model_reg/model_maml_{sheet_name}.json', elm_model)\n",
    "\n",
    "        # Melakukan testing dan menghitung loss\n",
    "        test_model(elm_model, X_tensor, y)\n",
    "\n",
    "# Tentukan path ke file Excel dan nama sheet\n",
    "file_path = 'dataset/regression_data2.xlsx'\n",
    "sheets = ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4']\n",
    "\n",
    "# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\n",
    "create_combined_sheet(file_path, sheets)\n",
    "\n",
    "# Lakukan regresi pada dataset di file Excel\n",
    "perform_elm_regression(file_path, sheets + ['CombinedSheet'], epochs=400)\n",
    "\n",
    "print()\n",
    "\n",
    "# Lakukan MAML pada dataset di file Excel\n",
    "maml_training(file_path, sheets + ['CombinedSheet'], inner_epochs=5, outer_epochs=10, inner_lr=0.01, outer_lr=0.001)\n",
    "\n",
    "print()\n",
    "\n",
    "# Lakukan testing setelah meload model dari file JSON untuk semua sheet\n",
    "load_and_test_all_models(file_path, sheets + ['CombinedSheet'])\n",
    "\n",
    "print()\n",
    "\n",
    "# Lakukan testing setelah meload maml model dari file JSON untuk semua sheet\n",
    "load_and_test_all_maml_models(file_path, sheets + ['CombinedSheet'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data sample --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data gen reg. multi target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets generated and saved to dataset/regression_data2.xlsx with sheets: Sheet1, Sheet2, Sheet3, Sheet4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fungsi untuk mengenerate dataset dengan 15 fitur input dan 10 target output untuk beberapa sheet\n",
    "def generate_datasets_for_sheets(n_samples=1000, n_features=15, n_targets=10, sheets=['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4'], file_path='dataset/regression_data2.xlsx'):\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "        for sheet in sheets:\n",
    "            # Menghasilkan 15 fitur input acak dengan distribusi normal\n",
    "            X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "            # Menghasilkan 10 target output acak dengan distribusi normal\n",
    "            y = np.random.randn(n_samples, n_targets)\n",
    "    \n",
    "            # Menggabungkan fitur input dan target output menjadi satu DataFrame\n",
    "            columns = [f'Feature_{i+1}' for i in range(n_features)] + [f'Target_{i+1}' for i in range(n_targets)]\n",
    "            data = np.hstack((X, y))  # Menggabungkan input dan output\n",
    "    \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "            # Menyimpan dataset ke sheet yang berbeda dalam file Excel\n",
    "            df.to_excel(writer, index=False, sheet_name=sheet)\n",
    "    \n",
    "    print(f\"Datasets generated and saved to {file_path} with sheets: {', '.join(sheets)}\")\n",
    "\n",
    "# Mengenerate dataset untuk beberapa sheet dengan 1000 sampel per sheet\n",
    "generate_datasets_for_sheets(n_samples=1000, n_features=15, n_targets=10, sheets=['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4'], file_path='dataset/regression_data2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data gen. for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fungsi untuk menghasilkan data acak dengan 15 fitur dan 5 kelas target\n",
    "def generate_random_data(num_samples=100):\n",
    "    # 15 fitur acak\n",
    "    X = np.random.rand(num_samples, 15)\n",
    "    # Target kelas acak dengan 5 kelas (0-4)\n",
    "    y = np.random.randint(0, 5, num_samples)\n",
    "    # Gabungkan fitur dan target ke DataFrame\n",
    "    df = pd.DataFrame(X, columns=[f'Feature{i+1}' for i in range(15)])\n",
    "    df['Target'] = y\n",
    "    return df\n",
    "\n",
    "# Buat file Excel dengan 4 sheet yang berisi data acak\n",
    "with pd.ExcelWriter('classification_data2.xlsx') as writer:\n",
    "    for i in range(1, 5):\n",
    "        df = generate_random_data(100)  # Misal 100 sampel per sheet\n",
    "        df.to_excel(writer, sheet_name=f'Sheet{i}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# # Define the ELM model for classification\n",
    "# class ELM(nn.Module):\n",
    "#     def __init__(self, n_input, n_hidden, n_classes):\n",
    "#         super(ELM, self).__init__()\n",
    "#         self.hidden = nn.Linear(n_input, n_hidden)\n",
    "#         self.output = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Activation function\n",
    "#         x = torch.sigmoid(self.hidden(x))\n",
    "#         return self.output(x)\n",
    "\n",
    "# # Function to load data and perform ELM classification\n",
    "# def perform_elm_classification(file_path, sheets):\n",
    "#     # Load the Excel file\n",
    "#     excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "#     # Iterate through each specified sheet\n",
    "#     for sheet_name in sheets:\n",
    "#         print(f\"Processing sheet: {sheet_name}\")\n",
    "        \n",
    "#         # Load dataset from the current sheet\n",
    "#         df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "#         # Assuming the last column is the target variable (class labels) and others are features\n",
    "#         X = df.iloc[:, :-1].values  # Features\n",
    "#         y = df.iloc[:, -1].values    # Class labels\n",
    "\n",
    "#         # Convert to tensor\n",
    "#         X_tensor = torch.FloatTensor(X)\n",
    "#         y_tensor = torch.LongTensor(y)  # Use LongTensor for class labels\n",
    "\n",
    "#         # Initialize ELM model\n",
    "#         n_input = X.shape[1]\n",
    "#         n_classes = len(np.unique(y))  # Number of classes\n",
    "#         n_hidden = 100  # Number of neurons in hidden layer\n",
    "#         elm_model = ELM(n_input, n_hidden, n_classes)\n",
    "\n",
    "#         # Loss function and optimizer\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = torch.optim.Adam(elm_model.parameters(), lr=0.01)\n",
    "\n",
    "#         # Train the model\n",
    "#         elm_model.train()\n",
    "#         for epoch in range(100):  # Change the number of epochs as needed\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = elm_model(X_tensor)\n",
    "#             loss = criterion(outputs, y_tensor)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # Make predictions\n",
    "#         elm_model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             logits = elm_model(X_tensor)\n",
    "#             predictions = torch.argmax(logits, dim=1)  # Get predicted class labels\n",
    "\n",
    "#         # Calculate and print performance metrics\n",
    "#         accuracy = (predictions.numpy() == y_tensor.numpy()).mean()\n",
    "#         precision = precision_score(y_tensor.numpy(), predictions.numpy(), average='weighted')\n",
    "#         recall = recall_score(y_tensor.numpy(), predictions.numpy(), average='weighted')\n",
    "#         f1 = f1_score(y_tensor.numpy(), predictions.numpy(), average='weighted')\n",
    "\n",
    "#         print(f\"Metrics for {sheet_name}:\")\n",
    "#         print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "#         print(f\"  Precision: {precision:.4f}\")\n",
    "#         print(f\"  Recall: {recall:.4f}\")\n",
    "#         print(f\"  F1 Score: {f1:.4f}\")\n",
    "\n",
    "# # Specify the path to your Excel file and sheet names\n",
    "# file_path = 'dataset/data_regression.xlsx'  # Change to your actual Excel file name\n",
    "# sheets = ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4']  # Change to your sheet names\n",
    "\n",
    "# # Perform classification on the datasets in the Excel file\n",
    "# perform_elm_classification(file_path, sheets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Deep ELM dgn one hot encoder n save + load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_hidden = k (dinamis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sheet 'CombinedSheet' created/overwritten in dataset/dataset_v4.xlsx\n",
      "Processing sheet: Sheet1-KM-SAR-Tiny-Reg\n",
      "Model saved as JSON at model_reg/model_Sheet1-KM-SAR-Tiny-Reg_0.000244_6_hidden_layers_100_50_25_12_6_3_31-10-2024-22-03-12.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet1-KM-SAR-Tiny-Reg_0.000244_6_hidden_layers_100_50_25_12_6_3_31-10-2024-22-03-12.json\n",
      "Loss plot saved as PNG and PDF for Sheet1-KM-SAR-Tiny-Reg\n",
      "Final Loss for Sheet1-KM-SAR-Tiny-Reg: 0.00024402352573815733\n",
      "Processing sheet: Sheet2-M-SAK-T-Tiny-Reg\n",
      "Model saved as JSON at model_reg/model_Sheet2-M-SAK-T-Tiny-Reg_0.000000_6_hidden_layers_100_50_25_12_6_3_31-10-2024-22-03-12.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet2-M-SAK-T-Tiny-Reg_0.000000_6_hidden_layers_100_50_25_12_6_3_31-10-2024-22-03-12.json\n",
      "Loss plot saved as PNG and PDF for Sheet2-M-SAK-T-Tiny-Reg\n",
      "Final Loss for Sheet2-M-SAK-T-Tiny-Reg: 2.8543223082951215e-15\n",
      "Processing sheet: Comb-KMT-Tiny-Reg\n",
      "Model saved as JSON at model_reg/model_Comb-KMT-Tiny-Reg_0.119698_6_hidden_layers_100_50_25_12_6_3_31-10-2024-22-03-12.json\n",
      "Loss per epoch saved as JSON at loss/loss_Comb-KMT-Tiny-Reg_0.119698_6_hidden_layers_100_50_25_12_6_3_31-10-2024-22-03-12.json\n",
      "Loss plot saved as PNG and PDF for Comb-KMT-Tiny-Reg\n",
      "Final Loss for Comb-KMT-Tiny-Reg: 0.11969752609729767\n",
      "Loss plot all saved as PNG and PDF with identifier 31-10-2024-22-03-12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# set var global\n",
    "n_input = 14\n",
    "# n_hidden1 = 100\n",
    "# n_hidden2 = 50\n",
    "# hidden_layers = \n",
    "hidden_layers = [100, 50, 25, 12, 6, 3] \n",
    "n_output = 44\n",
    "\n",
    "\n",
    "# Define ELM Model with dynamic hidden layers for regression\n",
    "class ELMRegression(nn.Module):\n",
    "    def __init__(self, n_input, hidden_layers, n_output):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_layer_size = n_input\n",
    "\n",
    "        # Dynamically create hidden layers\n",
    "        for hidden_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(prev_layer_size, hidden_size))\n",
    "            prev_layer_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(prev_layer_size, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = torch.sigmoid(layer(x))  # Sigmoid activation for each hidden layer\n",
    "        return self.output(x)  # Linear output layer for regression\n",
    "\n",
    "# Function to save model parameters in JSON format\n",
    "def save_model_json(model, file_path):\n",
    "    model_params = {\n",
    "        f\"layer_{i}\": layer.weight.detach().numpy().tolist()\n",
    "        for i, layer in enumerate(model.layers)\n",
    "    }\n",
    "    model_params.update({\n",
    "        f\"layer_{i}_bias\": layer.bias.detach().numpy().tolist()\n",
    "        for i, layer in enumerate(model.layers)\n",
    "    })\n",
    "    model_params[\"output_weights\"] = model.output.weight.detach().numpy().tolist()\n",
    "    model_params[\"output_bias\"] = model.output.bias.detach().numpy().tolist()\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(model_params, json_file)\n",
    "    print(f\"Model saved as JSON at {file_path}\")\n",
    "    \n",
    "# Fungsi untuk mencatat dan menyimpan loss dalam format JSON berdasarkan epoch\n",
    "def save_loss_json(loss_per_epoch, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(loss_per_epoch, json_file)\n",
    "    print(f\"Loss per epoch saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mem-plot dan menyimpan hasil training (loss per epoch)\n",
    "def plot_loss(loss_per_epoch, sheet_name, name_unik):\n",
    "    epochs = list(range(1, len(loss_per_epoch) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss_per_epoch, label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss per Epoch for {sheet_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Menyimpan plot dalam format PNG dan PDF\n",
    "    plt.savefig(f'loss/loss_plot_{sheet_name}_{name_unik}.png')\n",
    "    plt.savefig(f'loss/loss_plot_{sheet_name}_{name_unik}.pdf')\n",
    "    print(f\"Loss plot saved as PNG and PDF for {sheet_name}\")\n",
    "\n",
    "    plt.close()  # Menutup plot setelah selesai\n",
    "    \n",
    "# Function to plot loss for all sheets in a single file\n",
    "# def plot_loss_all(loss_per_epoch_dict, sheets, gen_name_unik):\n",
    "#     plt.figure(figsize=(10, 6))  # Set a larger figure size for better readability\n",
    "    \n",
    "#     # Plot the loss for each sheet\n",
    "#     for sheet_name in sheets:\n",
    "#         plt.plot(loss_per_epoch_dict[sheet_name], label=f'Loss for {sheet_name}')\n",
    "    \n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Training Loss per Epoch for All Sheets')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # Save the combined plot in both PNG and PDF formats\n",
    "#     file_name = f\"loss/loss_plot_all_{gen_name_unik}\"\n",
    "#     plt.savefig(f\"{file_name}.png\")\n",
    "#     plt.savefig(f\"{file_name}.pdf\")\n",
    "#     print(f\"Loss plot all saved as PNG and PDF with unique name: {gen_name_unik}\")\n",
    "\n",
    "#     plt.close()  # Close the plot after saving\n",
    "    \n",
    "# Define plot_loss_all to plot loss for all sheets in one file\n",
    "def plot_loss_all(loss_per_epoch_dict, sheets, gen_name_unik):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    #     for sheet_name, loss_per_epoch in zip(sheets, loss_per_epoch_dict):\n",
    "    #         epochs = list(range(1, len(loss_per_epoch) + 1))\n",
    "    #         # Adding legend with sheet name and final loss value formatted to 6 decimal places\n",
    "    #         # plt.plot(epochs, loss_per_epoch, label=f\"{sheet_name}+{loss_per_epoch[-1]:.6f}\")\n",
    "    #         # plt.plot(epochs, loss_per_epoch, label=f\"{sheet_name} - {loss_per_epoch[-1]:.6f}\")\n",
    "\n",
    "    #         print(loss_per_epoch[-1])\n",
    "\n",
    "    #         final_loss = float(loss_per_epoch[-1])  # Ensure this is a float\n",
    "    #         # Adding legend with sheet name and final loss value formatted to 6 decimal places\n",
    "    #         plt.plot(epochs, loss_per_epoch, label=f\"{sheet_name} - {final_loss:.6f}\")\n",
    "        \n",
    "    for sheet_name, loss_per_epoch in loss_per_epoch_dict.items():\n",
    "        epochs = list(range(1, len(loss_per_epoch) + 1))\n",
    "        final_loss = float(loss_per_epoch[-1])  # Ensure this is a float\n",
    "        # Adding legend with sheet name and final loss value formatted to 6 decimal places\n",
    "        plt.plot(epochs, loss_per_epoch, label=f\"{sheet_name} - {final_loss:.16f}\")\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch for All Sheets')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the combined plot in both PNG and PDF formats\n",
    "    plt.savefig(f'loss/loss_plot_all_{gen_name_unik}.png')\n",
    "    plt.savefig(f'loss/loss_plot_all_{gen_name_unik}.pdf')\n",
    "    print(f\"Loss plot all saved as PNG and PDF with identifier {gen_name_unik}\")\n",
    "\n",
    "    plt.close()  # Close the plot after saving\n",
    "\n",
    "\n",
    "# Function to perform ELM regression for each sheet with dynamic hidden layers\n",
    "def perform_elm_regression(file_path, sheets, hidden_layers, epochs=100):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    \n",
    "    gen_name_unik = datetime.today().astimezone(pytz.timezone('Asia/Jakarta')).strftime('%d-%m-%Y-%H-%M-%S')\n",
    "\n",
    "    # untuk plot loss all\n",
    "    # Collect loss data for all sheets\n",
    "    loss_per_epoch_dict = {}\n",
    "    \n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :n_input].values\n",
    "        y = df.iloc[:, n_input:].values\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "        elm_model = ELMRegression(n_input, hidden_layers, n_output)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(elm_model.parameters(), lr=0.01)\n",
    "        loss_per_epoch = []\n",
    "\n",
    "        elm_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = elm_model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_per_epoch.append(loss.item())\n",
    "\n",
    "        \n",
    "        save_model_json(elm_model, f\"model_reg/model_{sheet_name}_{loss_per_epoch[-1]:.6f}_{len(hidden_layers)}_hidden_layers_{'_'.join(map(str, hidden_layers))}_{gen_name_unik}.json\")\n",
    "        save_loss_json(loss_per_epoch, f\"loss/loss_{sheet_name}_{loss_per_epoch[-1]:.6f}_{len(hidden_layers)}_hidden_layers_{'_'.join(map(str, hidden_layers))}_{gen_name_unik}.json\")\n",
    "        \n",
    "        plot_loss(loss_per_epoch, sheet_name, f\"{loss_per_epoch[-1]:.6f}_{gen_name_unik}\")\n",
    "        print(f\"Final Loss for {sheet_name}: {loss_per_epoch[-1]}\")\n",
    "        \n",
    "        # existing code to train the model and get `loss_per_epoch`\n",
    "        loss_per_epoch_dict[sheet_name] = loss_per_epoch  # Add to dictionary\n",
    "        \n",
    "    # untuk plot loss all\n",
    "    # Collect loss data for all sheets\n",
    "    #     loss_per_epoch_dict = {}\n",
    "\n",
    "    #     for sheet_name in sheets:\n",
    "    #         # existing code to train the model and get `loss_per_epoch`\n",
    "    #         loss_per_epoch_dict[sheet_name] = loss_per_epoch  # Add to dictionary\n",
    "        \n",
    "    # After processing all sheets, call plot_loss_all\n",
    "    plot_loss_all(loss_per_epoch_dict, sheets, gen_name_unik)\n",
    "        \n",
    "    \n",
    "        \n",
    "# Fungsi untuk meload model dari file JSON dengan support hidden layers yang dinamis\n",
    "# def load_model_json(file_path, model, hidden_layers):\n",
    "#     with open(file_path, 'r') as json_file:\n",
    "#         model_params = json.load(json_file)\n",
    "\n",
    "#     # Set parameter model dari file JSON untuk setiap hidden layer\n",
    "#     for i, layer_size in enumerate(hidden_layers):\n",
    "#         weight_key = f\"hidden{i+1}_weights\"\n",
    "#         bias_key = f\"hidden{i+1}_bias\"\n",
    "#         getattr(model, f\"hidden{i+1}\").weight.data = torch.FloatTensor(model_params[weight_key])\n",
    "#         getattr(model, f\"hidden{i+1}\").bias.data = torch.FloatTensor(model_params[bias_key])\n",
    "    \n",
    "#     # Set parameter untuk output layer\n",
    "#     model.output.weight.data = torch.FloatTensor(model_params['output_weights'])\n",
    "#     model.output.bias.data = torch.FloatTensor(model_params['output_bias'])\n",
    "    \n",
    "#     print(f\"Model loaded from {file_path}\")\n",
    "    \n",
    "# Fungsi untuk meload model dari file JSON secara dinamis tanpa perlu hidden_layers sebagai parameter\n",
    "def load_model_json(file_path, model):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        model_params = json.load(json_file)\n",
    "    \n",
    "    # Set parameter untuk setiap hidden layer\n",
    "    i = 1\n",
    "    while hasattr(model, f\"hidden{i}\"):\n",
    "        weight_key = f\"hidden{i}_weights\"\n",
    "        bias_key = f\"hidden{i}_bias\"\n",
    "        getattr(model, f\"hidden{i}\").weight.data = torch.FloatTensor(model_params[weight_key])\n",
    "        getattr(model, f\"hidden{i}\").bias.data = torch.FloatTensor(model_params[bias_key])\n",
    "        i += 1\n",
    "    \n",
    "    # Set parameter untuk output layer\n",
    "    model.output.weight.data = torch.FloatTensor(model_params['output_weights'])\n",
    "    model.output.bias.data = torch.FloatTensor(model_params['output_bias'])\n",
    "    \n",
    "    # print(f\"Model loaded from {file_path}\")\n",
    "        \n",
    "# Fungsi untuk menggabungkan semua sheet menjadi satu\n",
    "def create_combined_sheet(file_path, sheets, combined_sheet_name=\"CombinedSheet\"):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name=combined_sheet_name, index=False)\n",
    "    \n",
    "    print(f\"Combined sheet '{combined_sheet_name}' created/overwritten in {file_path}\")\n",
    "    \n",
    "# Fungsi untuk menguji model\n",
    "def test_model(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_tensor)\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        print(f'Testing Loss: {mse_loss.item()}')\n",
    "\n",
    "# Usage example\n",
    "file_path = 'dataset/dataset_v4.xlsx'\n",
    "sheets = ['Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg']\n",
    "\n",
    "# name_CombinedSheet = 'x'.join(sheets)\n",
    "name_CombinedSheet = 'KMT-Tiny-Reg'\n",
    "\n",
    "# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\n",
    "create_combined_sheet(file_path, sheets)\n",
    "\n",
    "# Lakukan regresi pada dataset di file Excel\n",
    "# perform_elm_regression(file_path, sheets + ['CombinedSheet'], epochs=400)\n",
    "# perform_elm_regression(file_path, sheets + ['Comb-'+name_CombinedSheet], epochs=400)\n",
    "\n",
    "perform_elm_regression(file_path, sheets + ['Comb-'+name_CombinedSheet], hidden_layers, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_hidden = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sheet 'CombinedSheet' created/overwritten in dataset/dataset_v4.xlsx\n",
      "Processing sheet: Sheet1-KM-SAR-Tiny-Reg\n",
      "Model saved as JSON at model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet1-KM-SAR-Tiny-Reg.json\n",
      "Loss plot saved as PNG and PDF for Sheet1-KM-SAR-Tiny-Reg\n",
      "Final Loss for Sheet1-KM-SAR-Tiny-Reg: 7.897994074643601e-16\n",
      "Processing sheet: Sheet2-M-SAK-T-Tiny-Reg\n",
      "Model saved as JSON at model_reg/model_Sheet2-M-SAK-T-Tiny-Reg.json\n",
      "Loss per epoch saved as JSON at loss/loss_Sheet2-M-SAK-T-Tiny-Reg.json\n",
      "Loss plot saved as PNG and PDF for Sheet2-M-SAK-T-Tiny-Reg\n",
      "Final Loss for Sheet2-M-SAK-T-Tiny-Reg: 2.4784266092977593e-16\n",
      "Processing sheet: Comb-KMT-Tiny-Reg\n",
      "Model saved as JSON at model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "Loss per epoch saved as JSON at loss/loss_Comb-KMT-Tiny-Reg.json\n",
      "Loss plot saved as PNG and PDF for Comb-KMT-Tiny-Reg\n",
      "Final Loss for Comb-KMT-Tiny-Reg: 6.022079206084567e-16\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set var global\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_output = 44\n",
    "\n",
    "# n_input = 15\n",
    "# n_hidden1 = 100\n",
    "# n_hidden2 = 50\n",
    "# n_output = 10\n",
    "\n",
    "# Definisi model ELM dengan 2 hidden layers untuk regresi\n",
    "class ELMRegression(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_output):\n",
    "        super(ELMRegression, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)  # Lapisan hidden pertama\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)  # Lapisan hidden kedua\n",
    "        self.output = nn.Linear(n_hidden2, n_output)  # Lapisan output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden1(x))  # Aktivasi sigmoid pada hidden layer 1\n",
    "        x = torch.sigmoid(self.hidden2(x))  # Aktivasi sigmoid pada hidden layer 2\n",
    "        return self.output(x)  # Output lapisan terakhir tanpa aktivasi untuk regresi\n",
    "\n",
    "# Fungsi untuk menyimpan model dalam format JSON\n",
    "def save_model_json(model, file_path):\n",
    "    model_params = {\n",
    "        \"hidden1_weights\": model.hidden1.weight.detach().numpy().tolist(),\n",
    "        \"hidden1_bias\": model.hidden1.bias.detach().numpy().tolist(),\n",
    "        \"hidden2_weights\": model.hidden2.weight.detach().numpy().tolist(),\n",
    "        \"hidden2_bias\": model.hidden2.bias.detach().numpy().tolist(),\n",
    "        \"output_weights\": model.output.weight.detach().numpy().tolist(),\n",
    "        \"output_bias\": model.output.bias.detach().numpy().tolist()\n",
    "    }\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(model_params, json_file)\n",
    "    print(f\"Model saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mencatat dan menyimpan loss dalam format JSON berdasarkan epoch\n",
    "def save_loss_json(loss_per_epoch, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(loss_per_epoch, json_file)\n",
    "    print(f\"Loss per epoch saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mem-plot dan menyimpan hasil training (loss per epoch)\n",
    "def plot_loss(loss_per_epoch, sheet_name):\n",
    "    epochs = list(range(1, len(loss_per_epoch) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss_per_epoch, label='Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss per Epoch for {sheet_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Menyimpan plot dalam format PNG dan PDF\n",
    "    plt.savefig(f'loss/loss_plot_{sheet_name}.png')\n",
    "    plt.savefig(f'loss/loss_plot_{sheet_name}.pdf')\n",
    "    print(f\"Loss plot saved as PNG and PDF for {sheet_name}\")\n",
    "\n",
    "    plt.close()  # Menutup plot setelah selesai\n",
    "\n",
    "# Fungsi untuk memuat data dan melakukan ELM regresi\n",
    "def perform_elm_regression(file_path, sheets, epochs=100):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        # X = df.iloc[:, :15].values  # 15 fitur input\n",
    "        # y = df.iloc[:, 15:].values  # 10 target output\n",
    "        X = df.iloc[:, :n_input].values  # 15 fitur input\n",
    "        y = df.iloc[:, n_input:].values  # 10 target output\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        # n_input = 15\n",
    "        # n_hidden1 = 100  # Ukuran hidden layer pertama\n",
    "        # n_hidden2 = 50   # Ukuran hidden layer kedua\n",
    "        # n_output = 10\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        criterion = nn.MSELoss()  # Loss untuk regresi\n",
    "        optimizer = torch.optim.Adam(elm_model.parameters(), lr=0.01)\n",
    "\n",
    "        # List untuk menyimpan loss di setiap epoch\n",
    "        loss_per_epoch = []\n",
    "\n",
    "        # Training model dengan data yang ada\n",
    "        elm_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = elm_model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Simpan loss untuk setiap epoch\n",
    "            loss_per_epoch.append(loss.item())\n",
    "\n",
    "            # print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        # Menyimpan model dan loss\n",
    "        save_model_json(elm_model, f'model_reg/model_{sheet_name}.json')\n",
    "        save_loss_json(loss_per_epoch, f'loss/loss_{sheet_name}.json')\n",
    "\n",
    "        # Plot loss dan simpan dalam format PNG dan PDF\n",
    "        plot_loss(loss_per_epoch, sheet_name)\n",
    "\n",
    "        print(f\"Final Loss for {sheet_name}: {loss_per_epoch[-1]}\")\n",
    "\n",
    "# Fungsi untuk menggabungkan semua sheet menjadi satu\n",
    "def create_combined_sheet(file_path, sheets, combined_sheet_name=\"CombinedSheet\"):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name=combined_sheet_name, index=False)\n",
    "    \n",
    "    print(f\"Combined sheet '{combined_sheet_name}' created/overwritten in {file_path}\")\n",
    "\n",
    "# Fungsi untuk meload model dari file JSON\n",
    "def load_model_json(file_path, model):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        model_params = json.load(json_file)\n",
    "    \n",
    "    # Set parameter model dari file JSON\n",
    "    model.hidden1.weight.data = torch.FloatTensor(model_params['hidden1_weights'])\n",
    "    model.hidden1.bias.data = torch.FloatTensor(model_params['hidden1_bias'])\n",
    "    model.hidden2.weight.data = torch.FloatTensor(model_params['hidden2_weights'])\n",
    "    model.hidden2.bias.data = torch.FloatTensor(model_params['hidden2_bias'])\n",
    "    model.output.weight.data = torch.FloatTensor(model_params['output_weights'])\n",
    "    model.output.bias.data = torch.FloatTensor(model_params['output_bias'])\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "\n",
    "# Fungsi untuk menguji model\n",
    "def test_model(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_tensor)\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        print(f'Testing Loss: {mse_loss.item()}')\n",
    "\n",
    "# Fungsi untuk meload model dari semua sheet dan lakukan testing\n",
    "def load_and_test_all_models(file_path, sheets):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Testing model for sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        # X = df.iloc[:, :15].values  # 15 fitur input\n",
    "        # y = df.iloc[:, 15:].values  # 10 target output\n",
    "        X = df.iloc[:, :n_input].values  # \n",
    "        y = df.iloc[:, n_input:].values  # \n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "\n",
    "        # n_input = 15\n",
    "        # n_hidden1 = 100\n",
    "        # n_hidden2 = 50\n",
    "        # n_output = 10\n",
    "        elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        # Memuat model dari file JSON\n",
    "        load_model_json(f'model_reg/model_{sheet_name}.json', elm_model)\n",
    "\n",
    "        # Melakukan testing dan menghitung loss\n",
    "        test_model(elm_model, X_tensor, y)\n",
    "\n",
    "# Tentukan path ke file Excel dan nama sheet\n",
    "# file_path = 'dataset/regression_data2.xlsx'\n",
    "file_path = 'dataset/dataset_v4.xlsx'\n",
    "\n",
    "# sheets = ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4']\n",
    "sheets = ['Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg']\n",
    "\n",
    "#generate kode unik\n",
    "\n",
    "# name_CombinedSheet = 'x'.join(sheets)\n",
    "name_CombinedSheet = 'KMT-Tiny-Reg'\n",
    "\n",
    "# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\n",
    "create_combined_sheet(file_path, sheets)\n",
    "\n",
    "# Lakukan regresi pada dataset di file Excel\n",
    "# perform_elm_regression(file_path, sheets + ['CombinedSheet'], epochs=400)\n",
    "perform_elm_regression(file_path, sheets + ['Comb-'+name_CombinedSheet], epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh pengujian data test tunggal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_hidden = k (dinamis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Uji: \n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "panjang fitur input =  14\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json\n",
      "Hasil Regresi: [ 0.1073793   0.26509696 -0.1784997   0.5654343   0.60516614  0.32936832\n",
      "  0.18486352  0.21288307  0.09744537 -0.09417725  0.29688022 -0.18494943\n",
      "  0.22081739  0.25770527  0.19827545 -0.04199389  0.23522541  0.04537347\n",
      " -0.17349482  0.0087724   0.25121891 -0.18447196  0.21870555  0.13491783\n",
      "  0.29832458 -0.05456029 -0.17469424  0.17618102  0.15832575 -0.06701343\n",
      "  0.03177232  0.6486465  -0.00766893 -0.11467499  0.01232845  0.06094086\n",
      "  0.27129242 -0.23931825  0.93241143  0.24489582  0.66373444 -0.08085257\n",
      "  0.34670475 -0.03250673]\n",
      "Panjang dim Hasil Regresi: 44\n",
      "\n",
      "Top Values: [0.93241143 0.66373444]\n",
      "Top Indices: [38 40]\n",
      "Top Column Names: ['telur', 'tuna']\n",
      "\n",
      "Hasil nilai loss: 0.06408128887414932\n"
     ]
    }
   ],
   "source": [
    "n_input = 14   # Jumlah kolom yang akan diambil\n",
    "n_output = 44\n",
    "hidden_layers = [100, 50, 25, 12, 6, 3] \n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "# Fungsi utama untuk memuat dan memproses data berdasarkan parameter yang diberikan\n",
    "def get_data_test(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    # data_test_row = df.iloc[id_test_data, :n_input]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    # data_test = data_test_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "    \n",
    "    # Konversi menjadi list\n",
    "    # data_test = data_test_row.values.tolist() if isinstance(data_test_row, pd.DataFrame) else data_test_row.tolist()\n",
    "    \n",
    "    # # Ambil data dari indeks tertentu atau banyak indeks (jika id_test_data berupa array)\n",
    "    # if isinstance(id_test_data, (list, np.ndarray)):\n",
    "    #     data_test_rows = df.iloc[id_test_data, :n_input]\n",
    "    #     data_test = data_test_rows.values.tolist()  # Mengembalikan sebagai list of lists\n",
    "    # else:\n",
    "    #     data_test_row = df.iloc[id_test_data, :n_input]\n",
    "    #     data_test = data_test_row.tolist()  # Mengembalikan sebagai list\n",
    "    \n",
    "    # Pastikan id_test_data adalah array 1 dimensi dari indeks\n",
    "    if isinstance(id_test_data, (list, np.ndarray)):\n",
    "        id_test_data = np.array(id_test_data).flatten()  # Konversi ke array 1D\n",
    "\n",
    "    # Ambil data dari indeks tertentu atau banyak indeks (jika id_test_data berupa array)\n",
    "    data_test_rows = df.iloc[id_test_data, :n_input]\n",
    "    data_test = data_test_rows.values.tolist()  # Mengembalikan sebagai list of lists\n",
    "    \n",
    "    return data_test\n",
    "\n",
    "def get_y_gt(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    y_gt_row = df.iloc[id_test_data, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "    return y_gt\n",
    "\n",
    "def test_single_data_return_loss(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.FloatTensor(X_tensor))\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        # print(f'Testing Loss: {mse_loss.item()}')\n",
    "        \n",
    "    return mse_loss.item()\n",
    "        \n",
    "def test_single_data_return_pred(model, single_data_test):\n",
    "    model.eval()  # Set model ke mode evaluasi\n",
    "    with torch.no_grad():  # Matikan gradient calculation\n",
    "        input_tensor = torch.FloatTensor(single_data_test).unsqueeze(0)  # Tambahkan dimensi batch\n",
    "        prediction = model(input_tensor)  # Lakukan prediksi\n",
    "        # print(f\"Data Uji: {single_data}\")  # Tampilkan data uji\n",
    "        # print(f\"Hasil Regresi: {prediction.numpy().flatten()}\")  # Tampilkan hasil regresi\n",
    "\n",
    "    return prediction.numpy().flatten()\n",
    "\n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()\n",
    "\n",
    "def get_top_k_columns(predictions, topk, n_input = 14, n_output = 44, input_file_path = \"dataset/dataset_v4.xlsx\", sheet_name = 'CombinedSheet'):\n",
    "    # Baca file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Ambil nama-nama kolom terakhir (n_output)\n",
    "    output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "    # Dapatkan nilai dan indeks top-k\n",
    "    top_values, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "    # Ambil nama kolom berdasarkan indeks top-k dan konversi menjadi daftar string\n",
    "    top_column_names = output_column_names[top_indices].tolist()\n",
    "\n",
    "    # Return top values, indices, dan nama kolom\n",
    "    return top_values, top_indices, top_column_names\n",
    "\n",
    "id_test_data = 0\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "test_data = get_data_test(id_test_data)\n",
    "print(\"Data Uji: \")  # Tampilkan data uji\n",
    "print(test_data)\n",
    "print('panjang fitur input = ',len(test_data))\n",
    "print()\n",
    "\n",
    "y_true_test_data = get_y_gt(id_test_data)\n",
    "\n",
    "# Uji model dengan satu data uji tunggal\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "elm_model_n_hidden_layers = ELMRegression(n_input, hidden_layers, n_output)\n",
    "# load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "# load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers, hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg_6_hidden_layers_100_50_25_12_6_3_31-10-2024-10-09-13.json', elm_model_n_hidden_layers)  # Ganti dengan nama sheet yang sesuai\n",
    "hasil_pred = test_single_data_return_pred(elm_model_n_hidden_layers, test_data)\n",
    "print(f\"Hasil Regresi: {hasil_pred}\") \n",
    "print(f\"Panjang dim Hasil Regresi: {len(hasil_pred)}\") \n",
    "\n",
    "print()\n",
    "topk = 2\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(hasil_pred, topk)\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n",
    "\n",
    "print()\n",
    "\n",
    "# test_single_data_return_loss(elm_model, test_data, y)\n",
    "# X_tensor = torch.FloatTensor(X)\n",
    "# nilai_loss = test_single_data_return_loss(elm_model, torch.FloatTensor(test_data), np.array(y_true_test_data))\n",
    "nilai_loss = test_single_data_return_loss(elm_model_n_hidden_layers, test_data, y_true_test_data)\n",
    "print(f\"Hasil nilai loss: {nilai_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_hidden = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Uji: \n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "panjang fitur input =  14\n",
      "\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "Hasil Regresi: [-3.72529030e-09  0.00000000e+00  7.45058060e-09 -2.98023224e-08\n",
      "  1.00000000e+00  1.49011612e-08 -3.72529030e-08  0.00000000e+00\n",
      " -3.72529030e-08  1.49011612e-08  1.00000000e+00  3.72529030e-09\n",
      "  2.98023224e-08 -1.30385160e-08  4.47034836e-08  3.72529030e-08\n",
      "  7.91624188e-09  4.47034836e-08 -2.98023224e-08 -2.98023224e-08\n",
      " -2.98023224e-08 -1.67638063e-08 -2.98023224e-08  2.60770321e-08\n",
      "  0.00000000e+00 -6.98491931e-09  2.23517418e-08  0.00000000e+00\n",
      " -2.23517418e-08  0.00000000e+00 -1.49011612e-08  1.00000000e+00\n",
      "  0.00000000e+00 -5.96046448e-08  7.45058060e-09 -3.11993062e-08\n",
      "  0.00000000e+00 -1.49011612e-08  1.00000024e+00  1.21071935e-08\n",
      "  1.49011612e-08  1.11758709e-08  1.16415322e-08  1.49011612e-08]\n",
      "Panjang dim Hasil Regresi: 44\n",
      "\n",
      "Top Values: [1.0000002 1.       ]\n",
      "Top Indices: [38 10]\n",
      "Top Column Names: ['telur', 'daging merah segar']\n",
      "\n",
      "Hasil nilai loss: 1.794965729734954e-15\n"
     ]
    }
   ],
   "source": [
    "# def get_y_gt(input_no_surah):\n",
    "#     # Memuat file Excel\n",
    "#     df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "#     # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "#     df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "#     # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "#     y_gt_row = df.iloc[input_no_surah - 1, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "#     y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "#     return y_gt\n",
    "\n",
    "n_input = 14   # Jumlah kolom yang akan diambil\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "# Fungsi utama untuk memuat dan memproses data berdasarkan parameter yang diberikan\n",
    "def get_data_test(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    # data_test_row = df.iloc[id_test_data, :n_input]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    # data_test = data_test_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "    \n",
    "    # Konversi menjadi list\n",
    "    # data_test = data_test_row.values.tolist() if isinstance(data_test_row, pd.DataFrame) else data_test_row.tolist()\n",
    "    \n",
    "    # # Ambil data dari indeks tertentu atau banyak indeks (jika id_test_data berupa array)\n",
    "    # if isinstance(id_test_data, (list, np.ndarray)):\n",
    "    #     data_test_rows = df.iloc[id_test_data, :n_input]\n",
    "    #     data_test = data_test_rows.values.tolist()  # Mengembalikan sebagai list of lists\n",
    "    # else:\n",
    "    #     data_test_row = df.iloc[id_test_data, :n_input]\n",
    "    #     data_test = data_test_row.tolist()  # Mengembalikan sebagai list\n",
    "    \n",
    "    # Pastikan id_test_data adalah array 1 dimensi dari indeks\n",
    "    if isinstance(id_test_data, (list, np.ndarray)):\n",
    "        id_test_data = np.array(id_test_data).flatten()  # Konversi ke array 1D\n",
    "\n",
    "    # Ambil data dari indeks tertentu atau banyak indeks (jika id_test_data berupa array)\n",
    "    data_test_rows = df.iloc[id_test_data, :n_input]\n",
    "    data_test = data_test_rows.values.tolist()  # Mengembalikan sebagai list of lists\n",
    "    \n",
    "    return data_test\n",
    "\n",
    "def get_y_gt(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    y_gt_row = df.iloc[id_test_data, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "    return y_gt\n",
    "\n",
    "def test_single_data_return_loss(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.FloatTensor(X_tensor))\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        # print(f'Testing Loss: {mse_loss.item()}')\n",
    "        \n",
    "    return mse_loss.item()\n",
    "        \n",
    "def test_single_data_return_pred(model, single_data_test):\n",
    "    model.eval()  # Set model ke mode evaluasi\n",
    "    with torch.no_grad():  # Matikan gradient calculation\n",
    "        input_tensor = torch.FloatTensor(single_data_test).unsqueeze(0)  # Tambahkan dimensi batch\n",
    "        prediction = model(input_tensor)  # Lakukan prediksi\n",
    "        # print(f\"Data Uji: {single_data}\")  # Tampilkan data uji\n",
    "        # print(f\"Hasil Regresi: {prediction.numpy().flatten()}\")  # Tampilkan hasil regresi\n",
    "\n",
    "    return prediction.numpy().flatten()\n",
    "\n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()\n",
    "\n",
    "def get_top_k_columns(predictions, topk, n_input = 14, n_output = 44, input_file_path = \"dataset/dataset_v4.xlsx\", sheet_name = 'CombinedSheet'):\n",
    "    # Baca file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Ambil nama-nama kolom terakhir (n_output)\n",
    "    output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "    # Dapatkan nilai dan indeks top-k\n",
    "    top_values, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "    # Ambil nama kolom berdasarkan indeks top-k dan konversi menjadi daftar string\n",
    "    top_column_names = output_column_names[top_indices].tolist()\n",
    "\n",
    "    # Return top values, indices, dan nama kolom\n",
    "    return top_values, top_indices, top_column_names\n",
    "\n",
    "id_test_data = 0\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "test_data = get_data_test(id_test_data)\n",
    "print(\"Data Uji: \")  # Tampilkan data uji\n",
    "print(test_data)\n",
    "print('panjang fitur input = ',len(test_data))\n",
    "print()\n",
    "\n",
    "# y_true_test_data = [-5.0489027e-03, -5.2880775e-03, -9.3119070e-03,  9.9426097e-01,\n",
    "#        -1.1416838e-02, -1.7818958e-03, -1.5794784e-03, -6.5919980e-03,\n",
    "#         1.4290400e-03, -2.1043243e-02, -6.3767806e-03, -2.2926459e-02,\n",
    "#        -2.2397846e-02, -2.0889074e-02,  9.8889613e-01, -7.4259415e-03,\n",
    "#        -1.5015602e-03,  2.1077983e-02,  9.3125552e-03,  7.7149123e-03,\n",
    "#         6.8175085e-03, -1.6389485e-02, -7.5197443e-03,  4.2867437e-03,\n",
    "#         9.8920465e-01,  5.5645481e-03, -1.9291833e-02, -6.0817376e-03,\n",
    "#         3.3459663e-03, -3.7721694e-03,  6.6576460e-03, -1.7112121e-02,\n",
    "#         2.5550649e-03,  2.9437169e-03, -5.9491619e-03, -1.1914853e-02,\n",
    "#        -2.5991499e-03, -6.0369158e-03, -3.2470629e-02, -2.7662516e-04,\n",
    "#         9.8775882e-01,  5.0603524e-03, -3.6268830e-03, -7.5169578e-03]\n",
    "\n",
    "y_true_test_data = get_y_gt(id_test_data)\n",
    "\n",
    "# Uji model dengan satu data uji tunggal\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "# load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "hasil_pred = test_single_data_return_pred(elm_model, test_data)\n",
    "print(f\"Hasil Regresi: {hasil_pred}\") \n",
    "print(f\"Panjang dim Hasil Regresi: {len(hasil_pred)}\") \n",
    "\n",
    "print()\n",
    "topk = 2\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(hasil_pred, topk)\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n",
    "\n",
    "print()\n",
    "\n",
    "# test_single_data_return_loss(elm_model, test_data, y)\n",
    "# X_tensor = torch.FloatTensor(X)\n",
    "# nilai_loss = test_single_data_return_loss(elm_model, torch.FloatTensor(test_data), np.array(y_true_test_data))\n",
    "nilai_loss = test_single_data_return_loss(elm_model, test_data, y_true_test_data)\n",
    "print(f\"Hasil nilai loss: {nilai_loss}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Deep ELM utk Model Reptile Adoption di EG-MAML - nn.Sequential dgn hidden layer = k (dinamis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom activation function NRReLU\n",
    "def NRReLU(x):\n",
    "    return 1 / (torch.exp(-x) - torch.exp(x))\n",
    "\n",
    "# Define a function to create the model with configurable layers and activation functions\n",
    "def create_model(n_input, n_hidden_layers, n_output, activations=None):\n",
    "    layers = []\n",
    "    input_dim = n_input\n",
    "    \n",
    "    # Ensure activations list matches the number of hidden layers, or use ReLU as default\n",
    "    if activations is None:\n",
    "        activations = [F.relu] * len(n_hidden_layers)  # Default to ReLU for all layers\n",
    "    elif len(activations) != len(n_hidden_layers):\n",
    "        raise ValueError(\"Length of activations must match number of hidden layers\")\n",
    "\n",
    "    # Add each hidden layer with the specified number of neurons and activation\n",
    "    for hidden_units, activation in zip(n_hidden_layers, activations):\n",
    "        layers.append(nn.Linear(input_dim, hidden_units))\n",
    "        input_dim = hidden_units  # Update input_dim for the next layer\n",
    "\n",
    "        # Add the activation layer as a callable function\n",
    "        layers.append(activation)  # Add the activation function directly\n",
    "\n",
    "    # Add the final output layer without activation\n",
    "    layers.append(nn.Linear(input_dim, n_output))\n",
    "    \n",
    "    # Create the model with nn.Sequential\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_hidden_layers = [64, 64, 32]  # Define hidden layers\n",
    "n_output = 44\n",
    "activations = [NRReLU, nn.Sigmoid(), nn.Tanh()]  # Custom activations, including NRReLU\n",
    "\n",
    "# Create the model with custom activations\n",
    "model = create_model(n_input, n_hidden_layers, n_output, activations)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Custom activation function NRReLU\n",
    "class NRReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 1 / (torch.exp(-x) - torch.exp(x))\n",
    "\n",
    "# Define a function to create the model with configurable layers and activation functions\n",
    "def create_model(n_input, n_hidden_layers, n_output, activations=None):\n",
    "    layers = []\n",
    "    input_dim = n_input\n",
    "    \n",
    "    # Ensure activations list matches the number of hidden layers, or use ReLU as default\n",
    "    if activations is None:\n",
    "        activations = [nn.ReLU()] * len(n_hidden_layers)  # Default to ReLU for all layers\n",
    "    elif len(activations) != len(n_hidden_layers):\n",
    "        raise ValueError(\"Length of activations must match number of hidden layers\")\n",
    "\n",
    "    # Add each hidden layer with the specified number of neurons and activation\n",
    "    for hidden_units, activation in zip(n_hidden_layers, activations):\n",
    "        layers.append(nn.Linear(input_dim, hidden_units))\n",
    "        layers.append(activation)\n",
    "        input_dim = hidden_units  # Update input_dim for the next layer\n",
    "    \n",
    "    # Add the final output layer without activation\n",
    "    layers.append(nn.Linear(input_dim, n_output))\n",
    "    \n",
    "    # Create the model with nn.Sequential\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_hidden_layers = [64, 64, 32]  # Define hidden layers\n",
    "n_output = 44\n",
    "activations = [NRReLU(), nn.Sigmoid(), nn.Tanh()]  # Custom activations, including NRReLU\n",
    "\n",
    "# Create the model with custom activations\n",
    "model = create_model(n_input, n_hidden_layers, n_output, activations)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a function to create the model with configurable input, hidden layers, and output layers\n",
    "def create_model(n_input, n_hidden_layers, n_output):\n",
    "    layers = []\n",
    "    input_dim = n_input\n",
    "    \n",
    "    # Add each hidden layer with alternating activation functions\n",
    "    for i, hidden_units in enumerate(n_hidden_layers):\n",
    "        layers.append(nn.Linear(input_dim, hidden_units))\n",
    "        \n",
    "        # Use ReLU for the first layer, Tanh for others\n",
    "        if i % 2 == 0:\n",
    "            layers.append(nn.ReLU())\n",
    "        else:\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        input_dim = hidden_units  # Update input_dim for the next layer\n",
    "    \n",
    "    # Add the final output layer\n",
    "    layers.append(nn.Linear(input_dim, n_output))\n",
    "    \n",
    "    # Create the model with nn.Sequential\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_hidden_layers = [64, 64, 32]  # Define your hidden layers\n",
    "n_output = 44\n",
    "model = create_model(n_input, n_hidden_layers, n_output)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a function to create the model with configurable input, hidden layers, and output layers\n",
    "def create_model(n_input, n_hidden_layers, n_output):\n",
    "    layers = []\n",
    "    input_dim = n_input\n",
    "    \n",
    "    # Add each hidden layer with the specified number of neurons\n",
    "    for hidden_units in n_hidden_layers:\n",
    "        layers.append(nn.Linear(input_dim, hidden_units))\n",
    "        layers.append(nn.Tanh())\n",
    "        input_dim = hidden_units  # Update input_dim for the next layer\n",
    "    \n",
    "    # Add the final output layer\n",
    "    layers.append(nn.Linear(input_dim, n_output))\n",
    "    \n",
    "    # Create the model with nn.Sequential\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_hidden_layers = [64, 64, 32]  # Define your hidden layers\n",
    "n_output = 44\n",
    "model = create_model(n_input, n_hidden_layers, n_output)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Deep ELM utk Model Reptile Adoption di EG-MAML - nn.Sequential dgn hidden layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a function to create the model with configurable input and output layers\n",
    "def create_model(n_input, n_output):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(n_input, 64),   # Input layer\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(64, 64),        # Hidden layer 1\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(64, n_output)   # Output layer\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_output = 44\n",
    "model = create_model(n_input, n_output)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Deep ELM utk Model Reptile Adoption di EG-MAML - nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_combined_sheet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6k/t6s5_7b156500_twxhplmtjr0000gn/T/ipykernel_6290/4044791934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mcreate_combined_sheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m# Lakukan regresi pada dataset di file Excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_combined_sheet' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "   \n",
    "# untuk support \n",
    "# for param in elm_model_reptile.parameters():\n",
    "#     print(param.data)\n",
    "#     print(param.grad.data)\n",
    "\n",
    "class ModelForSyntheticReptile(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(ModelForSyntheticReptile, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.output = nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "        # Remove the following lines if you need gradients for hidden layers\n",
    "        # for param in self.hidden1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # for param in self.hidden2.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # for param in self.hidden3.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def save_model_elm_reptile_to_json(model, file_path):\n",
    "    model_dict = {\n",
    "        \"architecture\": {\n",
    "            \"n_input\": model.hidden1.in_features,\n",
    "            \"n_hidden1\": model.hidden1.out_features,\n",
    "            \"n_hidden2\": model.hidden2.out_features,\n",
    "            \"n_hidden3\": model.hidden3.out_features,\n",
    "            \"n_output\": model.output.out_features,\n",
    "        },\n",
    "        \"state_dict\": {k: v.tolist() for k, v in model.state_dict().items()}\n",
    "    }\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(model_dict, f)\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "\n",
    "def load_model_elm_reptile_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "\n",
    "    arch = model_dict[\"architecture\"]\n",
    "    model = ELMRegressionForReptile(\n",
    "        n_input=arch[\"n_input\"],\n",
    "        n_hidden1=arch[\"n_hidden1\"],\n",
    "        n_hidden2=arch[\"n_hidden2\"],\n",
    "        n_hidden3=arch[\"n_hidden3\"],\n",
    "        n_output=arch[\"n_output\"]\n",
    "    )\n",
    "\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in model_dict[\"state_dict\"].items()})\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Helper function to save loss to a JSON file\n",
    "def save_loss_model_elm_reptile_to_json(loss_per_epoch, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(loss_per_epoch, f)\n",
    "    print(f\"Loss saved to {file_path}\")\n",
    "\n",
    "# Helper function to plot loss and save as PNG and PDF\n",
    "def plot_loss_model_elm_reptile(loss_per_epoch, sheet_name):\n",
    "    plt.plot(loss_per_epoch)\n",
    "    plt.title(f'Loss per Epoch for {sheet_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    png_path = f'loss/loss_plot_reptile_{sheet_name}.png'\n",
    "    pdf_path = f'loss/loss_plot_reptile_{sheet_name}.pdf'\n",
    "    plt.savefig(png_path)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved to {png_path} and {pdf_path}\")\n",
    "\n",
    "# Main function to perform ELM regression using Reptile\n",
    "def perform_elm_regression_reptile(file_path, sheets, epochs=100):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :n_input].values  \n",
    "        y = df.iloc[:, n_input:].values  \n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        elm_model_reptile = ModelForSyntheticReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(elm_model_reptile.parameters(), lr=0.01)\n",
    "\n",
    "        # List to store loss per epoch\n",
    "        loss_per_epoch = []\n",
    "\n",
    "        # Training model with data\n",
    "        elm_model_reptile.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = elm_model_reptile(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_per_epoch.append(loss.item())\n",
    "\n",
    "        # Save model and loss\n",
    "        save_model_elm_reptile_to_json(elm_model_reptile, f'model_reg/model_reptile_{sheet_name}.json')\n",
    "        save_loss_model_elm_reptile_to_json(loss_per_epoch, f'loss/loss_reptile_{sheet_name}.json')\n",
    "\n",
    "        # Plot and save loss plot\n",
    "        plot_loss_model_elm_reptile(loss_per_epoch, sheet_name)\n",
    "\n",
    "        print(f\"Final Loss for {sheet_name}: {loss_per_epoch[-1]}\")\n",
    "\n",
    "# Parameters\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "\n",
    "# Example usage\n",
    "# file_path = 'dataset/dataset_v4.xlsx'\n",
    "# sheets = ['Sheet1', 'Sheet2']  # List of sheet names to process\n",
    "# perform_elm_regression_reptile(file_path, sheets, epochs=100)\n",
    "\n",
    "file_path = 'dataset/dataset_v4.xlsx'\n",
    "\n",
    "# sheets = ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4']\n",
    "sheets = ['Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg']\n",
    "\n",
    "#generate kode unik\n",
    "\n",
    "# name_CombinedSheet = 'x'.join(sheets)\n",
    "name_CombinedSheet = 'KMT-Tiny-Reg'\n",
    "\n",
    "# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\n",
    "create_combined_sheet(file_path, sheets)\n",
    "\n",
    "# Lakukan regresi pada dataset di file Excel\n",
    "# perform_elm_regression(file_path, sheets + ['CombinedSheet'], epochs=400)\n",
    "perform_elm_regression_reptile(file_path, sheets + ['Comb-'+name_CombinedSheet], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter data: tensor([[ 0.0013,  0.1339,  0.1422,  ..., -0.1968, -0.2744,  0.1805],\n",
      "        [ 0.0457,  0.2202,  0.1676,  ..., -0.0113, -0.0954,  0.0327],\n",
      "        [ 0.0985, -0.1751,  0.2561,  ...,  0.0146, -0.1956,  0.2069],\n",
      "        ...,\n",
      "        [ 0.1906,  0.0475,  0.2151,  ...,  0.0829,  0.2090, -0.2170],\n",
      "        [ 0.0549,  0.1472, -0.2568,  ..., -0.2081,  0.2251,  0.0528],\n",
      "        [ 0.0294, -0.0088, -0.1388,  ...,  0.1956, -0.1646, -0.1462]])\n",
      "Gradient data: tensor([[-1.1015e-04, -2.6513e-04,  1.3624e-03,  ..., -8.1812e-04,\n",
      "          1.3827e-03, -6.8681e-04],\n",
      "        [-1.4396e-03, -7.4219e-04, -3.1177e-03,  ...,  1.3130e-03,\n",
      "         -2.7446e-03,  7.8270e-04],\n",
      "        [-1.4069e-03,  2.4611e-05,  2.7217e-03,  ..., -2.2257e-03,\n",
      "          2.1891e-03, -1.4023e-03],\n",
      "        ...,\n",
      "        [-2.3830e-03, -7.1504e-04,  3.4413e-03,  ...,  8.7713e-05,\n",
      "          6.6853e-04,  1.2421e-03],\n",
      "        [ 5.1256e-04, -1.2806e-03, -2.1708e-03,  ..., -1.4421e-04,\n",
      "         -1.7908e-03,  6.3003e-04],\n",
      "        [ 6.5178e-04,  2.3925e-04, -1.3443e-03,  ...,  2.5612e-06,\n",
      "          3.3830e-04,  3.6563e-04]])\n",
      "Parameter data: tensor([ 0.0706, -0.1867,  0.2099,  0.1963, -0.0662, -0.1577, -0.1593, -0.0698,\n",
      "        -0.2121,  0.1937, -0.1821,  0.1587,  0.0821,  0.1251, -0.0383,  0.0636,\n",
      "         0.1203, -0.2739, -0.2594, -0.0293, -0.1370, -0.1225,  0.0343,  0.1208,\n",
      "        -0.2186, -0.1712,  0.2469, -0.1726, -0.1415,  0.2091,  0.0309,  0.1379,\n",
      "         0.0396, -0.0904,  0.1775, -0.0100,  0.0828,  0.2088, -0.1509,  0.1081,\n",
      "        -0.1856,  0.1002,  0.2258, -0.2499, -0.2296,  0.1519,  0.2146, -0.0870,\n",
      "         0.0507, -0.2494, -0.2342,  0.2173, -0.0599,  0.0361, -0.1905, -0.1107,\n",
      "        -0.0375,  0.0144, -0.0634, -0.0646, -0.0025,  0.0491,  0.0030,  0.0155,\n",
      "        -0.1356,  0.2531, -0.2429, -0.0767,  0.2218,  0.1132,  0.0968,  0.1145,\n",
      "         0.0169, -0.0577,  0.0681,  0.1391, -0.1748,  0.1613, -0.2407,  0.1848,\n",
      "         0.1602, -0.0425, -0.1414, -0.2542, -0.1337,  0.1233,  0.0710, -0.2572,\n",
      "         0.2334,  0.2091, -0.1559,  0.2503,  0.0809, -0.0437, -0.0599, -0.2147,\n",
      "         0.2764, -0.2017,  0.0535, -0.0275])\n",
      "Gradient data: tensor([-4.2152e-04, -1.1003e-03,  1.3902e-03,  4.3143e-04, -3.0696e-03,\n",
      "        -1.6473e-04,  6.9408e-04,  2.1317e-04,  6.9742e-05,  1.0209e-03,\n",
      "        -4.7769e-03, -1.2666e-03,  1.4000e-04, -6.0112e-05, -2.7008e-05,\n",
      "        -5.7885e-04,  4.4804e-03,  1.0381e-03,  2.8097e-04,  6.4120e-04,\n",
      "        -7.6064e-04,  9.7528e-04, -7.8245e-04,  2.1753e-03,  6.5967e-04,\n",
      "        -3.0223e-04, -6.2362e-05, -4.2551e-04, -1.4999e-03,  2.6864e-04,\n",
      "        -1.2619e-03,  8.3276e-04,  2.2692e-04, -9.9713e-04,  3.1364e-04,\n",
      "         9.3988e-04,  3.6395e-04, -1.0576e-04,  6.3643e-04, -2.6673e-04,\n",
      "        -1.2424e-03,  1.2775e-03,  5.3668e-04,  8.1930e-04, -8.3440e-04,\n",
      "         1.3826e-03, -1.2371e-03, -8.2943e-04,  2.3549e-03, -6.9103e-04,\n",
      "        -1.1396e-03,  7.9264e-06,  9.3549e-04,  9.3602e-04,  1.0884e-03,\n",
      "         2.3680e-04, -2.6443e-03, -8.0174e-04,  5.1908e-04, -6.5378e-04,\n",
      "        -5.6714e-04, -5.7812e-04, -1.0457e-03, -6.3398e-04, -1.2853e-03,\n",
      "         1.9728e-03,  1.7383e-03,  1.0108e-03,  4.2076e-04, -1.0586e-03,\n",
      "         1.5387e-03, -4.2427e-04, -2.1616e-03, -3.4355e-04,  1.6213e-03,\n",
      "        -1.9624e-03, -9.0311e-04,  7.9543e-04, -9.6772e-04, -2.0938e-03,\n",
      "        -1.1103e-03, -1.2825e-04, -1.3250e-04,  1.5658e-03, -1.5477e-03,\n",
      "        -2.3159e-04, -1.4809e-03,  3.1017e-04,  2.7053e-03,  1.9213e-03,\n",
      "         1.0649e-03,  3.2324e-04, -2.4581e-03, -7.3001e-04,  1.7543e-03,\n",
      "        -8.9320e-04, -3.5482e-04,  3.8910e-04, -2.5809e-03,  2.2092e-04])\n",
      "Parameter data: tensor([[-0.0106,  0.0106, -0.0427,  ..., -0.0327, -0.0182,  0.0595],\n",
      "        [-0.0056, -0.0196,  0.0098,  ...,  0.0157, -0.0464,  0.0480],\n",
      "        [-0.0489, -0.0119,  0.0675,  ..., -0.0263, -0.0791, -0.0878],\n",
      "        ...,\n",
      "        [-0.0368,  0.0257,  0.0474,  ..., -0.0028,  0.1084,  0.0290],\n",
      "        [ 0.0406, -0.0529, -0.0698,  ...,  0.0010, -0.0782, -0.0452],\n",
      "        [ 0.0185,  0.0289,  0.0051,  ..., -0.0178, -0.0682,  0.0332]])\n",
      "Gradient data: tensor([[-2.2416e-03, -9.1363e-04, -3.2437e-03,  ..., -1.1116e-03,\n",
      "          1.5619e-03,  3.0585e-03],\n",
      "        [-1.5958e-04,  5.9585e-04, -1.0052e-03,  ...,  6.0882e-04,\n",
      "          2.0452e-03, -1.3464e-03],\n",
      "        [ 1.8203e-04, -1.7452e-03,  3.6109e-05,  ..., -4.6881e-04,\n",
      "          1.1771e-03, -7.4926e-04],\n",
      "        ...,\n",
      "        [-1.2859e-03, -2.1190e-03,  1.0393e-04,  ..., -3.5019e-04,\n",
      "         -4.6696e-04, -2.3095e-03],\n",
      "        [-2.4130e-04, -5.4298e-04, -1.8818e-03,  ...,  4.1094e-04,\n",
      "          1.8533e-03,  2.7416e-04],\n",
      "        [ 1.3626e-04, -6.7424e-04,  1.7899e-03,  ..., -2.3678e-03,\n",
      "         -2.2524e-03,  2.5159e-03]])\n",
      "Parameter data: tensor([ 0.0325,  0.0203, -0.0541, -0.0440,  0.0002, -0.0567, -0.0249,  0.0124,\n",
      "        -0.0419, -0.0994, -0.0111,  0.0542,  0.0376,  0.0773,  0.0329, -0.0994,\n",
      "         0.0731,  0.0879, -0.0508, -0.0363,  0.0835,  0.0692, -0.0284,  0.0142,\n",
      "        -0.0967, -0.0557,  0.0135, -0.0620, -0.0884,  0.0066, -0.0080, -0.0048,\n",
      "        -0.0346,  0.0275,  0.0622, -0.0024,  0.0327,  0.0741,  0.0878, -0.0274,\n",
      "        -0.0256,  0.0819,  0.0483, -0.0161,  0.0068, -0.0076, -0.0684,  0.0866,\n",
      "        -0.0657,  0.0815])\n",
      "Gradient data: tensor([ 0.0059,  0.0016,  0.0055,  0.0020,  0.0026, -0.0044,  0.0067, -0.0005,\n",
      "        -0.0043,  0.0022,  0.0047,  0.0081, -0.0052, -0.0012, -0.0031,  0.0073,\n",
      "        -0.0046,  0.0052,  0.0022, -0.0033, -0.0031,  0.0027, -0.0054,  0.0005,\n",
      "         0.0057, -0.0005, -0.0119,  0.0004,  0.0050, -0.0002,  0.0008,  0.0009,\n",
      "        -0.0019,  0.0062, -0.0025, -0.0025,  0.0073,  0.0028, -0.0052, -0.0006,\n",
      "        -0.0010,  0.0064,  0.0082,  0.0007,  0.0022, -0.0016,  0.0030,  0.0029,\n",
      "         0.0037,  0.0022])\n",
      "Parameter data: tensor([[ 2.1042e-02,  1.2294e-01, -3.9096e-02,  ..., -7.9495e-02,\n",
      "         -4.6060e-02,  1.0878e-01],\n",
      "        [ 3.3365e-02, -2.8697e-02,  3.5813e-03,  ...,  2.0686e-02,\n",
      "         -1.1080e-04,  1.6784e-03],\n",
      "        [-7.4966e-02, -7.9526e-02,  7.7720e-02,  ...,  1.0039e-02,\n",
      "         -1.4679e-01,  9.7731e-02],\n",
      "        ...,\n",
      "        [-1.0399e-01, -3.5536e-02,  5.9444e-02,  ...,  8.3417e-02,\n",
      "         -9.0019e-02,  1.2472e-01],\n",
      "        [-1.4707e-01, -2.4109e-02,  5.0286e-02,  ...,  1.3876e-01,\n",
      "         -1.0114e-01, -2.4473e-02],\n",
      "        [-6.9697e-02, -1.1016e-01, -6.6167e-02,  ..., -5.5680e-02,\n",
      "          4.9036e-03,  1.2721e-01]])\n",
      "Gradient data: tensor([[-0.0022, -0.0032, -0.0042,  ...,  0.0020,  0.0015, -0.0007],\n",
      "        [-0.0023, -0.0016, -0.0031,  ..., -0.0008,  0.0022, -0.0002],\n",
      "        [-0.0013,  0.0020,  0.0015,  ...,  0.0007,  0.0005,  0.0039],\n",
      "        ...,\n",
      "        [ 0.0001, -0.0040, -0.0022,  ...,  0.0011,  0.0029,  0.0030],\n",
      "        [ 0.0030,  0.0052,  0.0011,  ..., -0.0021,  0.0037,  0.0005],\n",
      "        [-0.0005,  0.0014, -0.0013,  ..., -0.0016,  0.0039, -0.0007]])\n",
      "Parameter data: tensor([-0.0189, -0.0937, -0.0489,  0.0691,  0.0112,  0.0381, -0.0691, -0.0819,\n",
      "         0.1032,  0.0691,  0.0722, -0.0744,  0.1274, -0.1225,  0.1134, -0.0018,\n",
      "         0.1110,  0.1044,  0.1070,  0.0466, -0.0856,  0.1417, -0.0167,  0.0289,\n",
      "        -0.0411])\n",
      "Gradient data: tensor([-3.8197e-03, -1.0856e-02,  8.0361e-03, -2.3197e-03, -7.1529e-03,\n",
      "        -5.0109e-03,  1.4456e-02, -1.9244e-02,  9.1545e-03,  1.0472e-02,\n",
      "         6.8333e-03,  2.3054e-02,  1.4935e-02,  1.3047e-02,  5.8410e-03,\n",
      "        -1.8925e-03, -1.4077e-02,  2.6066e-03, -3.9534e-03,  1.0336e-02,\n",
      "         8.1076e-03, -6.1021e-05, -6.0513e-03, -4.8016e-03, -1.1758e-02])\n",
      "Parameter data: tensor([[-0.0023, -0.0926,  0.1473,  ..., -0.0763,  0.0554, -0.1429],\n",
      "        [ 0.1098, -0.1127, -0.1093,  ...,  0.1365,  0.0402, -0.0782],\n",
      "        [ 0.0070,  0.0313,  0.0553,  ..., -0.0622, -0.1064,  0.0885],\n",
      "        ...,\n",
      "        [ 0.1376,  0.1613, -0.0778,  ...,  0.0339,  0.0846, -0.1292],\n",
      "        [-0.0512,  0.1591,  0.0249,  ..., -0.0130,  0.1187,  0.0576],\n",
      "        [ 0.0724, -0.1722, -0.0556,  ...,  0.1429, -0.0472,  0.0340]])\n",
      "Gradient data: tensor([[ 5.2546e-03, -4.0972e-03,  1.2271e-03,  ..., -6.2685e-03,\n",
      "          1.0238e-04, -4.0368e-03],\n",
      "        [ 1.4135e-03, -1.2180e-03,  1.3378e-04,  ...,  1.4192e-04,\n",
      "          2.5321e-03, -1.5722e-03],\n",
      "        [ 8.0272e-04, -3.3684e-03,  1.6415e-03,  ..., -3.9911e-03,\n",
      "         -3.6589e-03, -1.7861e-03],\n",
      "        ...,\n",
      "        [-2.6768e-03, -2.9800e-03,  8.3640e-04,  ...,  1.7739e-03,\n",
      "         -5.3578e-04, -1.0843e-03],\n",
      "        [-4.8131e-04,  4.1360e-04,  1.4078e-03,  ...,  5.1718e-04,\n",
      "         -2.1810e-03,  2.3554e-03],\n",
      "        [ 4.5481e-03,  8.0694e-05,  4.6836e-05,  ..., -5.8889e-04,\n",
      "          2.4763e-03, -1.3717e-03]])\n",
      "Parameter data: tensor([ 0.0277,  0.0275,  0.1415,  0.1850,  0.0475,  0.0429, -0.0659,  0.0912,\n",
      "        -0.1275, -0.0554,  0.0899, -0.1547, -0.0294,  0.0084, -0.0005,  0.0796,\n",
      "         0.1070,  0.1879, -0.0124,  0.1504, -0.1330, -0.0147,  0.0013,  0.1970,\n",
      "        -0.1192, -0.1166, -0.0073, -0.1797, -0.1770,  0.1356,  0.0982, -0.0179,\n",
      "        -0.1608,  0.1212,  0.1751,  0.0247,  0.0179, -0.0431,  0.0691, -0.0819,\n",
      "        -0.0090, -0.0800, -0.0965,  0.1130])\n",
      "Gradient data: tensor([ 3.3896e-02,  8.4263e-03,  4.3126e-03,  1.2304e-02, -4.9642e-03,\n",
      "         2.1552e-02, -1.7501e-02,  2.1159e-02, -2.5978e-02, -2.6040e-02,\n",
      "         1.0301e-02,  2.8787e-02,  4.5922e-03,  1.8982e-03, -8.2853e-03,\n",
      "         1.3487e-02,  1.9469e-02,  2.0218e-02,  4.2275e-03, -7.4817e-03,\n",
      "        -1.1751e-02,  5.9087e-03, -9.1869e-03, -1.4440e-03, -6.2549e-03,\n",
      "        -6.4166e-03,  1.6733e-02, -5.1684e-05, -1.2371e-02,  7.2244e-03,\n",
      "        -2.0393e-02,  7.4262e-03, -2.1833e-02,  8.6044e-03,  5.9873e-03,\n",
      "        -1.2611e-02,  1.4747e-02, -9.1828e-03, -1.4190e-03, -1.4577e-02,\n",
      "         2.9254e-02,  8.1865e-04, -2.5416e-02,  3.3190e-03])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "elm_model_reptile = ELMRegressionForReptile(n_input=14, n_hidden1=100, n_hidden2=50, n_hidden3=25, n_output=44)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(elm_model_reptile.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "X_dummy = torch.randn(10, 14)\n",
    "y_dummy = torch.randn(10, 44)\n",
    "\n",
    "# Forward and backward pass\n",
    "elm_model_reptile.train()\n",
    "optimizer.zero_grad()\n",
    "outputs = elm_model_reptile(X_dummy)\n",
    "loss = criterion(outputs, y_dummy)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Print parameter data and gradients\n",
    "for param in elm_model_reptile.parameters():\n",
    "    print(\"Parameter data:\", param.data)\n",
    "    if param.grad is not None:\n",
    "        print(\"Gradient data:\", param.grad.data)\n",
    "    else:\n",
    "        print(\"No gradient computed for this parameter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sheet 'CombinedSheet' created/overwritten in dataset/dataset_v4.xlsx\n",
      "Processing sheet: Sheet1-KM-SAR-Tiny-Reg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. This error is frequently encountered on macOS when running an x86 Python installation on ARM hardware. In this case, try installing an ARM build of Python. Otherwise, you may be able work around this issue by building jaxlib from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6k/t6s5_7b156500_twxhplmtjr0000gn/T/ipykernel_6290/321242477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# Lakukan regresi pada dataset di file Excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;31m# perform_elm_regression(file_path, sheets + ['CombinedSheet'], epochs=400)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m \u001b[0mperform_elm_regression_reptile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Comb-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname_CombinedSheet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/6k/t6s5_7b156500_twxhplmtjr0000gn/T/ipykernel_6290/321242477.py\u001b[0m in \u001b[0;36mperform_elm_regression_reptile\u001b[0;34m(file_path, sheets, epochs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melm_model_reptile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# List to store loss per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapturable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         differentiable=differentiable, fused=fused)\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallowed_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvert_frame\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m from .decorators import (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_dynamo/allowed_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbolic_trace\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_fx_tracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternal_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_compiling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_safe_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNP_SUPPORTED_MODULES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_dynamo/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbolic_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fx_tracing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_onnx_export\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mexternal_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_lazy_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from ._internal.exporter import (  # usort:skip. needs to be last to avoid circular import\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mExportOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mExportOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/onnx/_internal/exporter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m from torch.onnx._internal.fx import (\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mdecomposition_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mpatcher\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpatcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/onnx/_internal/fx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mONNXTorchPatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model_with_external_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/onnx/_internal/fx/patcher.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# safetensors is not an exporter requirement, but needed for some huggingface models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msafetensors\u001b[0m  \u001b[0;31m# type: ignore[import]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m  \u001b[0;31m# type: ignore[import]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msafetensors_torch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Set Cloud TPU env vars if necessary before transitively loading C++ backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud_tpu_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloud_tpu_init\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_cloud_tpu_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0m_cloud_tpu_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/cloud_tpu_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardware_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# uses instructions that are present on this machine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_feature_guard\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mcpu_feature_guard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cpu_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. This error is frequently encountered on macOS when running an x86 Python installation on ARM hardware. In this case, try installing an ARM build of Python. Otherwise, you may be able work around this issue by building jaxlib from source."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ELMRegressionForReptile(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(ELMRegressionForReptile, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.output = nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "        for param in self.hidden1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden3.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "# Fungsi untuk menggabungkan semua sheet menjadi satu\n",
    "def create_combined_sheet(file_path, sheets, combined_sheet_name=\"CombinedSheet\"):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name=combined_sheet_name, index=False)\n",
    "    \n",
    "    print(f\"Combined sheet '{combined_sheet_name}' created/overwritten in {file_path}\")\n",
    "\n",
    "def save_model_elm_reptile_to_json(model, file_path):\n",
    "    model_dict = {\n",
    "        \"architecture\": {\n",
    "            \"n_input\": model.hidden1.in_features,\n",
    "            \"n_hidden1\": model.hidden1.out_features,\n",
    "            \"n_hidden2\": model.hidden2.out_features,\n",
    "            \"n_hidden3\": model.hidden3.out_features,\n",
    "            \"n_output\": model.output.out_features,\n",
    "        },\n",
    "        \"state_dict\": {k: v.tolist() for k, v in model.state_dict().items()}\n",
    "    }\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(model_dict, f)\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "\n",
    "def load_model_elm_reptile_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "\n",
    "    arch = model_dict[\"architecture\"]\n",
    "    model = ELMRegressionForReptile(\n",
    "        n_input=arch[\"n_input\"],\n",
    "        n_hidden1=arch[\"n_hidden1\"],\n",
    "        n_hidden2=arch[\"n_hidden2\"],\n",
    "        n_hidden3=arch[\"n_hidden3\"],\n",
    "        n_output=arch[\"n_output\"]\n",
    "    )\n",
    "\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in model_dict[\"state_dict\"].items()})\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Helper function to save loss to a JSON file\n",
    "def save_loss_model_elm_reptile_to_json(loss_per_epoch, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(loss_per_epoch, f)\n",
    "    print(f\"Loss saved to {file_path}\")\n",
    "\n",
    "# Helper function to plot loss and save as PNG and PDF\n",
    "def plot_loss_model_elm_reptile(loss_per_epoch, sheet_name):\n",
    "    plt.plot(loss_per_epoch)\n",
    "    plt.title(f'Loss per Epoch for {sheet_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()\n",
    "    \n",
    "    png_path = f'loss/loss_plot_reptile_{sheet_name}.png'\n",
    "    pdf_path = f'loss/loss_plot_reptile_{sheet_name}.pdf'\n",
    "    plt.savefig(png_path)\n",
    "    plt.savefig(pdf_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved to {png_path} and {pdf_path}\")\n",
    "\n",
    "# Main function to perform ELM regression using Reptile\n",
    "def perform_elm_regression_reptile(file_path, sheets, epochs=100):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :n_input].values  \n",
    "        y = df.iloc[:, n_input:].values  \n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(elm_model_reptile.parameters(), lr=0.01)\n",
    "\n",
    "        # List to store loss per epoch\n",
    "        loss_per_epoch = []\n",
    "\n",
    "        # Training model with data\n",
    "        elm_model_reptile.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = elm_model_reptile(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_per_epoch.append(loss.item())\n",
    "\n",
    "        # Save model and loss\n",
    "        save_model_elm_reptile_to_json(elm_model_reptile, f'model_reg/model_reptile_{sheet_name}.json')\n",
    "        save_loss_model_elm_reptile_to_json(loss_per_epoch, f'loss/loss_reptile_{sheet_name}.json')\n",
    "\n",
    "        # Plot and save loss plot\n",
    "        plot_loss_model_elm_reptile(loss_per_epoch, sheet_name)\n",
    "\n",
    "        print(f\"Final Loss for {sheet_name}: {loss_per_epoch[-1]}\")\n",
    "\n",
    "# Parameters\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "\n",
    "# Example usage\n",
    "# file_path = 'dataset/dataset_v4.xlsx'\n",
    "# sheets = ['Sheet1', 'Sheet2']  # List of sheet names to process\n",
    "# perform_elm_regression_reptile(file_path, sheets, epochs=100)\n",
    "\n",
    "file_path = 'dataset/dataset_v4.xlsx'\n",
    "\n",
    "# sheets = ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4']\n",
    "sheets = ['Sheet1-KM-SAR-Tiny-Reg', 'Sheet2-M-SAK-T-Tiny-Reg']\n",
    "\n",
    "#generate kode unik\n",
    "\n",
    "# name_CombinedSheet = 'x'.join(sheets)\n",
    "name_CombinedSheet = 'KMT-Tiny-Reg'\n",
    "\n",
    "# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\n",
    "create_combined_sheet(file_path, sheets)\n",
    "\n",
    "# Lakukan regresi pada dataset di file Excel\n",
    "# perform_elm_regression(file_path, sheets + ['CombinedSheet'], epochs=400)\n",
    "perform_elm_regression_reptile(file_path, sheets + ['Comb-'+name_CombinedSheet], epochs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh pengujian data test tunggal - ELM-Reptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Uji: \n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "panjang fitur input =  14\n",
      "\n",
      "Model loaded from model_reg/model_reptile_Comb-KMT-Tiny-Reg.json\n",
      "Hasil Regresi: [-0.12000075 -0.06730618  0.07733716 -0.00994795  0.17467918 -0.16946383\n",
      " -0.22453593  0.03837297 -0.1567327   0.0053056   0.06305625 -0.10158335\n",
      " -0.21477349 -0.11649307  0.03974448 -0.21568018 -0.04273707  0.00049671\n",
      " -0.08075786  0.09297433  0.17740344 -0.09865375  0.0430309   0.08588327\n",
      "  0.14685765  0.02808246  0.2399827  -0.09006182 -0.0190486   0.1477305\n",
      " -0.0920798   0.10542276  0.12202488  0.0201454  -0.02017644  0.06110288\n",
      "  0.21422617 -0.08086558 -0.05691163  0.11137296  0.0174914  -0.25031805\n",
      " -0.24775949  0.08578105]\n",
      "Panjang dim Hasil Regresi: 44\n",
      "\n",
      "Top Values: [0.2399827  0.21422617]\n",
      "Top Indices: [26 36]\n",
      "Top Column Names: ['labu', 'susu kedelai']\n",
      "\n",
      "Hasil nilai loss: 0.09379100799560547\n"
     ]
    }
   ],
   "source": [
    "# def get_y_gt(input_no_surah):\n",
    "#     # Memuat file Excel\n",
    "#     df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "#     # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "#     df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "#     # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "#     y_gt_row = df.iloc[input_no_surah - 1, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "#     y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "#     return y_gt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_input = 14   # Jumlah kolom yang akan diambil\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "# Fungsi utama untuk memuat dan memproses data berdasarkan parameter yang diberikan\n",
    "def get_data_test(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Pastikan id_test_data adalah array 1 dimensi dari indeks\n",
    "    if isinstance(id_test_data, (list, np.ndarray)):\n",
    "        id_test_data = np.array(id_test_data).flatten()  # Konversi ke array 1D\n",
    "\n",
    "    # Ambil data dari indeks tertentu atau banyak indeks (jika id_test_data berupa array)\n",
    "    data_test_rows = df.iloc[id_test_data, :n_input]\n",
    "    data_test = data_test_rows.values.tolist()  # Mengembalikan sebagai list of lists\n",
    "    \n",
    "    return data_test\n",
    "\n",
    "def get_y_gt(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    y_gt_row = df.iloc[id_test_data, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "    return y_gt\n",
    "\n",
    "def test_single_data_return_loss(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.FloatTensor(X_tensor))\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        # print(f'Testing Loss: {mse_loss.item()}')\n",
    "        \n",
    "    return mse_loss.item()\n",
    "        \n",
    "def test_single_data_return_pred(model, single_data_test):\n",
    "    model.eval()  # Set model ke mode evaluasi\n",
    "    with torch.no_grad():  # Matikan gradient calculation\n",
    "        input_tensor = torch.FloatTensor(single_data_test).unsqueeze(0)  # Tambahkan dimensi batch\n",
    "        prediction = model(input_tensor)  # Lakukan prediksi\n",
    "        # print(f\"Data Uji: {single_data}\")  # Tampilkan data uji\n",
    "        # print(f\"Hasil Regresi: {prediction.numpy().flatten()}\")  # Tampilkan hasil regresi\n",
    "\n",
    "    return prediction.numpy().flatten()\n",
    "\n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()\n",
    "\n",
    "def get_top_k_columns(predictions, topk, n_input = 14, n_output = 44, input_file_path = \"dataset/dataset_v4.xlsx\", sheet_name = 'CombinedSheet'):\n",
    "    # Baca file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Ambil nama-nama kolom terakhir (n_output)\n",
    "    output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "    # Dapatkan nilai dan indeks top-k\n",
    "    top_values, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "    # Ambil nama kolom berdasarkan indeks top-k dan konversi menjadi daftar string\n",
    "    top_column_names = output_column_names[top_indices].tolist()\n",
    "\n",
    "    # Return top values, indices, dan nama kolom\n",
    "    return top_values, top_indices, top_column_names\n",
    "\n",
    "def get_top_k_columns_v2(predictions, topk, n_input = 14, n_output = 44, input_file_path = \"dataset/dataset_v4.xlsx\", sheet_name = 'CombinedSheet'):\n",
    "    # Baca file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Ambil nama-nama kolom terakhir (n_output)\n",
    "    output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "    # Dapatkan nilai dan indeks top-k\n",
    "    top_values, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "    # Ambil nama kolom berdasarkan indeks top-k dan konversi menjadi daftar string\n",
    "    top_column_names = output_column_names[top_indices].tolist()\n",
    "\n",
    "    # Return top values, indices, dan nama kolom\n",
    "    return top_values, top_indices, top_column_names, output_column_names\n",
    "\n",
    "id_test_data = 0\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "test_data = get_data_test(id_test_data)\n",
    "print(\"Data Uji: \")  # Tampilkan data uji\n",
    "print(test_data)\n",
    "print('panjang fitur input = ',len(test_data))\n",
    "print()\n",
    "\n",
    "y_true_test_data = get_y_gt(id_test_data)\n",
    "\n",
    "# Uji model dengan satu data uji tunggal\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3,n_output)\n",
    "# load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "# load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "# load_model_elm_reptile_from_json(f'model_reg/model_reptile_Comb-KMT-Tiny-Reg.json', elm_model_reptile)  # Ganti dengan nama sheet yang sesuai\n",
    "load_model_elm_reptile_from_json(f'model_reg/model_reptile_Comb-KMT-Tiny-Reg.json')  # Ganti dengan nama sheet yang sesuai\n",
    "hasil_pred = test_single_data_return_pred(elm_model_reptile, test_data)\n",
    "print(f\"Hasil Regresi: {hasil_pred}\") \n",
    "print(f\"Panjang dim Hasil Regresi: {len(hasil_pred)}\") \n",
    "\n",
    "print()\n",
    "topk = 2\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(hasil_pred, topk)\n",
    "top_values, top_indices, top_column_names, output_column_names = get_top_k_columns_v2(hasil_pred, topk)\n",
    "\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n",
    "\n",
    "print()\n",
    "\n",
    "# test_single_data_return_loss(elm_model, test_data, y)\n",
    "# X_tensor = torch.FloatTensor(X)\n",
    "# nilai_loss = test_single_data_return_loss(elm_model, torch.FloatTensor(test_data), np.array(y_true_test_data))\n",
    "nilai_loss = test_single_data_return_loss(elm_model_reptile, test_data, y_true_test_data)\n",
    "print(f\"Hasil nilai loss: {nilai_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air kelapa', 'air putih', 'bawang putih', 'bayam',\n",
       "       'biji-bijian utuh dan kacang-kacangan', 'brokoli', 'bubur',\n",
       "       'bubur gandum dari biji food grade (oatmeal)', 'bubur kacang hijau',\n",
       "       'daging (sapi)', 'daging merah segar', 'delima', 'hati ayam', 'ikan',\n",
       "       'jahe', 'jeruk', 'jus blewah', 'jus delima', 'jus melon', 'jus pisang',\n",
       "       'jus tomat', 'kalkun', 'kentang', 'kubis', 'kuning telur', 'kunyit',\n",
       "       'labu', 'madu', 'pasta', 'peppermint', 'pisang', 'sayuan berdaun gelap',\n",
       "       'sikat kayu siwak', 'smoothie bowl (mix jus bus buah dan chia seed)',\n",
       "       'sup ayam', 'sup hangat', 'susu kedelai', 'teh chamomile', 'telur',\n",
       "       'tiram', 'tuna', 'wijen', 'wortel', 'yoghurt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ELMRegressionForReptile(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(ELMRegressionForReptile, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.output = nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "        # Disable gradient tracking for hidden layers to simulate ELM\n",
    "        for param in self.hidden1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden3.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Standalone function to save model\n",
    "def save_model_elm_reptile_to_json(model, file_path):\n",
    "    model_dict = {\n",
    "        \"architecture\": {\n",
    "            \"n_input\": model.hidden1.in_features,\n",
    "            \"n_hidden1\": model.hidden1.out_features,\n",
    "            \"n_hidden2\": model.hidden2.out_features,\n",
    "            \"n_hidden3\": model.hidden3.out_features,\n",
    "            \"n_output\": model.output.out_features,\n",
    "        },\n",
    "        \"state_dict\": {k: v.tolist() for k, v in model.state_dict().items()}\n",
    "    }\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(model_dict, f)\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "\n",
    "# Standalone function to load model\n",
    "def load_model_elm_reptile_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "\n",
    "    arch = model_dict[\"architecture\"]\n",
    "    model = ELMRegressionForReptile(\n",
    "        n_input=arch[\"n_input\"],\n",
    "        n_hidden1=arch[\"n_hidden1\"],\n",
    "        n_hidden2=arch[\"n_hidden2\"],\n",
    "        n_hidden3=arch[\"n_hidden3\"],\n",
    "        n_output=arch[\"n_output\"]\n",
    "    )\n",
    "\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in model_dict[\"state_dict\"].items()})\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"Model loaded from {file_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "\n",
    "# Initialize and save model\n",
    "elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "save_model_elm_reptile_to_json(elm_model_reptile, 'elm_model.json')\n",
    "\n",
    "# Load model from JSON\n",
    "loaded_model = load_model_elm_reptile_from_json('elm_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ELMRegressionForReptile(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(ELMRegressionForReptile, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.output = nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "        # Disable gradient tracking for hidden layers to simulate ELM\n",
    "        for param in self.hidden1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden3.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def save_model_to_json(self, file_path):\n",
    "        # Save architecture and state dict to JSON\n",
    "        model_dict = {\n",
    "            \"architecture\": {\n",
    "                \"n_input\": self.hidden1.in_features,\n",
    "                \"n_hidden1\": self.hidden1.out_features,\n",
    "                \"n_hidden2\": self.hidden2.out_features,\n",
    "                \"n_hidden3\": self.hidden3.out_features,\n",
    "                \"n_output\": self.output.out_features,\n",
    "            },\n",
    "            \"state_dict\": {k: v.tolist() for k, v in self.state_dict().items()}\n",
    "        }\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(model_dict, f)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model_from_json(cls, file_path):\n",
    "        # Load architecture and state dict from JSON\n",
    "        with open(file_path, 'r') as f:\n",
    "            model_dict = json.load(f)\n",
    "\n",
    "        # Create model instance with saved architecture\n",
    "        arch = model_dict[\"architecture\"]\n",
    "        model = cls(\n",
    "            n_input=arch[\"n_input\"],\n",
    "            n_hidden1=arch[\"n_hidden1\"],\n",
    "            n_hidden2=arch[\"n_hidden2\"],\n",
    "            n_hidden3=arch[\"n_hidden3\"],\n",
    "            n_output=arch[\"n_output\"]\n",
    "        )\n",
    "\n",
    "        # Load state dict\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in model_dict[\"state_dict\"].items()})\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(f\"Model loaded from {file_path}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Example usage\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "\n",
    "# Initialize and save model\n",
    "elm_model = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "elm_model.save_model_to_json('model_reg/elm_model_reptile.json')\n",
    "\n",
    "# Load model from JSON\n",
    "loaded_model = ELMRegressionForReptile.load_model_from_json('model_reg/elm_model_reptile.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 44])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ELMRegressionForReptile(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(ELMRegression, self).__init__()\n",
    "        # Define the layers of the ELM model with random hidden weights\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.output = nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "        # Disable gradient tracking for hidden layers to keep weights fixed (ELM-specific)\n",
    "        for param in self.hidden1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.hidden3.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through each layer with activation\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation with the specified input and output dimensions\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25  # Added a third hidden layer for the ELM\n",
    "n_output = 44\n",
    "\n",
    "# Initialize the model\n",
    "elm_model_reptile = ELMRegressionForReptile(n_input, n_hidden1, n_hidden2, n_hidden3, n_output)\n",
    "\n",
    "# Example forward pass with random input\n",
    "example_input = torch.randn(1, n_input)  # Batch size of 1, 14 features\n",
    "output = elm_model_reptile(example_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2210, -0.0718,  0.1818,  0.0075, -0.0854,  0.1013,  0.2019,  0.0561,\n",
       "          0.0176, -0.0577, -0.1210,  0.3363, -0.0656, -0.0212,  0.1698,  0.0729,\n",
       "          0.0779,  0.0274, -0.1658, -0.2579, -0.0038,  0.2030,  0.1225,  0.1449,\n",
       "          0.0284,  0.1010,  0.0237, -0.0899,  0.0022,  0.2620, -0.0162,  0.1857,\n",
       "          0.0706, -0.1056, -0.0265,  0.0436, -0.0579,  0.1187,  0.2341,  0.1552,\n",
       "          0.1329, -0.1142, -0.2220, -0.3136]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7522,  0.3102, -1.0597,  0.4890,  0.4937, -0.7710,  0.5863, -0.3485,\n",
       "         -1.3217, -0.8827,  0.5210,  0.0093, -1.1410, -1.5055]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_test([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model for sheet: Sheet1-KM-SAR-Tiny-Reg\n",
      "Model loaded from model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json\n",
      "Testing Loss: 7.72236126363483e-16\n",
      "Testing model for sheet: Sheet2-M-SAK-T-Tiny-Reg\n",
      "Model loaded from model_reg/model_Sheet2-M-SAK-T-Tiny-Reg.json\n",
      "Testing Loss: 2.9147173985606884e-16\n",
      "Testing model for sheet: Comb-KMT-Tiny-Reg\n",
      "Model loaded from model_reg/model_Comb-KMT-Tiny-Reg.json\n",
      "Testing Loss: 5.346309438622389e-16\n",
      "Model loaded from model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.17992610e-03,  1.86512396e-02,  3.41992080e-03,  9.78489041e-01,\n",
       "        4.23134379e-02, -8.18528235e-04, -1.89460553e-02, -2.60318816e-03,\n",
       "       -4.43200767e-03, -1.28457621e-02,  3.82138267e-02,  1.02469474e-02,\n",
       "       -1.12136602e-02, -3.92608717e-03,  9.85177875e-01,  9.13687423e-03,\n",
       "        1.04745328e-02, -7.02193379e-03, -2.95397229e-02,  1.24486275e-02,\n",
       "        1.32074654e-02,  1.40234530e-02, -3.53866536e-03,  4.15021181e-03,\n",
       "        9.72698569e-01,  1.92141272e-02, -6.37222081e-03, -1.37380064e-02,\n",
       "       -5.66426665e-04, -1.14237815e-02, -8.11080635e-03,  2.65404638e-02,\n",
       "       -1.55364657e-02, -2.30008364e-03,  4.97524347e-03,  1.25560537e-02,\n",
       "       -8.48747045e-03, -1.77480914e-02,  4.21260372e-02, -1.23365968e-03,\n",
       "        9.67248440e-01, -2.08507478e-03, -9.84110311e-03,  1.28555074e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print()\n",
    "\n",
    "# Lakukan testing setelah meload model dari file JSON untuk semua sheet\n",
    "# load_and_test_all_models(file_path, sheets + ['CombinedSheet'])\n",
    "load_and_test_all_models(file_path, sheets + ['Comb-'+name_CombinedSheet])\n",
    "\n",
    "# Uji model dengan satu data uji tunggal\n",
    "test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "test_single_data_return_pred(elm_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Uji: \n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "panjang fitur input =  14\n",
      "\n",
      "Model loaded from model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json\n",
      "Hasil Regresi: [-7.4505806e-09 -1.4901161e-08 -1.4901161e-08  0.0000000e+00\n",
      "  1.0000002e+00 -2.2351742e-08 -1.4901161e-08  1.4901161e-08\n",
      " -3.7252903e-09 -2.7939677e-08  1.0000001e+00 -2.2351742e-08\n",
      "  1.4901161e-08 -1.1175871e-08 -7.4505806e-09 -2.6077032e-08\n",
      "  0.0000000e+00 -2.2351742e-08  7.4505806e-09  1.4901161e-08\n",
      "  0.0000000e+00 -2.2351742e-08 -1.9557774e-08  0.0000000e+00\n",
      "  0.0000000e+00 -2.9802322e-08 -5.2154064e-08 -1.4901161e-08\n",
      "  5.5879354e-09  1.4901161e-08 -1.4901161e-08  1.0000001e+00\n",
      "  4.6566129e-09 -4.4703484e-08 -9.3132257e-10  7.4505806e-09\n",
      " -1.4901161e-08  3.7252903e-09  1.0000001e+00  7.4505806e-09\n",
      " -7.4505806e-08 -1.4901161e-08 -2.6077032e-08  7.4505806e-09]\n",
      "Panjang dim Hasil Regresi: 44\n",
      "\n",
      "Top Values: [1.0000002 1.0000001]\n",
      "Top Indices: [ 4 31]\n",
      "Top Column Names: ['biji-bijian utuh dan kacang-kacangan', 'sayuan berdaun gelap']\n",
      "\n",
      "Hasil nilai loss: 2.684937953459127e-15\n"
     ]
    }
   ],
   "source": [
    "# def get_y_gt(input_no_surah):\n",
    "#     # Memuat file Excel\n",
    "#     df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "#     # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "#     df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "#     # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "#     y_gt_row = df.iloc[input_no_surah - 1, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "#     y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "#     return y_gt\n",
    "\n",
    "n_input = 14   # Jumlah kolom yang akan diambil\n",
    "n_output = 44\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "# Fungsi utama untuk memuat dan memproses data berdasarkan parameter yang diberikan\n",
    "def get_data_test(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    data_test_row = df.iloc[id_test_data, :n_input]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    data_test = data_test_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "    \n",
    "    return data_test\n",
    "\n",
    "def get_y_gt(id_test_data):\n",
    "    # Memuat file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Menghapus kolom \"Unnamed: 0\" jika ada\n",
    "    # df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Hapus semua kolom yang namanya mengandung 'Unnamed'\n",
    "\n",
    "    # Ambil baris sesuai input_no_surah, dan 114 kolom terakhir\n",
    "    y_gt_row = df.iloc[id_test_data, -n_output:]  # Indeks dimulai dari 0, jadi kurangi 1\n",
    "    y_gt = y_gt_row.tolist()  # Mengonversi ke dalam bentuk list\n",
    "\n",
    "    return y_gt\n",
    "\n",
    "def test_single_data_return_loss(model, X_tensor, y_true):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.FloatTensor(X_tensor))\n",
    "        mse_loss = nn.MSELoss()(predictions, torch.FloatTensor(y_true))\n",
    "        # print(f'Testing Loss: {mse_loss.item()}')\n",
    "        \n",
    "    return mse_loss.item()\n",
    "        \n",
    "def test_single_data_return_pred(model, single_data_test):\n",
    "    model.eval()  # Set model ke mode evaluasi\n",
    "    with torch.no_grad():  # Matikan gradient calculation\n",
    "        input_tensor = torch.FloatTensor(single_data_test).unsqueeze(0)  # Tambahkan dimensi batch\n",
    "        prediction = model(input_tensor)  # Lakukan prediksi\n",
    "        # print(f\"Data Uji: {single_data}\")  # Tampilkan data uji\n",
    "        # print(f\"Hasil Regresi: {prediction.numpy().flatten()}\")  # Tampilkan hasil regresi\n",
    "\n",
    "    return prediction.numpy().flatten()\n",
    "\n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()\n",
    "\n",
    "def get_top_k_columns(predictions, topk, n_input = 14, n_output = 44, input_file_path = \"dataset/dataset_v4.xlsx\", sheet_name = 'CombinedSheet'):\n",
    "    # Baca file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Ambil nama-nama kolom terakhir (n_output)\n",
    "    output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "    # Dapatkan nilai dan indeks top-k\n",
    "    top_values, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "    # Ambil nama kolom berdasarkan indeks top-k dan konversi menjadi daftar string\n",
    "    top_column_names = output_column_names[top_indices].tolist()\n",
    "\n",
    "    # Return top values, indices, dan nama kolom\n",
    "    return top_values, top_indices, top_column_names\n",
    "\n",
    "id_test_data = 0\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "test_data = get_data_test(id_test_data)\n",
    "print(\"Data Uji: \")  # Tampilkan data uji\n",
    "print(test_data)\n",
    "print('panjang fitur input = ',len(test_data))\n",
    "print()\n",
    "\n",
    "# y_true_test_data = [-5.0489027e-03, -5.2880775e-03, -9.3119070e-03,  9.9426097e-01,\n",
    "#        -1.1416838e-02, -1.7818958e-03, -1.5794784e-03, -6.5919980e-03,\n",
    "#         1.4290400e-03, -2.1043243e-02, -6.3767806e-03, -2.2926459e-02,\n",
    "#        -2.2397846e-02, -2.0889074e-02,  9.8889613e-01, -7.4259415e-03,\n",
    "#        -1.5015602e-03,  2.1077983e-02,  9.3125552e-03,  7.7149123e-03,\n",
    "#         6.8175085e-03, -1.6389485e-02, -7.5197443e-03,  4.2867437e-03,\n",
    "#         9.8920465e-01,  5.5645481e-03, -1.9291833e-02, -6.0817376e-03,\n",
    "#         3.3459663e-03, -3.7721694e-03,  6.6576460e-03, -1.7112121e-02,\n",
    "#         2.5550649e-03,  2.9437169e-03, -5.9491619e-03, -1.1914853e-02,\n",
    "#        -2.5991499e-03, -6.0369158e-03, -3.2470629e-02, -2.7662516e-04,\n",
    "#         9.8775882e-01,  5.0603524e-03, -3.6268830e-03, -7.5169578e-03]\n",
    "\n",
    "y_true_test_data = get_y_gt(id_test_data)\n",
    "\n",
    "# Uji model dengan satu data uji tunggal\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "hasil_pred = test_single_data_return_pred(elm_model, test_data)\n",
    "print(f\"Hasil Regresi: {hasil_pred}\") \n",
    "print(f\"Panjang dim Hasil Regresi: {len(hasil_pred)}\") \n",
    "\n",
    "print()\n",
    "topk = 2\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(hasil_pred, topk)\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n",
    "\n",
    "print()\n",
    "\n",
    "# test_single_data_return_loss(elm_model, test_data, y)\n",
    "# X_tensor = torch.FloatTensor(X)\n",
    "# nilai_loss = test_single_data_return_loss(elm_model, torch.FloatTensor(test_data), np.array(y_true_test_data))\n",
    "nilai_loss = test_single_data_return_loss(elm_model, test_data, y_true_test_data)\n",
    "print(f\"Hasil nilai loss: {nilai_loss}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_test(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_y_gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Uji: \n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "panjang fitur input =  14\n",
      "\n",
      "Model loaded from model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json\n",
      "Hasil Regresi: [-7.4505806e-09 -1.4901161e-08 -1.4901161e-08  0.0000000e+00\n",
      "  1.0000002e+00 -2.2351742e-08 -1.4901161e-08  1.4901161e-08\n",
      " -3.7252903e-09 -2.7939677e-08  1.0000001e+00 -2.2351742e-08\n",
      "  1.4901161e-08 -1.1175871e-08 -7.4505806e-09 -2.6077032e-08\n",
      "  0.0000000e+00 -2.2351742e-08  7.4505806e-09  1.4901161e-08\n",
      "  0.0000000e+00 -2.2351742e-08 -1.9557774e-08  0.0000000e+00\n",
      "  0.0000000e+00 -2.9802322e-08 -5.2154064e-08 -1.4901161e-08\n",
      "  5.5879354e-09  1.4901161e-08 -1.4901161e-08  1.0000001e+00\n",
      "  4.6566129e-09 -4.4703484e-08 -9.3132257e-10  7.4505806e-09\n",
      " -1.4901161e-08  3.7252903e-09  1.0000001e+00  7.4505806e-09\n",
      " -7.4505806e-08 -1.4901161e-08 -2.6077032e-08  7.4505806e-09]\n",
      "Panjang dim Hasil Regresi: 44\n",
      "\n",
      "Top Values: [1.0000002 1.0000001]\n",
      "Top Indices: [ 4 31]\n",
      "Top Column Names: ['biji-bijian utuh dan kacang-kacangan', 'sayuan berdaun gelap']\n",
      "\n",
      "Hasil nilai loss: 2.684937953459127e-15\n"
     ]
    }
   ],
   "source": [
    "id_test_data = 0\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "test_data = get_data_test(id_test_data)\n",
    "print(\"Data Uji: \")  # Tampilkan data uji\n",
    "print(test_data)\n",
    "print('panjang fitur input = ',len(test_data))\n",
    "print()\n",
    "\n",
    "# y_true_test_data = [-5.0489027e-03, -5.2880775e-03, -9.3119070e-03,  9.9426097e-01,\n",
    "#        -1.1416838e-02, -1.7818958e-03, -1.5794784e-03, -6.5919980e-03,\n",
    "#         1.4290400e-03, -2.1043243e-02, -6.3767806e-03, -2.2926459e-02,\n",
    "#        -2.2397846e-02, -2.0889074e-02,  9.8889613e-01, -7.4259415e-03,\n",
    "#        -1.5015602e-03,  2.1077983e-02,  9.3125552e-03,  7.7149123e-03,\n",
    "#         6.8175085e-03, -1.6389485e-02, -7.5197443e-03,  4.2867437e-03,\n",
    "#         9.8920465e-01,  5.5645481e-03, -1.9291833e-02, -6.0817376e-03,\n",
    "#         3.3459663e-03, -3.7721694e-03,  6.6576460e-03, -1.7112121e-02,\n",
    "#         2.5550649e-03,  2.9437169e-03, -5.9491619e-03, -1.1914853e-02,\n",
    "#        -2.5991499e-03, -6.0369158e-03, -3.2470629e-02, -2.7662516e-04,\n",
    "#         9.8775882e-01,  5.0603524e-03, -3.6268830e-03, -7.5169578e-03]\n",
    "\n",
    "y_true_test_data = get_y_gt(id_test_data)\n",
    "\n",
    "# Uji model dengan satu data uji tunggal\n",
    "# test_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]  # Contoh data uji\n",
    "elm_model = ELMRegression(n_input, n_hidden1, n_hidden2, n_output)\n",
    "load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "hasil_pred = test_single_data_return_pred(elm_model, test_data)\n",
    "print(f\"Hasil Regresi: {hasil_pred}\") \n",
    "print(f\"Panjang dim Hasil Regresi: {len(hasil_pred)}\") \n",
    "\n",
    "print()\n",
    "topk = 2\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(hasil_pred, topk)\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n",
    "\n",
    "print()\n",
    "\n",
    "# test_single_data_return_loss(elm_model, test_data, y)\n",
    "# X_tensor = torch.FloatTensor(X)\n",
    "# nilai_loss = test_single_data_return_loss(elm_model, torch.FloatTensor(test_data), np.array(y_true_test_data))\n",
    "nilai_loss = test_single_data_return_loss(elm_model, test_data, y_true_test_data)\n",
    "print(f\"Hasil nilai loss: {nilai_loss}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mendapatkan nilai dan indeks tertinggi dari hasil prediksi\n",
    "n_top = 1 \n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000002], dtype=float32), array([4]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topk_values_and_indices(hasil_pred, n_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Values: [1.0000002 1.0000001]\n",
      "Top Indices: [ 4 31]\n",
      "Top Column Names: ['biji-bijian utuh dan kacang-kacangan', 'sayuan berdaun gelap']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()\n",
    "\n",
    "def get_top_k_columns(predictions, topk, n_input = 14, n_output = 44, input_file_path = \"dataset/dataset_v4.xlsx\", sheet_name = 'CombinedSheet'):\n",
    "    # Baca file Excel\n",
    "    df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Ambil nama-nama kolom terakhir (n_output)\n",
    "    output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "    # Dapatkan nilai dan indeks top-k\n",
    "    top_values, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "    # Ambil nama kolom berdasarkan indeks top-k dan konversi menjadi daftar string\n",
    "    top_column_names = output_column_names[top_indices].tolist()\n",
    "\n",
    "    # Return top values, indices, dan nama kolom\n",
    "    return top_values, top_indices, top_column_names\n",
    "\n",
    "# Contoh penggunaan fungsi\n",
    "predictions = hasil_pred  # Prediksi contoh\n",
    "topk = 2  # Jumlah nilai top-k yang akan diambil\n",
    "n_input = 14   # Jumlah kolom yang akan diambil\n",
    "n_output = 44  # Kolom terakhir yang akan diambil\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "top_values, top_indices, top_column_names = get_top_k_columns(predictions, topk, n_input, n_output, input_file_path, sheet_name)\n",
    "\n",
    "print(\"Top Values:\", top_values)\n",
    "print(\"Top Indices:\", top_indices)\n",
    "print(\"Top Column Names:\", top_column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top Column Names: ['biji-bijian utuh dan kacang-kacangan', 'sayuan berdaun gelap', 'telur', 'daging merah segar']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_indices = [ 4 31 38]\n",
      "Nama-nama dari kolom terakhir berdasarkan top_k indeks: ['biji-bijian utuh dan kacang-kacangan', 'sayuan berdaun gelap', 'telur']\n"
     ]
    }
   ],
   "source": [
    "# Fungsi yang diberikan\n",
    "def get_topk_values_and_indices(predictions, topk):\n",
    "    top_values, top_indices = torch.topk(torch.FloatTensor(predictions), topk)\n",
    "    return top_values.numpy(), top_indices.numpy()\n",
    "\n",
    "# Konfigurasi\n",
    "n_input = 14   # Jumlah kolom yang akan diambil\n",
    "n_output = 44  # Kolom terakhir yang akan diambil\n",
    "input_file_path = \"dataset/dataset_v4.xlsx\"\n",
    "sheet_name = 'CombinedSheet'\n",
    "\n",
    "# Baca file Excel\n",
    "df = pd.read_excel(input_file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Pastikan kolom terakhir adalah n_output\n",
    "assert df.shape[1] >= n_output, \"Jumlah kolom di dataset tidak cukup untuk mengambil kolom ke-44.\"\n",
    "# output_column = df.iloc[:, n_output - 1]  # Mengambil kolom terakhir\n",
    "\n",
    "output_column_names = df.iloc[0:0, -n_output:].columns\n",
    "\n",
    "\n",
    "# Contoh prediksi dan top_k\n",
    "# predictions = [0.1, 0.9, 0.7, 0.3, 0.2]  # Prediksi contoh\n",
    "predictions = hasil_pred  # Prediksi contoh\n",
    "# topk = 3  # Jumlah nilai top-k yang akan diambil\n",
    "topk = 3  # Jumlah nilai top-k yang akan diambil\n",
    "\n",
    "# Dapatkan nilai dan indeks top-k\n",
    "_, top_indices = get_topk_values_and_indices(predictions, topk)\n",
    "\n",
    "print('top_indices =', top_indices)\n",
    "\n",
    "# Dapatkan nama-nama berdasarkan indeks top-k\n",
    "# top_names = output_column.iloc[top_indices].values\n",
    "top_column_names = output_column_names[top_indices].tolist()\n",
    "# top_column_names = output_column_names[top_indices].values\n",
    "\n",
    "# Output nama-nama top-k\n",
    "print(\"Nama-nama dari kolom terakhir berdasarkan top_k indeks:\", top_column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biji-bijian utuh dan kacang-kacangan', 'sayuan berdaun gelap', 'telur']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air kelapa', 'air putih', 'bawang putih', 'bayam',\n",
       "       'biji-bijian utuh dan kacang-kacangan', 'brokoli', 'bubur',\n",
       "       'bubur gandum dari biji food grade (oatmeal)', 'bubur kacang hijau',\n",
       "       'daging (sapi)', 'daging merah segar', 'delima', 'hati ayam', 'ikan',\n",
       "       'jahe', 'jeruk', 'jus blewah', 'jus delima', 'jus melon', 'jus pisang',\n",
       "       'jus tomat', 'kalkun', 'kentang', 'kubis', 'kuning telur', 'kunyit',\n",
       "       'labu', 'madu', 'pasta', 'peppermint', 'pisang', 'sayuan berdaun gelap',\n",
       "       'sikat kayu siwak', 'smoothie bowl (mix jus bus buah dan chia seed)',\n",
       "       'sup ayam', 'sup hangat', 'susu kedelai', 'teh chamomile', 'telur',\n",
       "       'tiram', 'tuna', 'wijen', 'wortel', 'yoghurt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:0, -n_output:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biji-bijian utuh dan kacang-kacangan'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0:0, -n_output:].columns[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air kelapa</th>\n",
       "      <th>air putih</th>\n",
       "      <th>bawang putih</th>\n",
       "      <th>bayam</th>\n",
       "      <th>biji-bijian utuh dan kacang-kacangan</th>\n",
       "      <th>brokoli</th>\n",
       "      <th>bubur</th>\n",
       "      <th>bubur gandum dari biji food grade (oatmeal)</th>\n",
       "      <th>bubur kacang hijau</th>\n",
       "      <th>daging (sapi)</th>\n",
       "      <th>...</th>\n",
       "      <th>sup ayam</th>\n",
       "      <th>sup hangat</th>\n",
       "      <th>susu kedelai</th>\n",
       "      <th>teh chamomile</th>\n",
       "      <th>telur</th>\n",
       "      <th>tiram</th>\n",
       "      <th>tuna</th>\n",
       "      <th>wijen</th>\n",
       "      <th>wortel</th>\n",
       "      <th>yoghurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   air kelapa  air putih  bawang putih  bayam  \\\n",
       "0           0          0             0      0   \n",
       "1           0          0             0      1   \n",
       "2           1          0             0      0   \n",
       "3           0          0             0      0   \n",
       "4           0          1             1      0   \n",
       "\n",
       "   biji-bijian utuh dan kacang-kacangan  brokoli  bubur  \\\n",
       "0                                     1        0      0   \n",
       "1                                     0        0      0   \n",
       "2                                     0        1      0   \n",
       "3                                     0        0      1   \n",
       "4                                     0        0      0   \n",
       "\n",
       "   bubur gandum dari biji food grade (oatmeal)  bubur kacang hijau  \\\n",
       "0                                            0                   0   \n",
       "1                                            0                   0   \n",
       "2                                            0                   0   \n",
       "3                                            1                   1   \n",
       "4                                            0                   0   \n",
       "\n",
       "   daging (sapi)  ...  sup ayam  sup hangat  susu kedelai  teh chamomile  \\\n",
       "0              0  ...         0           0             0              0   \n",
       "1              0  ...         0           0             0              0   \n",
       "2              1  ...         0           0             1              0   \n",
       "3              0  ...         0           1             0              0   \n",
       "4              0  ...         1           0             0              1   \n",
       "\n",
       "   telur  tiram  tuna  wijen  wortel  yoghurt  \n",
       "0      1      0     0      0       0        0  \n",
       "1      0      0     1      0       0        0  \n",
       "2      1      1     0      1       0        1  \n",
       "3      1      0     0      0       0        0  \n",
       "4      0      0     0      0       1        0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batuk</th>\n",
       "      <th>kehilangan keseimbangan</th>\n",
       "      <th>luka kecil pada mulut/lidah</th>\n",
       "      <th>migraine</th>\n",
       "      <th>nyeri di gigi atau gusi</th>\n",
       "      <th>nyeri di sekitar mulut/lidah/pipi/kepala</th>\n",
       "      <th>nyeri tenggorokan</th>\n",
       "      <th>pusing berputar</th>\n",
       "      <th>radang tenggorokan</th>\n",
       "      <th>sakit gigi</th>\n",
       "      <th>...</th>\n",
       "      <th>sup ayam</th>\n",
       "      <th>sup hangat</th>\n",
       "      <th>susu kedelai</th>\n",
       "      <th>teh chamomile</th>\n",
       "      <th>telur</th>\n",
       "      <th>tiram</th>\n",
       "      <th>tuna</th>\n",
       "      <th>wijen</th>\n",
       "      <th>wortel</th>\n",
       "      <th>yoghurt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   batuk  kehilangan keseimbangan  luka kecil pada mulut/lidah  migraine  \\\n",
       "0      0                        0                            0         1   \n",
       "1      0                        1                            0         0   \n",
       "2      0                        0                            1         0   \n",
       "3      0                        0                            0         0   \n",
       "4      1                        0                            0         0   \n",
       "\n",
       "   nyeri di gigi atau gusi  nyeri di sekitar mulut/lidah/pipi/kepala  \\\n",
       "0                        0                                         0   \n",
       "1                        0                                         0   \n",
       "2                        0                                         1   \n",
       "3                        1                                         0   \n",
       "4                        0                                         0   \n",
       "\n",
       "   nyeri tenggorokan  pusing berputar  radang tenggorokan  sakit gigi  ...  \\\n",
       "0                  0                0                   0           0  ...   \n",
       "1                  0                1                   0           0  ...   \n",
       "2                  0                0                   0           0  ...   \n",
       "3                  0                0                   0           1  ...   \n",
       "4                  1                0                   1           0  ...   \n",
       "\n",
       "   sup ayam  sup hangat  susu kedelai  teh chamomile  telur  tiram  tuna  \\\n",
       "0         0           0             0              0      1      0     0   \n",
       "1         0           0             0              0      0      0     1   \n",
       "2         0           0             1              0      1      1     0   \n",
       "3         0           1             0              0      1      0     0   \n",
       "4         1           0             0              1      0      0     0   \n",
       "\n",
       "   wijen  wortel  yoghurt  \n",
       "0      0       0        0  \n",
       "1      0       0        0  \n",
       "2      1       0        1  \n",
       "3      0       0        0  \n",
       "4      0       1        0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [-5.0489027e-03, -5.2880775e-03, -9.3119070e-03,  9.9426097e-01,\n",
    "       -1.1416838e-02, -1.7818958e-03, -1.5794784e-03, -6.5919980e-03,\n",
    "        1.4290400e-03, -2.1043243e-02, -6.3767806e-03, -2.2926459e-02,\n",
    "       -2.2397846e-02, -2.0889074e-02,  9.8889613e-01, -7.4259415e-03,\n",
    "       -1.5015602e-03,  2.1077983e-02,  9.3125552e-03,  7.7149123e-03,\n",
    "        6.8175085e-03, -1.6389485e-02, -7.5197443e-03,  4.2867437e-03,\n",
    "        9.8920465e-01,  5.5645481e-03, -1.9291833e-02, -6.0817376e-03,\n",
    "        3.3459663e-03, -3.7721694e-03,  6.6576460e-03, -1.7112121e-02,\n",
    "        2.5550649e-03,  2.9437169e-03, -5.9491619e-03, -1.1914853e-02,\n",
    "       -2.5991499e-03, -6.0369158e-03, -3.2470629e-02, -2.7662516e-04,\n",
    "        9.8775882e-01,  5.0603524e-03, -3.6268830e-03, -7.5169578e-03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasifikasi Deep ELM dgn one hot encoder n save + load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sheet 'CombinedSheet' created/overwritten in dataset/classification_data2.xlsx\n",
      "Processing sheet: Sheet1\n",
      "Model saved as JSON at model/model_Sheet1.json\n",
      "Accuracy for Sheet1: 1.0000\n",
      "Processing sheet: Sheet2\n",
      "Model saved as JSON at model/model_Sheet2.json\n",
      "Accuracy for Sheet2: 1.0000\n",
      "Processing sheet: Sheet3\n",
      "Model saved as JSON at model/model_Sheet3.json\n",
      "Accuracy for Sheet3: 1.0000\n",
      "Processing sheet: Sheet4\n",
      "Model saved as JSON at model/model_Sheet4.json\n",
      "Accuracy for Sheet4: 1.0000\n",
      "Processing sheet: CombinedSheet\n",
      "Model saved as JSON at model/model_CombinedSheet.json\n",
      "Accuracy for CombinedSheet: 1.0000\n",
      "\n",
      "Testing model for sheet: Sheet1\n",
      "Model loaded from JSON from model/model_Sheet1.json\n",
      "Test Accuracy: 1.0000\n",
      "Testing model for sheet: Sheet2\n",
      "Model loaded from JSON from model/model_Sheet2.json\n",
      "Test Accuracy: 1.0000\n",
      "Testing model for sheet: Sheet3\n",
      "Model loaded from JSON from model/model_Sheet3.json\n",
      "Test Accuracy: 1.0000\n",
      "Testing model for sheet: Sheet4\n",
      "Model loaded from JSON from model/model_Sheet4.json\n",
      "Test Accuracy: 1.0000\n",
      "Testing model for sheet: CombinedSheet\n",
      "Model loaded from JSON from model/model_CombinedSheet.json\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Definisi model ELM menggunakan PyTorch untuk klasifikasi dengan lebih dari 1 hidden layer\n",
    "class ELMClassification(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_output):\n",
    "        super(ELMClassification, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)  # Lapisan hidden pertama\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)  # Lapisan hidden kedua\n",
    "        self.output = nn.Linear(n_hidden2, n_output)  # Lapisan output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.hidden1(x))  # Aktivasi sigmoid pada hidden layer 1\n",
    "        x = torch.sigmoid(self.hidden2(x))  # Aktivasi sigmoid pada hidden layer 2\n",
    "        return self.output(x)  # Output lapisan terakhir, tanpa softmax karena digunakan oleh loss function\n",
    "\n",
    "# Fungsi untuk menyimpan model dalam format JSON\n",
    "def save_model_json(model, file_path):\n",
    "    model_params = {\n",
    "        \"hidden1_weights\": model.hidden1.weight.detach().numpy().tolist(),\n",
    "        \"hidden1_bias\": model.hidden1.bias.detach().numpy().tolist(),\n",
    "        \"hidden2_weights\": model.hidden2.weight.detach().numpy().tolist(),\n",
    "        \"hidden2_bias\": model.hidden2.bias.detach().numpy().tolist(),\n",
    "        \"output_weights\": model.output.weight.detach().numpy().tolist(),\n",
    "        \"output_bias\": model.output.bias.detach().numpy().tolist()\n",
    "    }\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(model_params, json_file)\n",
    "    print(f\"Model saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mencatat dan menyimpan akurasi dalam format JSON berdasarkan epoch\n",
    "def save_accuracy_json(accuracy_per_epoch, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(accuracy_per_epoch, json_file)\n",
    "    print(f\"Accuracy per epoch saved as JSON at {file_path}\")\n",
    "\n",
    "# Fungsi untuk mem-plot dan menyimpan hasil training (akurasi per epoch)\n",
    "def plot_accuracy(accuracy_per_epoch, sheet_name):\n",
    "    epochs = list(range(1, len(accuracy_per_epoch) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, accuracy_per_epoch, label='Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Training Accuracy per Epoch for {sheet_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Menyimpan plot dalam format PNG dan PDF\n",
    "    plt.savefig(f'acc/accuracy_plot_{sheet_name}.png')\n",
    "    plt.savefig(f'acc/accuracy_plot_{sheet_name}.pdf')\n",
    "    print(f\"Accuracy plot saved as PNG and PDF for {sheet_name}\")\n",
    "\n",
    "    plt.close()  # Menutup plot setelah selesai\n",
    "\n",
    "# Fungsi untuk memuat data dan melakukan ELM klasifikasi dengan multi hidden layers\n",
    "def perform_elm_classification(file_path, sheets, epochs=100):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Processing sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :-1].values  # Fitur\n",
    "        y = df.iloc[:, -1].values   # Target (kelas)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        \n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "        y_tensor = torch.FloatTensor(y_one_hot)\n",
    "\n",
    "        n_input = X.shape[1]\n",
    "        n_hidden1 = 100  # Ukuran hidden layer pertama\n",
    "        n_hidden2 = 50   # Ukuran hidden layer kedua\n",
    "        n_output = 5\n",
    "        elm_model = ELMClassification(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(elm_model.parameters(), lr=0.01)\n",
    "\n",
    "        # List untuk menyimpan akurasi di setiap epoch\n",
    "        accuracy_per_epoch = []\n",
    "\n",
    "        # Training model dengan data yang ada\n",
    "        elm_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = elm_model(X_tensor)\n",
    "            loss = criterion(outputs, torch.argmax(y_tensor, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Kalkulasi akurasi setiap epoch\n",
    "            with torch.no_grad():\n",
    "                predictions = elm_model(X_tensor)\n",
    "                predicted_classes = torch.argmax(predictions, dim=1)\n",
    "                accuracy = (predicted_classes.numpy() == torch.argmax(y_tensor, dim=1).numpy()).mean()\n",
    "                accuracy_per_epoch.append(accuracy)\n",
    "\n",
    "            # print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        # Menyimpan model dan akurasi\n",
    "        save_model_json(elm_model, f'model/model_{sheet_name}.json')\n",
    "        save_accuracy_json(accuracy_per_epoch, f'acc/accuracy_{sheet_name}.json')\n",
    "\n",
    "        # Plot akurasi dan simpan dalam format PNG dan PDF\n",
    "        plot_accuracy(accuracy_per_epoch, sheet_name)\n",
    "\n",
    "        print(f\"Final Accuracy for {sheet_name}: {accuracy_per_epoch[-1]:.4f}\")\n",
    "\n",
    "# Fungsi untuk menggabungkan semua sheet menjadi satu\n",
    "def create_combined_sheet(file_path, sheets, combined_sheet_name=\"CombinedSheet\"):\n",
    "    # Muat semua data dari sheet\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Memeriksa apakah sheet sudah ada, lalu menimpa atau mengganti\n",
    "    with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name=combined_sheet_name, index=False)\n",
    "    \n",
    "    print(f\"Combined sheet '{combined_sheet_name}' created/overwritten in {file_path}\")\n",
    "\n",
    "# Fungsi untuk meload model dari semua sheet dan lakukan testing\n",
    "def load_and_test_all_models(file_path, sheets):\n",
    "    excel_file = pd.ExcelFile(file_path)\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        print(f\"Testing model for sheet: {sheet_name}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "        X = df.iloc[:, :-1].values  # Fitur\n",
    "        y = df.iloc[:, -1].values   # Target (kelas)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "\n",
    "        n_input = X.shape[1]\n",
    "        n_hidden1 = 100\n",
    "        n_hidden2 = 50\n",
    "        n_output = 5\n",
    "        elm_model = ELMClassification(n_input, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "        # Memuat model dari file JSON\n",
    "        load_model_json(f'model/model_{sheet_name}.json', elm_model)\n",
    "\n",
    "        # Melakukan testing dan menghitung akurasi\n",
    "        test_model(elm_model, X_tensor, y)\n",
    "\n",
    "# Tentukan path ke file Excel dan nama sheet\n",
    "file_path = 'dataset/classification_data2.xlsx'\n",
    "sheets = ['Sheet1', 'Sheet2', 'Sheet3', 'Sheet4']\n",
    "\n",
    "# Buat sheet gabungan yang menggabungkan semua data dari sheet yang ada\n",
    "create_combined_sheet(file_path, sheets)\n",
    "\n",
    "# Lakukan klasifikasi pada dataset di file Excel dengan multi hidden layers dan simpan akurasi tiap epoch\n",
    "perform_elm_classification(file_path, sheets + ['CombinedSheet'], epochs=400)\n",
    "\n",
    "print()\n",
    "\n",
    "# Lakukan testing setelah meload model dari file JSON untuk semua sheet\n",
    "load_and_test_all_models(file_path, sheets + ['CombinedSheet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart EG-MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.path as mpath\n",
    "\n",
    "# Prepare the data for the PathPatch below.\n",
    "Path = mpath.Path\n",
    "codes, verts = zip(*[\n",
    "    (Path.MOVETO, [0.018, -0.11]),\n",
    "    (Path.CURVE4, [-0.031, -0.051]),\n",
    "    (Path.CURVE4, [-0.115, 0.073]),\n",
    "    (Path.CURVE4, [-0.03, 0.073]),\n",
    "    (Path.LINETO, [-0.011, 0.039]),\n",
    "    (Path.CURVE4, [0.043, 0.121]),\n",
    "    (Path.CURVE4, [0.075, -0.005]),\n",
    "    (Path.CURVE4, [0.035, -0.027]),\n",
    "    (Path.CLOSEPOLY, [0.018, -0.11])])\n",
    "\n",
    "artists = [\n",
    "    mpatches.Circle((0, 0), 0.1, ec=\"none\"),\n",
    "    mpatches.Rectangle((-0.025, -0.05), 0.05, 0.1, ec=\"none\"),\n",
    "    mpatches.Wedge((0, 0), 0.1, 30, 270, ec=\"none\"),\n",
    "    mpatches.RegularPolygon((0, 0), 4, radius=0.1),\n",
    "    mpatches.Ellipse((0, 0), 0.2, 0.1),\n",
    "    mpatches.Arrow(-0.05, -0.05, 0.1, 0.1, width=0.1),\n",
    "    mpatches.PathPatch(mpath.Path(verts, codes), ec=\"none\"),\n",
    "    mpatches.FancyBboxPatch((-0.025, -0.05), 0.05, 0.1, ec=\"none\",\n",
    "                            boxstyle=mpatches.BoxStyle(\"Round\", pad=0.02)),\n",
    "    mlines.Line2D([-0.06, 0.0, 0.1], [0.05, -0.05, 0.05], lw=5),\n",
    "]\n",
    "\n",
    "axs = plt.figure(figsize=(6, 6), layout=\"constrained\").subplots(3, 3)\n",
    "for i, (ax, artist) in enumerate(zip(axs.flat, artists)):\n",
    "    artist.set(color=mpl.colormaps[\"hsv\"](i / len(artists)))\n",
    "    ax.add_artist(artist)\n",
    "    ax.set(title=type(artist).__name__,\n",
    "           aspect=1, xlim=(-.2, .2), ylim=(-.2, .2))\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Membuat flowchart EG-MAML menggunakan matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Tambah kotak untuk setiap langkah di flowchart\n",
    "ax.text(0.5, 0.95, 'Start', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightgrey\"))\n",
    "\n",
    "ax.text(0.5, 0.85, 'Initialize Population', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"))\n",
    "\n",
    "ax.text(0.5, 0.75, 'Select Meta-Learning Tasks', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"))\n",
    "\n",
    "ax.text(0.5, 0.65, 'Apply MAML Update', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightgreen\"))\n",
    "\n",
    "ax.text(0.5, 0.55, 'Mutation and Crossover', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightgreen\"))\n",
    "\n",
    "ax.text(0.5, 0.45, 'Evaluate and Select', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"orange\"))\n",
    "\n",
    "ax.text(0.5, 0.35, 'Converged?', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=1\", edgecolor=\"black\", facecolor=\"pink\"))\n",
    "\n",
    "ax.text(0.3, 0.25, 'No', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
    "\n",
    "ax.text(0.7, 0.25, 'Yes', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
    "\n",
    "ax.text(0.5, 0.15, 'Test on New Tasks', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightgrey\"))\n",
    "\n",
    "ax.text(0.5, 0.05, 'End', ha='center', va='center', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightgrey\"))\n",
    "\n",
    "# Garis penghubung antar langkah\n",
    "ax.annotate(\"\", xy=(0.5, 0.91), xytext=(0.5, 0.86), arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.annotate(\"\", xy=(0.5, 0.81), xytext=(0.5, 0.76), arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.annotate(\"\", xy=(0.5, 0.71), xytext=(0.5, 0.66), arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.annotate(\"\", xy=(0.5, 0.61), xytext=(0.5, 0.56), arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.annotate(\"\", xy=(0.5, 0.51), xytext=(0.5, 0.46), arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.annotate(\"\", xy=(0.5, 0.41), xytext=(0.5, 0.36), arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "# Garis percabangan Yes/No\n",
    "ax.annotate(\"\", xy=(0.4, 0.31), xytext=(0.35, 0.26), arrowprops=dict(arrowstyle=\"->\"))\n",
    "ax.annotate(\"\", xy=(0.6, 0.31), xytext=(0.65, 0.26), arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "# Garis penghubung dari Yes ke Test on New Tasks\n",
    "ax.annotate(\"\", xy=(0.5, 0.31), xytext=(0.5, 0.16), arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "# Garis penghubung dari No ke Mutasi dan Crossover (mengulang siklus)\n",
    "ax.annotate(\"\", xy=(0.35, 0.25), xytext=(0.5, 0.56), arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "# Hapus sumbu\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Tampilkan flowchart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from netgraph import Graph # pip install netgraph\n",
    "\n",
    "graph_data = nx.DiGraph([(0, 1)]) # edge lists or igraph Graph objects are also supported\n",
    "\n",
    "g = Graph(graph_data,\n",
    "          node_layout = {0: (0, 0), 1: (1, 0)},\n",
    "          node_size = {0 : 10, 1 : 20},\n",
    "          node_color = {0 : \"red\", 1 : \"green\"},\n",
    "          node_labels = {0:\"Python\", 1:\"Programming\"},\n",
    "          node_label_fontdict = {'backgroundcolor' : 'lightgray'},\n",
    "          node_shape = {0 : \"s\", 1 : \"d\"},\n",
    "          arrows=True,\n",
    ")\n",
    "\n",
    "# Netgraph currently does not support multiple values for label backgroundcolors.\n",
    "# However, all artists are exposed in simple to query dictionaries.\n",
    "# As node label artists are matplotlib text objects,\n",
    "# we can vary the node label background colors by using matplotlib.text.Text methods:\n",
    "g.node_label_artists[0].set_backgroundcolor('salmon')\n",
    "g.node_label_artists[1].set_backgroundcolor('lightgreen')\n",
    "\n",
    "# Netgraph assumes circular node shapes when computing fontsizes.\n",
    "# We hence have to manually adjust the node label fontsizes\n",
    "# by the ratio of the diagonal to the width in a square.\n",
    "for node, label in g.node_label_artists.items():\n",
    "    fontsize = label.get_fontsize()\n",
    "    label.set_fontsize(fontsize * 1./np.sqrt(2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bypass Cross-Origin Resource Sharing (CORS) utk PyTorch Model di Static Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static_web/index.html\n"
     ]
    }
   ],
   "source": [
    "%%file static_web/index.html\n",
    "\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Data Test and Model Evaluation</title>\n",
    "  <style>\n",
    "    /* Basic styling for layout */\n",
    "    body {\n",
    "      font-family: Arial, sans-serif;\n",
    "      padding: 10px;\n",
    "      margin: 0;\n",
    "    }\n",
    "    h1 {\n",
    "      text-align: center;\n",
    "    }\n",
    "    .checkbox-container {\n",
    "      display: flex;\n",
    "      flex-wrap: wrap;\n",
    "      gap: 10px;\n",
    "    }\n",
    "    .checkbox-group {\n",
    "      flex: 1 1 200px;\n",
    "      min-width: 200px;\n",
    "      background-color: #f9f9f9;\n",
    "      padding: 10px;\n",
    "      border-radius: 5px;\n",
    "      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "    }\n",
    "    .checkbox-group label {\n",
    "      display: block;\n",
    "      margin: 5px 0;\n",
    "    }\n",
    "\n",
    "    /* Container and output styling */\n",
    "    /* Container and output styling */\n",
    "    .output-container {\n",
    "      display: flex;\n",
    "      flex-wrap: wrap; /* Enables wrapping */\n",
    "      gap: 15px;\n",
    "      margin-top: 20px;\n",
    "      padding: 10px;\n",
    "      border-radius: 8px;\n",
    "      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "      background-color: #f7f9fc;\n",
    "    }\n",
    "    .output-container pre {\n",
    "      flex: 1 1 250px; /* Allows elements to grow and shrink while maintaining a minimum width */\n",
    "      min-width: 200px;\n",
    "      padding: 15px;\n",
    "      background-color: #ffffff;\n",
    "      border: 1px solid #ddd;\n",
    "      border-radius: 5px;\n",
    "      overflow: auto;\n",
    "    }\n",
    "\n",
    "    /* Responsive styles */\n",
    "    @media (max-width: 768px) {\n",
    "      .output-container {\n",
    "        flex-direction: column;\n",
    "      }\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <h1>Data Test (Select Symptoms) and Model Evaluation</h1>\n",
    "\n",
    "  <!-- Symptom Checklist Container -->\n",
    "  <div class=\"checkbox-container\">\n",
    "    <div class=\"checkbox-group\">\n",
    "      <label><input type=\"checkbox\" id=\"checkbox1\" onclick=\"updateDataTest()\"> Migraine</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox1_1\" onclick=\"updateDataTest()\"> Sakit kepala berdenyut di satu sisi</label>\n",
    "    </div>\n",
    "    <div class=\"checkbox-group\">\n",
    "      <label><input type=\"checkbox\" id=\"checkbox2\" onclick=\"updateDataTest()\"> Vertigo</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox2_1\" onclick=\"updateDataTest()\"> Pusing berputar</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox2_2\" onclick=\"updateDataTest()\"> Kehilangan keseimbangan</label>\n",
    "    </div>\n",
    "    <div class=\"checkbox-group\">\n",
    "      <label><input type=\"checkbox\" id=\"checkbox3\" onclick=\"updateDataTest()\"> Sariawan (di mulut/lidah)</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox3_1\" onclick=\"updateDataTest()\"> Luka kecil pada mulut/lidah</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox3_2\" onclick=\"updateDataTest()\"> Nyeri di sekitar mulut/lidah/pipi/kepala</label>\n",
    "    </div>\n",
    "    <div class=\"checkbox-group\">\n",
    "      <label><input type=\"checkbox\" id=\"checkbox4\" onclick=\"updateDataTest()\"> Sakit Gigi</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox4_1\" onclick=\"updateDataTest()\"> Nyeri di gigi atau gusi</label>\n",
    "    </div>\n",
    "    <div class=\"checkbox-group\">\n",
    "      <label><input type=\"checkbox\" id=\"checkbox5\" onclick=\"updateDataTest()\"> Radang Tenggorokan</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox5_1\" onclick=\"updateDataTest()\"> Nyeri tenggorokan</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox5_2\" onclick=\"updateDataTest()\"> Batuk</label>\n",
    "      <label style=\"margin-left: 20px;\"><input type=\"checkbox\" id=\"checkbox5_3\" onclick=\"updateDataTest()\"> Suara serak</label>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <button id=\"runTestButton\" onclick=\"runTestData(data_test)\" disabled>Run Test Data</button>\n",
    "\n",
    "  <!-- Output Containers -->\n",
    "  <div class=\"output-container\">\n",
    "    <pre id=\"input_data_test\"></pre>\n",
    "    <pre id=\"output\"></pre>\n",
    "    <pre id=\"final_output\"></pre>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    const n_input = 14;\n",
    "    let data_test = new Array(n_input).fill(0);\n",
    "    let topk = 0;  // Initialize topk\n",
    "\n",
    "    function updateDataTest() {\n",
    "      data_test[0] = document.getElementById('checkbox1').checked ? 1 : 0;\n",
    "      data_test[1] = document.getElementById('checkbox1_1').checked ? 1 : 0;\n",
    "      data_test[2] = document.getElementById('checkbox2').checked ? 1 : 0;\n",
    "      data_test[3] = document.getElementById('checkbox2_1').checked ? 1 : 0;\n",
    "      data_test[4] = document.getElementById('checkbox2_2').checked ? 1 : 0;\n",
    "      data_test[5] = document.getElementById('checkbox3').checked ? 1 : 0;\n",
    "      data_test[6] = document.getElementById('checkbox3_1').checked ? 1 : 0;\n",
    "      data_test[7] = document.getElementById('checkbox3_2').checked ? 1 : 0;\n",
    "      data_test[8] = document.getElementById('checkbox4').checked ? 1 : 0;\n",
    "      data_test[9] = document.getElementById('checkbox4_1').checked ? 1 : 0;\n",
    "      data_test[10] = document.getElementById('checkbox5').checked ? 1 : 0;\n",
    "      data_test[11] = document.getElementById('checkbox5_1').checked ? 1 : 0;\n",
    "      data_test[12] = document.getElementById('checkbox5_2').checked ? 1 : 0;\n",
    "      data_test[13] = document.getElementById('checkbox5_3').checked ? 1 : 0;\n",
    "\n",
    "      document.getElementById('input_data_test').innerText = `data_test: ${JSON.stringify(data_test)}`;\n",
    "\n",
    "      // Enable or disable the button based on checked checkboxes\n",
    "      const hasCheckedCheckboxes = Array.from(document.querySelectorAll('input[type=\"checkbox\"]')).some(checkbox => checkbox.checked);\n",
    "      document.getElementById('runTestButton').disabled = !hasCheckedCheckboxes;\n",
    "\n",
    "      // Clear output if no checkboxes are checked\n",
    "      if (!hasCheckedCheckboxes) {\n",
    "        document.getElementById('input_data_test').innerText = '';\n",
    "        document.getElementById('output').innerText = '';\n",
    "        document.getElementById('final_output').innerText = '';\n",
    "      }\n",
    "\n",
    "      // Calculate topk based on checked groups\n",
    "      topk = 10;\n",
    "      if (document.getElementById('checkbox1').checked || document.getElementById('checkbox1_1').checked) topk += 1;\n",
    "      if (document.getElementById('checkbox2').checked || document.getElementById('checkbox2_1').checked || document.getElementById('checkbox2_2').checked) topk += 1;\n",
    "      if (document.getElementById('checkbox3').checked || document.getElementById('checkbox3_1').checked || document.getElementById('checkbox3_2').checked) topk += 1;\n",
    "      if (document.getElementById('checkbox4').checked || document.getElementById('checkbox4_1').checked) topk += 1;\n",
    "      if (document.getElementById('checkbox5').checked || document.getElementById('checkbox5_1').checked || document.getElementById('checkbox5_2').checked || document.getElementById('checkbox5_3').checked) topk += 1;\n",
    "\n",
    "    }\n",
    "  </script>\n",
    "\n",
    "  <script>\n",
    "\n",
    "    // Tiny dataset untuk Output column names\n",
    "    const output_column_names = [\n",
    "      'air kelapa', 'air putih', 'bawang putih', 'bayam',\n",
    "      'biji-bijian utuh dan kacang-kacangan', 'brokoli', 'bubur',\n",
    "      'bubur gandum dari biji food grade (oatmeal)', 'bubur kacang hijau',\n",
    "      'daging (sapi)', 'daging merah segar', 'delima', 'hati ayam', 'ikan',\n",
    "      'jahe', 'jeruk', 'jus blewah', 'jus delima', 'jus melon', 'jus pisang',\n",
    "      'jus tomat', 'kalkun', 'kentang', 'kubis', 'kuning telur', 'kunyit',\n",
    "      'labu', 'madu', 'pasta', 'peppermint', 'pisang', 'sayuran berdaun gelap',\n",
    "      'sikat kayu siwak', 'smoothie bowl (mix jus buah dan chia seed)',\n",
    "      'sup ayam', 'sup hangat', 'susu kedelai', 'teh chamomile', 'telur',\n",
    "      'tiram', 'tuna', 'wijen', 'wortel', 'yoghurt'\n",
    "    ];\n",
    "\n",
    "    // Model data\n",
    "    //const model_json = {\n",
    "    //  \"architecture\": { \"n_input\": 14, \"n_hidden1\": 100, \"n_hidden2\": 50, \"n_hidden3\": 25, \"n_output\": 44 },\n",
    "    //  \"state_dict\": {\n",
    "    //    \"hidden1.weight\": [[-0.271908700466156, -0.019576219841837883, /*...*/]],\n",
    "    //    \"hidden1.bias\": [/*...*/],\n",
    "    //    \"hidden2.weight\": [[0.08144842833280563, 0.07619614899158478, /*...*/]],\n",
    "    //    \"hidden2.bias\": [/*...*/],\n",
    "    //    \"hidden3.weight\": [[0.023470161482691765, 0.04224829375743866, /*...*/]],\n",
    "    //    \"hidden3.bias\": [/*...*/],\n",
    "    //    \"output.weight\": [[-0.018862934783101082, 0.12144787609577179, /*...*/]],\n",
    "    //    \"output.bias\": [/*...*/]\n",
    "    //  }\n",
    "    // };*/\n",
    "\n",
    "    const model_json = {\n",
    "      \"architecture\": { \"n_input\": 14, \"n_hidden1\": 100, \"n_hidden2\": 50, \"n_hidden3\": 25, \"n_output\": 44 },\n",
    "      \"state_dict\": {\"hidden1.weight\": [[0.13837428390979767, -0.11767198145389557, -0.05219254642724991, 0.12516281008720398, -0.2519550025463104, 0.15990032255649567, -0.05431799963116646, 0.13625885546207428, 0.03781823441386223, -0.03306996449828148, 0.0738464966416359, 0.012803610414266586, 0.09827572107315063, -0.10386066138744354], [-0.01934213563799858, -0.023282228037714958, 0.039258237928152084, -0.0012667762348428369, 0.23317408561706543, 0.08368942141532898, -0.09938620775938034, -0.16063685715198517, -0.044653624296188354, -0.1157374233007431, -0.0858425721526146, 0.013317259959876537, 0.15946538746356964, 0.14604489505290985], [-0.2608841359615326, 0.16643750667572021, 0.07522296160459518, 0.2537623643875122, 0.17661507427692413, -0.24295197427272797, -0.2537369132041931, -0.12815187871456146, 0.23506249487400055, -0.044310785830020905, 0.11462719738483429, -0.12363951653242111, 0.2626188397407532, -0.11232098937034607], [0.19992280006408691, 0.002855234546586871, -0.14137127995491028, 0.13695316016674042, -0.14293894171714783, 0.07802940160036087, -0.07767662405967712, -0.02961062267422676, -0.2574447989463806, -0.12849657237529755, 0.1446019560098648, -0.06553851813077927, 0.26570236682891846, 0.21392391622066498], [-0.012835195288062096, -0.17846471071243286, 0.16198164224624634, 0.08280233293771744, -0.1728040874004364, 0.17282775044441223, 0.1619337499141693, 0.2369602620601654, -0.15013688802719116, -0.044033948332071304, -0.005321442149579525, 0.0382651649415493, -0.20315048098564148, -0.18972691893577576], [0.14532572031021118, -0.06243293732404709, 0.13076724112033844, 0.015149110928177834, 0.08713795989751816, 0.05898497253656387, 0.09711040556430817, 0.1327219009399414, -0.24757948517799377, 0.13391046226024628, -0.18800503015518188, -0.20143376290798187, 0.016187692061066628, -0.045305561274290085], [0.1579645723104477, -0.154678076505661, -0.23854383826255798, 0.1941879540681839, -0.03980054333806038, 0.14936958253383636, 0.0869160071015358, -0.20031017065048218, 0.05468796566128731, 0.06402468681335449, -0.17926348745822906, -0.12776146829128265, 0.09212521463632584, 0.04797493666410446], [-0.1137857437133789, -0.08060578256845474, 0.24487324059009552, -0.049431364983320236, 0.150590181350708, 0.11581826955080032, -0.17285968363285065, -0.22698840498924255, 0.2564409077167511, 0.01386602595448494, 0.18321402370929718, 0.055478211492300034, 0.08585715293884277, 0.1999315321445465], [0.25303205847740173, -0.1769549548625946, 0.03304180130362511, 0.19924409687519073, 0.19351938366889954, 0.1656593382358551, -0.19381095468997955, -0.1920614242553711, -0.1620209515094757, 0.033481549471616745, 0.26616835594177246, -0.16919603943824768, 0.14202946424484253, -0.1475313901901245], [-0.2513220012187958, -0.05674433335661888, 0.15366168320178986, 0.24813008308410645, -0.16592662036418915, 0.05767221003770828, 0.23057028651237488, 0.17716090381145477, 0.16652455925941467, 0.1899775117635727, 0.16905859112739563, 0.06870613247156143, -0.1827974170446396, -0.22437560558319092], [-0.12148527055978775, -0.03095163218677044, -0.16439957916736603, 0.0971941277384758, 0.08186719566583633, -0.061098601669073105, 0.1036987230181694, 0.08654461055994034, 0.16415275633335114, 0.17913848161697388, -0.09109311550855637, 0.26053982973098755, -0.029918139800429344, -0.009012192487716675], [-0.25229352712631226, -0.172495037317276, -0.15600568056106567, -0.11396471410989761, 0.19087989628314972, -0.08720661699771881, -0.19975019991397858, 0.10234736651182175, 0.08553799986839294, 0.17396791279315948, -0.13795119524002075, 0.05807551369071007, -0.09732753038406372, -0.06052820757031441], [-0.21256691217422485, -0.1219879686832428, -0.08247718214988708, 0.11405505985021591, 0.04911072552204132, 0.06539155542850494, 0.26717332005500793, 0.2602993845939636, 0.18270325660705566, 0.008795451372861862, -0.18512432277202606, 0.20824088156223297, -0.0664137676358223, -0.021787572652101517], [-0.22999058663845062, -0.05332902818918228, -0.1723218858242035, 0.24560502171516418, -0.23085547983646393, -0.20826716721057892, -0.008820493705570698, -0.1450495719909668, 0.09588389843702316, -0.10267821699380875, -0.1255122423171997, 0.015195395797491074, 0.19372454285621643, -0.1885351985692978], [0.1250935047864914, 0.17169339954853058, 0.26177018880844116, -0.18700791895389557, 0.06474549323320389, -0.19728411734104156, 0.22776027023792267, -0.10367856174707413, 0.16057147085666656, 0.007974271662533283, -0.020758522674441338, -0.008191708475351334, 0.04501885548233986, 0.12597961723804474], [0.043013691902160645, 0.08198274672031403, -0.2405625283718109, 0.19477128982543945, 0.23319168388843536, 0.22076524794101715, 0.1977159082889557, -0.19241072237491608, -0.09892337769269943, 0.2358272224664688, -0.20350033044815063, 0.24231059849262238, -0.2100495547056198, -0.18780770897865295], [0.13092836737632751, -0.19227342307567596, -0.06071873754262924, 0.19458281993865967, 0.21136322617530823, 0.2532748579978943, -0.053961750119924545, -0.20798082649707794, 0.263418585062027, -0.05726504698395729, -0.10980824381113052, 0.06566229462623596, -0.18663211166858673, 0.17541638016700745], [0.16837772727012634, -0.21155902743339539, -0.21947355568408966, -0.02364501543343067, 0.11234655231237411, -0.0076930224895477295, -0.13464581966400146, 0.006617977283895016, -0.2503310441970825, -0.18879978358745575, -0.1781468242406845, 0.22014516592025757, 0.23646870255470276, -0.09027203172445297], [0.012597125954926014, 0.11843187361955643, 0.03165009245276451, -0.13438312709331512, 0.1532757431268692, 0.16392222046852112, -0.014878886751830578, -0.03366125747561455, 0.24348273873329163, 0.23275665938854218, -0.1520826816558838, 0.26111263036727905, 0.0667501837015152, -0.1782919019460678], [0.14726249873638153, -0.19853360950946808, 0.24659249186515808, -0.17170581221580505, 0.07607471197843552, 0.0810432955622673, 0.06453058123588562, 0.22266922891139984, -0.11004643142223358, -0.11234397441148758, -0.2314900904893875, -0.011306141503155231, -0.12564677000045776, 0.011705761775374413], [-0.05463156849145889, 0.08930034935474396, 0.2320549339056015, 0.07912332564592361, -0.08623412251472473, -0.13029049336910248, -0.23293611407279968, 0.2255108654499054, -0.0504734143614769, 0.05380949005484581, 0.010146666318178177, -0.16946321725845337, 0.15067315101623535, 0.15040293335914612], [0.013617308810353279, -0.15182319283485413, -0.11415719985961914, 0.0035764279309660196, -0.15268002450466156, -0.14971676468849182, -0.03970852494239807, -0.07689958810806274, 0.06576554477214813, -0.07023218274116516, -0.03553302586078644, -0.10260482132434845, -0.24408148229122162, 0.2331441342830658], [0.05044421926140785, 0.2394186109304428, -0.043605171144008636, -0.0668550655245781, -0.2507559359073639, -0.10198826342821121, 0.17471793293952942, -0.2613407075405121, -0.09402322769165039, 0.1633293479681015, 0.15434588491916656, 0.09815191477537155, -0.05344754084944725, -0.019808776676654816], [-0.05009331554174423, -0.09803665429353714, 0.11229581385850906, -0.09112653881311417, -0.15045931935310364, 0.06139213219285011, -0.19251659512519836, -0.05655085667967796, 0.13723555207252502, 0.09956622868776321, 0.22743308544158936, -0.18152360618114471, 0.00298514636233449, -0.07843633741140366], [-0.14950041472911835, -0.1504518836736679, 0.0676816999912262, 0.027667367830872536, -0.13660353422164917, -0.18803265690803528, -0.22145019471645355, 0.26183021068573, -0.045570217072963715, -0.061980679631233215, -0.09857150912284851, -0.1333259791135788, -0.24443037807941437, -0.15700124204158783], [0.16325116157531738, 0.05878037214279175, -0.0942402109503746, -0.10070231556892395, 0.1711234450340271, 0.1926923394203186, 0.2573821246623993, 0.07816476374864578, 0.1290103793144226, -0.02108071930706501, -0.1559513956308365, -0.26179245114326477, 0.16268077492713928, -0.03864242136478424], [0.11138704419136047, 0.013325328007340431, -0.24954478442668915, -0.13127119839191437, 0.07521285861730576, -0.05419804900884628, -0.2565002739429474, 0.09388795495033264, 0.011833807453513145, -0.08586893230676651, 0.10781171172857285, 0.05631982535123825, -0.09698288142681122, 0.2402639389038086], [0.1338856965303421, 0.03340783715248108, -0.05108371376991272, -0.22682182490825653, 0.20609226822853088, -0.06871995329856873, -0.172904834151268, 0.1712048500776291, 0.09472431242465973, 0.06588472425937653, -0.05343358963727951, -0.08316010236740112, 0.04516002535820007, 0.2066696435213089], [0.03821934014558792, -0.12595978379249573, 0.2654314637184143, 0.006101564504206181, 0.21169202029705048, 0.06507419049739838, 0.239507794380188, 0.15461687743663788, -0.13631059229373932, 0.1881563812494278, 0.0021049538627266884, -0.2145978808403015, -0.05605708807706833, -0.2408190816640854], [-0.13802674412727356, -0.18827933073043823, 0.18238922953605652, -0.05581328272819519, -0.04587600752711296, 0.12767115235328674, 0.10920937359333038, -0.17532570660114288, 0.09893693029880524, 0.11383916437625885, -0.21216678619384766, 0.23196226358413696, -0.16218149662017822, -0.1034545823931694], [-0.22069132328033447, 0.24417757987976074, 0.05025670304894447, 0.22523222863674164, 0.01798652857542038, 0.14463365077972412, -0.08588472753763199, -0.21330781280994415, 0.04235072061419487, -0.02329983562231064, -0.02679615281522274, -0.13189049065113068, 0.004980695899575949, 0.07665976881980896], [0.09599696099758148, -0.21601954102516174, 0.12937100231647491, 0.027285592630505562, 0.1834818720817566, 0.18912892043590546, -0.21088913083076477, 0.25593286752700806, -0.26325523853302, 0.15278643369674683, 0.018613295629620552, 0.126722052693367, -0.14494286477565765, 0.1599530726671219], [-0.13181762397289276, -0.23571118712425232, 0.0896243080496788, 0.1463063806295395, 0.2647385001182556, -0.027849266305565834, 0.2575315833091736, 0.1722186654806137, -0.13211998343467712, -0.20633728802204132, 0.14991553127765656, -0.14634306728839874, 0.12302332371473312, -0.2283724546432495], [0.19481144845485687, 0.07984109967947006, -0.030810385942459106, 0.12245678156614304, -0.19061027467250824, -0.0585104338824749, 0.002445748308673501, -0.15469254553318024, -0.06846548616886139, -0.11559225618839264, 0.2545928955078125, 0.22696265578269958, 0.23033328354358673, 0.21818046271800995], [-0.2566881775856018, 0.08906914293766022, 0.09839114546775818, -0.19573283195495605, 0.10811521857976913, -0.14513874053955078, 0.04176880419254303, 0.08819438517093658, 0.2298882156610489, -0.14042583107948303, -0.20997685194015503, 0.24716737866401672, 0.11145748943090439, 0.12957119941711426], [0.09646586328744888, -0.09403588622808456, 0.1605851948261261, -0.233408585190773, 0.03719001263380051, 0.1206272542476654, 0.18286612629890442, -0.13289834558963776, -0.1803777813911438, 0.10017415881156921, -0.2123296558856964, -0.22229638695716858, -0.24009162187576294, -0.12904013693332672], [-0.05136646702885628, -0.158919557929039, -0.11310310661792755, -0.2369067519903183, 0.1642056256532669, -0.061282142996788025, -0.1689794957637787, 0.06893359869718552, -0.2001855969429016, 0.18701548874378204, 0.15342895686626434, -0.26273879408836365, -0.21013790369033813, -0.1768750101327896], [-0.14582189917564392, 0.03395693749189377, -0.26428502798080444, -0.19809506833553314, 0.12233159691095352, -0.08753335475921631, -0.1074943020939827, -0.22051401436328888, 0.10717897862195969, 0.019150326028466225, -0.10326097905635834, -0.16366839408874512, 0.055694010108709335, 0.07330577820539474], [0.1180337518453598, 0.07963873445987701, -0.04120350629091263, -0.20677949488162994, 0.17024187743663788, -0.0027250375133007765, -0.1782114952802658, 0.23069743812084198, 0.14841784536838531, -0.20994733273983002, -0.13336507976055145, -0.09138420224189758, -0.15487851202487946, 0.1250215619802475], [-0.19062259793281555, 0.24807105958461761, -0.11100666970014572, 0.15759512782096863, 0.00889289565384388, -0.11807752400636673, 0.17851249873638153, -0.20424257218837738, -0.14136305451393127, 0.03182539716362953, 0.21184897422790527, -0.11504792422056198, -0.16270355880260468, -0.17093707621097565], [-0.1174788624048233, -0.09234325587749481, -0.06209945306181908, -0.1524871438741684, 0.0829446092247963, 0.002321934560313821, -0.17426949739456177, -0.15264536440372467, 0.0569436140358448, -0.004404502455145121, 0.18870753049850464, -0.040364380925893784, -0.21615828573703766, -0.1976916790008545], [-0.07874840497970581, -0.0590222030878067, 0.03036634810268879, -0.05968165025115013, 0.0990876853466034, -0.0890209749341011, 0.19585691392421722, 0.06155657768249512, -0.08246621489524841, 0.23865805566310883, 0.06848733872175217, -0.18176227807998657, -0.14904306828975677, 0.16774681210517883], [0.12184279412031174, 0.2083667814731598, -0.02105102129280567, 0.18698067963123322, 0.13323339819908142, 0.0821312889456749, -0.06207755580544472, 0.25732752680778503, 0.05386294052004814, -0.06860020756721497, -0.003747170092537999, 0.26220962405204773, 0.1792852133512497, -0.02011493779718876], [0.26221150159835815, 0.1175607219338417, -0.1428755521774292, -0.24337054789066315, 0.15480011701583862, 0.25006523728370667, -0.15551529824733734, 0.10996714979410172, 0.011575418524444103, -0.05766080319881439, -0.07281547784805298, 0.07377137988805771, 0.0591171570122242, 0.1807073950767517], [0.09784217178821564, 0.12070856243371964, 0.23562082648277283, 0.08289963752031326, 0.03915800526738167, -0.09744586050510406, 0.19601131975650787, 0.03855888172984123, 0.17227290570735931, 0.2575649321079254, -0.06009535491466522, 0.1880333125591278, -0.05427083745598793, 0.24027390778064728], [0.21073448657989502, -0.09591058641672134, 0.009389938786625862, -0.11685489118099213, -0.10865552723407745, -0.054306790232658386, -0.0014477050863206387, -0.09328006953001022, -0.2519197165966034, -0.1622094213962555, -0.02297833189368248, -0.08551297336816788, 0.19888409972190857, 0.21713048219680786], [0.1710047572851181, 0.11861943453550339, -0.2121943235397339, 0.2525218725204468, -0.23518900573253632, -0.08818333595991135, -0.18735995888710022, 0.09644170850515366, -0.22164960205554962, -0.13143400847911835, -0.16974444687366486, -0.16460105776786804, -0.12401985377073288, 0.2630572021007538], [-0.020850611850619316, 0.0827794075012207, -0.2653738260269165, 0.23770225048065186, -0.05802381411194801, -0.24534521996974945, 0.18516437709331512, -0.07095520198345184, 0.24368725717067719, -0.18135841190814972, -0.2480577826499939, -0.20084887742996216, 0.116750568151474, -0.02866191789507866], [-0.2491232305765152, 0.04999842122197151, -0.017423199489712715, 0.0972556322813034, -0.13525834679603577, -0.1911822110414505, 0.1806335747241974, -0.12816263735294342, 0.18647357821464539, 0.2173890471458435, -0.186202272772789, -0.12360566854476929, 0.03064577281475067, -0.2515478730201721], [0.08283493667840958, -0.11094198375940323, -0.1180860847234726, -0.20642957091331482, 0.12551330029964447, -0.1698998361825943, -0.21286547183990479, -0.0698675587773323, -0.1480279117822647, -0.2593472898006439, 0.10789132118225098, -0.10127215087413788, 0.18796421587467194, 0.1885453760623932], [-0.17044316232204437, -0.15550319850444794, -0.09507827460765839, 0.15640169382095337, -0.010050272569060326, -0.09568969905376434, 0.1655714064836502, 0.2537589371204376, 0.1975938230752945, -0.20522649586200714, -0.05565791204571724, -0.05147597938776016, 0.22808967530727386, -0.13792698085308075], [-0.1546432226896286, 0.002616616664454341, -0.0626169741153717, 0.10107421875, 0.03212343528866768, -0.16010171175003052, 0.19287095963954926, 0.16479948163032532, 0.057058874517679214, -0.24226731061935425, 0.12550236284732819, 0.1770525574684143, 0.07293808460235596, -0.10487291216850281], [0.01833547279238701, -0.10270956158638, 0.251414954662323, 0.2578361928462982, -0.031665053218603134, -0.18001185357570648, 0.15762825310230255, 0.13636672496795654, 0.14207996428012848, -0.201621413230896, 0.09038910269737244, -0.11425301432609558, -0.23449882864952087, -0.1866610050201416], [0.1134970411658287, 0.10047261416912079, -0.2350732535123825, 0.1516246348619461, 0.20494914054870605, 0.20237858593463898, 0.15111033618450165, -0.041538700461387634, 0.17928583920001984, 0.20939630270004272, 0.13938535749912262, -0.19273464381694794, -0.09307283908128738, 0.21214500069618225], [0.20463012158870697, 0.17160426080226898, -0.11202835291624069, -0.05295285955071449, -0.19925595819950104, -0.15263622999191284, 0.20603527128696442, -0.21829186379909515, -0.065170519053936, 0.056587450206279755, -0.16038228571414948, 0.06197499856352806, -0.042086947709321976, 0.10795458406209946], [-0.20457524061203003, 0.03747781738638878, -0.24371720850467682, 0.1660812348127365, 0.2598147988319397, -0.0954398438334465, 0.04003659263253212, -0.13710106909275055, -0.17044641077518463, -0.13515080511569977, -0.127048060297966, 0.05068368837237358, -0.030118755996227264, 0.01933404430747032], [-0.20486201345920563, -0.16795134544372559, -0.23154890537261963, -0.19235946238040924, 0.2233504354953766, 0.09911878407001495, -0.25493818521499634, -0.044648315757513046, 0.02139783464372158, 0.14231890439987183, 0.2476058155298233, 0.06436173617839813, 0.1882166713476181, -0.0951446071267128], [-0.008594447746872902, -0.16162733733654022, 0.062345873564481735, 0.24630768597126007, 0.16665303707122803, 0.2667234539985657, -0.12028685957193375, -0.033731091767549515, 0.055266983807086945, 0.21368935704231262, -0.24606679379940033, -0.1304781138896942, 0.03890714794397354, -0.1547686755657196], [0.09922618418931961, -0.018821651116013527, -0.0690435841679573, 0.14331278204917908, 0.19093111157417297, 0.2601732313632965, 0.12041480094194412, -0.007909993641078472, 0.21790248155593872, 0.21262499690055847, -0.13396671414375305, 0.12194813042879105, 0.03070482611656189, -0.11610396206378937], [-0.06363027542829514, -0.06265918910503387, -0.18648789823055267, -0.08182375878095627, 0.0892506018280983, 0.19673559069633484, 0.02031821943819523, -0.2583661377429962, 0.1415618658065796, -0.08155397325754166, 0.23841048777103424, -0.25771474838256836, -0.06251326948404312, 0.20771703124046326], [0.11501974612474442, -0.1328951120376587, -0.2315734177827835, -0.002627095440402627, 0.10353974252939224, -0.22202949225902557, -0.22424279153347015, -0.0029206660110503435, -0.05540968105196953, 0.1720735877752304, -0.2587151825428009, -0.042659223079681396, -0.20770610868930817, -0.0384826622903347], [-0.13634851574897766, 0.0842759981751442, -0.2233446091413498, -0.21007318794727325, -0.1511831283569336, -0.23384316265583038, 0.16891473531723022, -0.03808462992310524, -0.1482853889465332, 0.18776275217533112, -0.0604645311832428, 0.13845910131931305, -0.08264681696891785, -0.22344055771827698], [0.05861813947558403, 0.16587336361408234, -0.26124823093414307, 0.02783520705997944, 0.05346909910440445, 0.0300150066614151, -0.006693356204777956, -0.0836247056722641, -0.20756421983242035, 0.04379267245531082, -0.1794823855161667, -0.2214839607477188, -0.08588474988937378, -0.18350322544574738], [0.0805453509092331, 0.1907535344362259, 0.20789547264575958, -0.16511847078800201, 0.1459328830242157, 0.19010750949382782, 0.001627763151191175, 0.11313743144273758, 0.14985716342926025, 0.20471736788749695, 0.14354021847248077, 0.1142672747373581, -0.10280907154083252, -0.14862532913684845], [-0.1269625425338745, 0.22058793902397156, -0.06485792249441147, 0.009561756625771523, -0.24890463054180145, -0.01199719961732626, -0.0756256952881813, -0.17175734043121338, 0.20675264298915863, -0.2160385698080063, -0.0382021889090538, -0.019075309857726097, -0.24218393862247467, 0.16227972507476807], [-0.085239939391613, 0.11941149830818176, 0.14939217269420624, 0.23581770062446594, 0.06609802693128586, 0.20087118446826935, 0.24662767350673676, 0.05231471359729767, -0.054656241089105606, -0.07318504899740219, 0.043448492884635925, 0.09640823304653168, -0.25233009457588196, 0.19959205389022827], [0.041690222918987274, -0.249274343252182, 0.08526064455509186, 0.16969098150730133, 0.09966272115707397, 0.21781271696090698, 0.00507019367069006, -0.22203554213047028, -0.0010613066842779517, 0.19405747950077057, -0.1284652203321457, 0.11364968121051788, 0.06115248054265976, -0.04367577284574509], [0.001464412547647953, 0.11415834724903107, -0.1827627271413803, 0.16876371204853058, 0.1808491051197052, 0.15009863674640656, 0.26663196086883545, -0.011218409985303879, -0.18915846943855286, 0.012116732075810432, -0.07751858979463577, 0.09447264671325684, 0.22563868761062622, -0.14582912623882294], [0.011349901556968689, 0.2603473961353302, -0.21204844117164612, -0.04589080065488815, 0.25972986221313477, 0.18805082142353058, 0.07772822678089142, -0.11487054824829102, -0.0247555673122406, -0.23945631086826324, -0.25138941407203674, -0.24910087883472443, 0.25685518980026245, 0.2117844671010971], [0.10617343336343765, -0.06749019026756287, 0.22945737838745117, -0.07086296379566193, 0.18481245636940002, -0.009350834414362907, -0.12556369602680206, -0.07495282590389252, -0.05690915882587433, -0.24536196887493134, 0.0796540379524231, -0.08208158612251282, 0.12294826656579971, 0.17004060745239258], [-0.043306466192007065, 0.1296357810497284, 0.05440928786993027, 0.17976368963718414, -0.17271272838115692, -0.02245270647108555, -0.08987411111593246, 0.2342023253440857, -0.24070636928081512, -0.15057404339313507, 0.030450407415628433, 0.23826898634433746, 0.18001693487167358, -0.2500723898410797], [-0.18794555962085724, 0.2595219016075134, 0.03957781568169594, -0.22869627177715302, -0.0844726487994194, 0.11747105419635773, -0.05592983961105347, -0.2049080729484558, -0.16505490243434906, -0.1387225091457367, -0.010253839194774628, 0.10536570847034454, 0.12827618420124054, -0.12945032119750977], [-0.04424373805522919, -0.2060202807188034, 0.20271508395671844, 0.26666948199272156, 0.2600924074649811, 0.10982359200716019, -0.056390970945358276, -0.006334042642265558, 0.09702260047197342, -0.2313428819179535, 0.016545670107007027, -0.2166440337896347, -0.2666160464286804, -0.008604475297033787], [-0.08476318418979645, -0.1696121096611023, 0.04370628297328949, 0.010493765585124493, 0.040620625019073486, -0.1501815766096115, -0.2614595890045166, 0.005536437965929508, -0.10871067643165588, -0.09026703983545303, 0.19636058807373047, 0.04304985702037811, 0.12238197028636932, -0.07500476390123367], [-0.21197813749313354, 0.05277884006500244, -0.17684172093868256, 0.033263783901929855, -0.20761261880397797, 0.20163707435131073, -0.18197205662727356, 0.23151281476020813, 0.1888841986656189, 0.20299822092056274, 0.16051742434501648, 0.06201133131980896, -0.012935902923345566, -0.004778233356773853], [-0.10645243525505066, -0.24597959220409393, 0.2441612035036087, 0.013405406847596169, 0.12477018684148788, 0.010368676856160164, 0.07664649188518524, -0.16498184204101562, -0.20791657269001007, 0.2028968632221222, 0.10090773552656174, 0.13252313435077667, -0.2654285728931427, 0.01775498501956463], [-0.24956132471561432, -0.030457595363259315, -0.013649718835949898, -0.18794994056224823, -0.26455816626548767, 0.0022378989960998297, 0.24007540941238403, 0.24098943173885345, -0.16298678517341614, -0.0473860464990139, -0.22342173755168915, -0.20823979377746582, -0.07355181872844696, 0.2595999240875244], [-0.06995625048875809, 0.014137170277535915, 0.2514630854129791, 0.04746066406369209, 0.21544253826141357, 0.1776258796453476, 0.2503819167613983, 0.04450659453868866, -0.21636123955249786, -0.23007163405418396, -0.13825517892837524, 0.2206241637468338, 0.153452068567276, 0.07689496129751205], [-0.20386359095573425, -0.19635185599327087, 0.21805982291698456, -0.02776849828660488, -0.26631975173950195, -0.03302440792322159, -0.06449902057647705, 0.004518701694905758, 0.11151474714279175, 0.10262306779623032, -0.20452837646007538, -0.002914326498284936, -0.1327837109565735, 0.18127036094665527], [0.13188879191875458, 0.14628008008003235, 0.2432814985513687, 0.13131392002105713, -0.2668580412864685, 0.06774070113897324, -0.1838451325893402, 0.03527563065290451, 0.0024023824371397495, 0.265289306640625, -0.18256434798240662, -0.2615495026111603, -0.07210148125886917, 0.042652957141399384], [0.1954904943704605, 0.2169020026922226, 0.20700842142105103, 0.08464974164962769, -0.07589025050401688, -0.05533229187130928, -0.1464480608701706, 0.05246920883655548, -0.18615446984767914, 0.09237385541200638, 0.17288269102573395, 0.22796735167503357, -0.2637194097042084, 0.21948592364788055], [-0.16797614097595215, -0.021890318021178246, 0.24431629478931427, 0.1449994444847107, -0.08488411456346512, 0.1581123024225235, -0.26003268361091614, 0.20887112617492676, 0.2593507766723633, -0.012244882993400097, 0.04415486007928848, -0.06237227842211723, -0.16927507519721985, -0.12095411866903305], [-0.01838676817715168, -0.21857163310050964, -0.21541482210159302, 0.029303161427378654, -0.1740717589855194, -0.14478619396686554, -0.002949346788227558, -0.14076882600784302, 0.24675197899341583, -0.09665283560752869, -0.14863333106040955, -0.07061071693897247, -0.21868941187858582, 0.13776041567325592], [0.2295423299074173, -0.15390649437904358, -0.09959827363491058, 0.24948343634605408, -0.19315624237060547, 0.0721394270658493, -0.21784242987632751, -0.23764856159687042, 0.00031707179732620716, -0.1523570567369461, 0.1448020339012146, -0.08291091024875641, 0.002545957686379552, -0.1393052041530609], [0.10957113653421402, 0.24740929901599884, -0.03992270678281784, -0.16467100381851196, -0.260184645652771, 0.2563612461090088, 0.11238855868577957, 0.11016087234020233, 0.045246534049510956, 0.13458925485610962, 0.09519263356924057, 0.13075177371501923, 0.16689351201057434, 0.03768331557512283], [0.22886909544467926, 0.1654987633228302, -0.007691753096878529, -0.11938679963350296, 0.018220236524939537, 0.20863574743270874, 0.16797105967998505, -0.10899046063423157, -0.057129692286252975, 0.10661681741476059, 0.10068362206220627, 0.2549866735935211, 0.12844540178775787, 0.08445601165294647], [0.061967991292476654, -0.2521235942840576, -0.09907267987728119, 0.028215935453772545, -0.18611250817775726, -0.2347746193408966, 0.1805422604084015, 0.08994525671005249, -0.2154717594385147, 0.01567358523607254, 0.15732626616954803, 0.1533534973859787, 0.24426418542861938, -0.10662776231765747], [0.11906851083040237, -0.2175125777721405, -0.03296700865030289, -0.17013536393642426, -0.05056113749742508, 0.15214698016643524, 0.1333010196685791, -0.1412983387708664, -0.06620769947767258, -0.0531373992562294, -0.10183172672986984, -0.1464046835899353, 0.19631412625312805, 0.18402381241321564], [0.2234560251235962, -0.14645375311374664, 0.19541680812835693, -0.08031302690505981, -0.06160438433289528, -0.07225075364112854, -0.012656616978347301, -0.11673082411289215, 0.036889031529426575, -0.17107383906841278, 0.11065022647380829, -0.2244262993335724, -0.09654948860406876, -0.18636763095855713], [0.18046873807907104, 0.1844581961631775, -0.21234619617462158, -0.14566251635551453, 0.2391851842403412, 0.07809914648532867, 0.1842121183872223, -0.2040608525276184, 0.12080413848161697, 0.17011329531669617, -0.06183576211333275, -0.012094087898731232, 0.1950637400150299, 0.07371355593204498], [0.07684957981109619, 0.0016147169517353177, -0.11094506829977036, -0.16363105177879333, 0.2666237950325012, -0.020480234175920486, 0.23733919858932495, 0.21947452425956726, -0.13959014415740967, 0.01808367297053337, -0.06840803474187851, -0.00421978859230876, 0.17630502581596375, 0.14796969294548035], [-0.15134643018245697, -0.1318134367465973, -0.19065023958683014, -0.2267654687166214, 0.17173057794570923, -0.16593267023563385, 0.003947873134166002, 6.721652607666329e-05, 0.04617633298039436, -0.01535299327224493, 0.13884136080741882, 0.03591690957546234, 0.1921047568321228, 0.05812109634280205], [-0.003431125544011593, 0.0375642366707325, 0.17848306894302368, 0.04022570326924324, 0.188442200422287, -0.024991163983941078, 0.0022196630015969276, 0.22423085570335388, -0.25241658091545105, 0.026824457570910454, 0.24399486184120178, -0.13376262784004211, -0.04939615726470947, -0.01640911214053631], [0.0888606384396553, 0.10093337297439575, 0.06110768765211105, 0.10516058653593063, -0.11475401371717453, 0.11300157755613327, -0.08393155783414841, 0.16784785687923431, -0.07762516289949417, -0.22765924036502838, 0.018819892778992653, 0.13963131606578827, -0.009662294760346413, 0.0033870816696435213], [0.0018762403633445501, 0.14213933050632477, 0.021088236942887306, -0.23842361569404602, -0.18980014324188232, 0.015974367037415504, -0.0205311868339777, 0.2557585537433624, -0.06189775839447975, 0.20533831417560577, -0.2250678837299347, -0.25383248925209045, -0.023656049743294716, -0.1689290553331375], [-0.07796116173267365, 0.030413061380386353, -0.231207475066185, 0.18408840894699097, -0.16200487315654755, 0.205683171749115, 0.08734022080898285, -0.04981284588575363, -0.23391486704349518, 0.04442930966615677, -0.14887362718582153, 0.23160460591316223, -0.2248552143573761, -0.07028890401124954], [-0.14750027656555176, -0.1715186983346939, -0.13914623856544495, -0.026100028306245804, 0.0184926800429821, 0.04552163556218147, 0.13636288046836853, -0.10879281908273697, -0.16603127121925354, -0.05589907243847847, 0.25225311517715454, -0.03240283951163292, -0.22520452737808228, 0.2100813239812851], [0.06519221514463425, 0.04659537598490715, 0.04603096470236778, 0.186180979013443, -0.12234362959861755, 0.19494055211544037, 0.2571607232093811, -0.0687040463089943, 0.16883987188339233, 0.14680498838424683, 0.24999934434890747, -0.06204601004719734, -0.1588214933872223, -0.09899773448705673], [-0.18164868652820587, 0.009081460535526276, 0.2580607831478119, 0.2593230605125427, 0.20223817229270935, 0.026828713715076447, 0.005983863957226276, 0.17687174677848816, 0.1588774174451828, -0.02616054192185402, -0.0675516352057457, 0.10606124252080917, -0.009091580286622047, -0.18784868717193604], [-0.12540902197360992, 0.21596074104309082, 0.22249314188957214, -0.10421513020992279, -0.05790175497531891, -0.1325429528951645, -0.0869544968008995, 0.2428390383720398, -0.23740385472774506, 0.12698517739772797, -0.2402849644422531, -0.12759053707122803, -0.05412478372454643, 0.017733873799443245]], \"hidden1.bias\": [-0.04345453903079033, -0.12138228863477707, -0.11705727875232697, -0.16543805599212646, -0.18789876997470856, -0.16682355105876923, 0.2559424936771393, -0.1257060170173645, -0.2229149043560028, 0.18344783782958984, 0.10282304883003235, 0.18467095494270325, -0.2661823630332947, -0.25441089272499084, -0.1896979808807373, -0.1401071399450302, -0.00023545532894786447, 0.11689375340938568, 0.15453298389911652, -0.22765664756298065, 0.10834365338087082, -0.15502691268920898, 0.061591316014528275, 0.08540670573711395, 0.042524151504039764, 0.09538580477237701, -0.01182554941624403, 0.1756477952003479, 0.0568414144217968, 0.23587645590305328, 0.057377103716135025, -0.14342090487480164, 0.17250119149684906, 0.013506308197975159, 0.2571939527988434, -0.029000209644436836, 0.2630174458026886, -0.1562567800283432, 0.13371846079826355, -0.2005610466003418, 0.18293119966983795, 0.048981014639139175, 0.1510927528142929, -0.23100613057613373, -0.2677466869354248, 0.1281389445066452, -0.1088496521115303, 0.10831867903470993, -0.1586397886276245, -0.08037099987268448, 0.00029802406788803637, 0.15692955255508423, 0.1503448188304901, -0.023531535640358925, -0.0741378515958786, -0.0030924531165510416, 0.13675321638584137, -0.2274690866470337, -0.08276180922985077, -0.257589727640152, 0.038811299949884415, -0.030357662588357925, 0.15032976865768433, 0.21346676349639893, -0.2278423011302948, -0.11876591295003891, -0.04419587552547455, -0.002588742645457387, -0.05871197208762169, -0.16988995671272278, -0.10312739759683609, -0.005972260143607855, 0.21672986447811127, -0.06926105171442032, 0.17733952403068542, -0.21732203662395477, 0.012449472211301327, -0.21041159331798553, -0.2508644163608551, 0.026168348267674446, -0.12051168084144592, 0.1970100700855255, 0.21092945337295532, -0.2658819854259491, -0.01807413063943386, -0.046703778207302094, -0.1539531797170639, -0.18706609308719635, -0.11083727329969406, 0.11940329521894455, 0.06501533836126328, 0.16138477623462677, -0.01594259776175022, 0.13975079357624054, 0.05421541631221771, -0.056741274893283844, -0.014401172287762165, -0.24347734451293945, -0.005255670752376318, -0.08898942172527313], \"hidden2.weight\": [[-0.033772461116313934, -0.04003506898880005, -0.02156350389122963, -0.07434678077697754, 0.006326985079795122, -0.004108967259526253, -0.026624692603945732, 0.08736813813447952, 0.06795837730169296, 0.029033862054347992, -0.03669539839029312, 0.013615734875202179, -0.06913203746080399, 0.06929218769073486, 0.09224060922861099, -0.014026009477674961, 0.077492855489254, 0.09382019191980362, -0.0059025222435593605, -0.037335142493247986, 0.005080911796540022, 0.016241634264588356, -0.09267819672822952, -0.005788397043943405, -0.03563755750656128, 0.06556740403175354, -0.002993106609210372, 0.06529045104980469, 0.08949724584817886, -0.0305853933095932, 0.02210969291627407, -0.007323909550905228, -0.043201811611652374, -0.0559854656457901, 0.0017787476535886526, 0.03064516931772232, -0.01640724577009678, 0.016665825620293617, 0.061896517872810364, 0.04227994382381439, 0.042396899312734604, 0.028341177850961685, -0.07705084979534149, -0.06120838597416878, -0.07463882118463516, 0.06499230861663818, -0.08855700492858887, -0.07609393447637558, 0.06552958488464355, 0.0586896575987339, -0.09545397758483887, 0.057496074587106705, -0.022738153114914894, 0.02131291665136814, -0.06370214372873306, -0.014616589993238449, 0.023269038647413254, 0.07697762548923492, 0.06066257879137993, -0.05449237674474716, -0.06520860642194748, -0.006115598138421774, 0.03151477128267288, 0.055258411914110184, -0.10007897019386292, -0.007676522713154554, 0.004364269319921732, -0.04680153355002403, 0.07761698961257935, -0.02666197344660759, -0.09508709609508514, -0.05947358161211014, 0.0042417533695697784, -0.07368065416812897, 0.02331182360649109, 0.09883122146129608, 0.0032775599975138903, -0.0246211439371109, 0.09428350627422333, 0.07384944707155228, 0.06716734170913696, 0.008467203937470913, 0.004086323082447052, -0.07740672677755356, 0.009294266812503338, 0.09688489139080048, 0.00484022032469511, -0.09016925096511841, 0.0546087808907032, 0.08897025883197784, 0.08304158598184586, 0.004532188177108765, 0.0195718165487051, 0.016173051670193672, -0.041540756821632385, -0.013824893161654472, 0.08230514824390411, 0.010228510946035385, 0.09495659172534943, -0.06635337322950363], [-0.014472760260105133, 0.0010148731525987387, -0.05899537354707718, 0.07763199508190155, 0.06928391009569168, -0.05389206483960152, 0.08864355832338333, 0.09282592684030533, -0.05851307883858681, -0.003822915256023407, 0.09963903576135635, -0.06524158269166946, -0.015689143911004066, -0.06791751086711884, 0.07259827107191086, 0.015924375504255295, 0.04147440940141678, 0.07174374908208847, -0.016305506229400635, -0.10040374845266342, 0.004701540805399418, -0.00738142104819417, 0.036413371562957764, 0.046635501086711884, -0.04995458573102951, -0.03874639794230461, 0.0630943775177002, 0.028603389859199524, 0.02281980961561203, 0.04375339299440384, 0.04616173356771469, -0.07327854633331299, 0.00824720785021782, 0.009766836650669575, -0.09700945019721985, 0.020976584404706955, -0.0014191679656505585, 0.07811947166919708, -0.0955457091331482, 0.09443177282810211, -0.04921770095825195, 0.05906350538134575, 0.0736035481095314, -0.01253960095345974, -0.05119631066918373, -0.01636016182601452, -0.06210269033908844, 0.013677146285772324, 0.04271251708269119, 0.021111879497766495, -0.05682215839624405, 0.05483318492770195, 0.09195131063461304, -0.029788730666041374, 0.05247194319963455, -0.006509477738291025, 0.04015181586146355, -0.07147891819477081, -0.08535056561231613, -0.002320315456017852, -0.08400516211986542, 0.036254119127988815, 0.09391462802886963, -0.020233938470482826, 0.0009740468231029809, 0.014506938867270947, -0.0486820787191391, -0.004899223335087299, 0.026324689388275146, 0.08079993724822998, -0.08342165499925613, -0.002629219787195325, -0.08196637779474258, -0.00893102865666151, -0.0030551173258572817, -0.04244847595691681, 0.061240941286087036, 0.0678931251168251, -0.039200302213430405, -0.01419686060398817, -0.03149685636162758, 0.001893616747111082, 0.03350498899817467, 0.09293275326490402, -0.09188234806060791, 0.040407564491033554, -0.05954146757721901, -0.08119267225265503, 0.07745246589183807, -0.029205551370978355, -0.055743083357810974, 0.02787020057439804, -0.0211038775742054, -0.09208467602729797, -0.05213794484734535, -0.04286530241370201, -0.007905655540525913, -0.06570518016815186, -0.09682512283325195, 0.055110298097133636], [-0.08589719235897064, -0.01277785561978817, 0.08563434332609177, -0.03686487302184105, -0.06775153428316116, 0.026041502133011818, 0.08856958895921707, -0.011650464497506618, -0.007046096958220005, 0.047476135194301605, 0.013417642563581467, 0.01902296207845211, 0.05213884636759758, 0.043915919959545135, -0.07127287983894348, -0.04051800072193146, 0.06825283169746399, 0.015274702571332455, 0.0241556316614151, 0.09462752193212509, 0.06283188611268997, 0.03886401653289795, 0.09606660902500153, -0.08809978514909744, 0.014491652138531208, 0.027711430564522743, -0.017192766070365906, -0.04535131901502609, 0.01868760585784912, -0.08293645828962326, -0.04250510782003403, 0.07491125911474228, -0.035998787730932236, -0.027832280844449997, 0.046242378652095795, -0.026531582698225975, 0.014613280072808266, -0.10091225057840347, 0.057861898094415665, -0.0901162177324295, 0.012698842212557793, -0.09859281033277512, -0.08395818620920181, 0.08020938187837601, -0.05476268753409386, -0.0027780234813690186, 0.00419933907687664, 0.09341233968734741, 0.031093621626496315, 0.04829857870936394, -0.04207612946629524, -0.033126212656497955, 0.07340966910123825, 0.03820003569126129, 0.025224296376109123, -0.07596836984157562, -0.07989394664764404, 0.018763326108455658, -0.07471834123134613, 0.06907840073108673, 0.08272796869277954, -0.04225274920463562, -0.06424316018819809, 0.09820812195539474, -0.054607365280389786, -0.07983803749084473, -0.07335028052330017, 0.017985204234719276, 0.020238006487488747, 0.06385660171508789, 0.07977758347988129, 0.04155630245804787, 0.014537479728460312, 0.08920060098171234, -0.04033549129962921, -0.019680196419358253, 0.08465635031461716, -0.03450029343366623, 0.051709748804569244, 0.021664446219801903, 0.03496305271983147, -0.02254571206867695, -0.08466790616512299, 0.07002776861190796, 0.08336975425481796, -0.0786934569478035, -0.04730726033449173, 0.003574433270841837, 0.04855147376656532, -0.012405098415911198, -0.04230690374970436, 0.07873735576868057, -0.0500413253903389, 0.09151896834373474, 0.0018955800915136933, 0.0512007474899292, 0.02283838763833046, 0.01719512790441513, 0.07359828799962997, -0.005601809825748205], [0.07120832800865173, 0.032943885773420334, -0.056716322898864746, 0.0655737817287445, -0.07076290994882584, 0.051667969673871994, -0.02112036943435669, 0.031252626329660416, -0.012699637562036514, 0.020077230408787727, -0.03020676225423813, 0.04048478230834007, 0.04684143140912056, -0.09187297523021698, 0.08875883370637894, -0.08120376616716385, -0.04435303062200546, 0.03965237736701965, 0.09294357895851135, 0.032885659486055374, -0.05594742298126221, 0.016992829740047455, -0.05984213203191757, 0.046957582235336304, 0.004315381404012442, 0.01031146664172411, -0.07728156447410583, 0.053273629397153854, 0.09482701867818832, -0.06865856051445007, 0.03378095477819443, -0.04461805149912834, -0.02683638222515583, 0.08942968398332596, -0.09263736754655838, -0.019056331366300583, 0.03447295352816582, 0.04443690925836563, -0.03893402963876724, -0.05584730580449104, 0.019464557990431786, -0.09185799956321716, 0.014120440930128098, -0.06295177340507507, -0.017904693260788918, 0.09058284014463425, -0.08370474725961685, -0.04938012734055519, -0.07746460288763046, 0.05909395590424538, 0.08298661559820175, -0.06415943801403046, 0.06343699246644974, 0.0635494813323021, -0.08561625331640244, 0.03306989371776581, 0.04468487948179245, -0.01840493083000183, 0.06766558438539505, -0.006861827801913023, -0.014329942874610424, 0.05542030185461044, -0.09227310121059418, -0.09758419543504715, 0.018460145220160484, 0.011275879107415676, 0.007274061907082796, -0.06109943613409996, 0.07981108874082565, -0.061693791300058365, -0.04388650879263878, -0.08401090651750565, -0.09975965321063995, 0.023549247533082962, -0.04028990864753723, 0.06046261638402939, 0.09657976776361465, 0.033368516713380814, 0.08736918121576309, -0.06959328800439835, 0.05255782976746559, 0.05462443456053734, -0.040028493851423264, -0.007400670554488897, -0.05875243991613388, 0.010652651078999043, -0.0955236554145813, -0.05925707146525383, -0.08607456088066101, -0.01236256118863821, 0.010306605137884617, -0.0807267427444458, -0.03747255355119705, 0.060935284942388535, 0.00468252319842577, 0.017339903861284256, -0.04491569846868515, -0.029262710362672806, -0.08847425132989883, 0.09256796538829803], [0.01112113706767559, 0.046185772866010666, 0.058606602251529694, 0.08755714446306229, -0.06878537684679031, 0.028092768043279648, -0.08703908324241638, 0.03731173649430275, -0.05108971148729324, 0.03885096311569214, -0.07369828969240189, 0.05331486463546753, 0.07164531201124191, 0.05503489449620247, -0.08427903056144714, 0.06287264078855515, -0.06350496411323547, -0.09360328316688538, 0.06490214169025421, 0.01717497408390045, -0.03480438143014908, -0.08721689879894257, 0.02557835355401039, -0.043191734701395035, -0.07889579236507416, -0.01301636639982462, -0.08500175178050995, 0.04914076253771782, -0.06583631038665771, -0.03404165804386139, -0.0987836942076683, -0.013384559191763401, -0.031024914234876633, 0.054745983332395554, -0.02291981689631939, 0.06755577772855759, 0.08779335021972656, 0.02065887674689293, -0.0343509167432785, -0.08200471848249435, -0.06963855773210526, -0.03352051228284836, -0.0630250945687294, 0.05294099077582359, -0.03240078687667847, -0.003954949788749218, 0.00433213310316205, -0.08689844608306885, 0.08344720304012299, -0.03812512382864952, -0.08133166283369064, -0.0019033519783988595, -0.08813369274139404, 0.012331177480518818, -0.06494492292404175, -0.021578431129455566, -0.02913789451122284, 0.0727044865489006, 0.07701089978218079, 0.0992865115404129, 0.011431016959249973, 0.0640210434794426, -0.08189616352319717, -0.06723730266094208, 0.05397086963057518, 0.02207845263183117, 0.02776431106030941, 0.056996725499629974, 0.06317513436079025, 0.047034189105033875, -0.0369742214679718, -0.03152694180607796, 0.021165117621421814, 0.08260342478752136, -0.007558309938758612, -0.029163051396608353, 0.055292196571826935, 0.06628799438476562, -0.0015524900518357754, -0.029518771916627884, -0.09933743625879288, 0.006227153353393078, 0.03046504408121109, -0.07332664728164673, -0.07467049360275269, -0.10029549151659012, 0.07798764109611511, 0.09355175495147705, 0.09888731688261032, 0.08890219777822495, -0.010776453651487827, 0.04438672214746475, 0.05947365239262581, -0.02690795436501503, -0.05755868926644325, 0.08356685191392899, -0.010171052068471909, 0.036762505769729614, -0.04669700190424919, -0.05118803307414055], [-0.01839122176170349, 0.003912206273525953, 0.05973709747195244, 0.06554136425256729, -0.010795483365654945, 0.09897936135530472, -0.05486483499407768, 0.007101427763700485, -0.06135931983590126, 0.015334873460233212, 0.08991866558790207, -0.08281345665454865, 0.02437460608780384, -0.08971620351076126, -0.02280593290925026, -0.0462738499045372, -0.00011541754065547138, -0.0694584995508194, 0.023634811863303185, -0.09298260509967804, 0.021112879738211632, -0.03691725432872772, -0.06020865961909294, 0.08489502966403961, 0.012262610718607903, -0.06816763430833817, -0.056225694715976715, -0.04650924354791641, -0.05985148251056671, 0.011880059726536274, 0.0013857027515769005, 0.08807296305894852, 0.0020054469350725412, -0.05419537052512169, 0.08405640721321106, 0.02612246945500374, -0.021531904116272926, 0.07146137207746506, 0.08099722862243652, -0.08239857107400894, -0.08194750547409058, -0.06641130149364471, -0.08825471252202988, -0.029773974791169167, 0.036127880215644836, -0.0918814018368721, -0.09660793840885162, -0.08718854188919067, -0.05114062875509262, 0.06364704668521881, 0.0881413072347641, 0.09818974137306213, 0.0004797130241058767, -0.08627067506313324, 0.09751511365175247, -0.06640616804361343, 0.03757514804601669, 0.051253654062747955, 0.030026022344827652, 0.03450655937194824, -0.0925586074590683, -0.03268183767795563, -0.06443210691213608, 0.06690166890621185, -0.05489082634449005, -0.0022055080626159906, 0.08912158012390137, 0.07493530213832855, -0.07267310470342636, -0.08258138597011566, 0.08018121123313904, 0.02983328327536583, -0.07711037248373032, 0.08788678050041199, -0.07319319993257523, 0.08860840648412704, 0.07212872803211212, 0.07185719162225723, -0.01701500453054905, 0.008474733680486679, -0.08299064636230469, 0.055834703147411346, 0.005408556666225195, 0.0763479620218277, -0.04343719035387039, 0.0954006090760231, 0.06748566776514053, -0.027307972311973572, 0.028519850224256516, -0.07523123919963837, -0.06840388476848602, -0.06956524401903152, -0.05747982859611511, 0.027114512398838997, -0.04329004883766174, -0.033101268112659454, 0.07096997648477554, 0.01707591488957405, -0.049042489379644394, -0.026329021900892258], [-0.026385534554719925, 0.04649564251303673, 0.07009467482566833, 0.0797535628080368, -0.06815320253372192, -0.010939527302980423, 0.02318575605750084, -0.03926567733287811, 0.06497397273778915, -0.010881973430514336, 0.010760409757494926, -0.0854647159576416, 0.07207348197698593, -0.06699752807617188, -0.002606702269986272, -0.08210448920726776, 0.07579756528139114, 0.005513906478881836, 0.04499800130724907, -0.07875195145606995, 0.009227429516613483, 0.040799375623464584, 0.07062968611717224, 0.07336024940013885, -0.017693012952804565, -0.010266531258821487, 0.03364459052681923, -0.03550482913851738, -0.040986377745866776, -0.0922727957367897, -0.09770943969488144, -0.05233068764209747, 0.012270184233784676, 0.008320542983710766, 0.00861943420022726, 0.014469371177256107, -0.01851601153612137, -0.09932245314121246, 0.08988051861524582, -0.07351010292768478, -0.0014071534387767315, 0.03988390788435936, 0.026329481974244118, -0.04325038939714432, -0.08680737763643265, 0.08020367473363876, 0.003399715991690755, 0.09097331762313843, 0.045891549438238144, 0.042559798806905746, 0.021431084722280502, -0.02227083593606949, -0.05855976790189743, 0.012204012833535671, 0.09590855240821838, 0.05984533205628395, 0.09060657024383545, 0.093112513422966, 0.07770054042339325, -0.05662506818771362, -0.054783087223768234, -0.0037813002709299326, -0.05178433656692505, -0.026684554293751717, -0.034113265573978424, 0.08809009939432144, -0.007345783989876509, 0.09661587327718735, -0.0774177834391594, -0.09875597804784775, 0.014595616608858109, 0.02002997323870659, -0.0402219295501709, -0.05084571987390518, 0.029155908152461052, 0.004541691392660141, -0.012789526954293251, 0.05086047947406769, -0.042675215750932693, 0.07903029024600983, -0.06742933392524719, -0.05144848674535751, 0.034497153013944626, 0.060723818838596344, -0.0007675801170989871, -0.07841862738132477, -0.0045778341591358185, -0.034349922090768814, -0.03262826427817345, -0.01865646429359913, 0.024304140359163284, -0.023377375677227974, -0.06972774118185043, -0.049201760441064835, -0.08352303504943848, -0.09981215745210648, 0.048358380794525146, 0.055453889071941376, 0.07568351924419403, 0.008486529812216759], [-0.04132004454731941, -0.05970059707760811, 0.05018843337893486, -0.06095554679632187, 0.07524072378873825, -0.05375707149505615, -0.09328712522983551, -0.07141878455877304, 0.08277799189090729, 0.07540053129196167, 0.024334952235221863, 0.002898073522374034, 0.08594491332769394, 0.06102565675973892, 0.05892179533839226, -0.06671423465013504, 0.016546182334423065, 0.09096261858940125, -0.0702102854847908, 0.10002720355987549, -0.06646876037120819, -0.029280677437782288, -0.0048934160731732845, 0.026333153247833252, 0.03088132105767727, -0.05771784484386444, 0.045195337384939194, 0.025668030604720116, -0.010439572855830193, -0.09872821718454361, 0.09877380728721619, -0.03648857772350311, -0.05306153744459152, 0.09813820570707321, -0.0516262985765934, 0.06787172704935074, 0.061743494123220444, -0.06441953033208847, -0.08160930871963501, -0.05370135232806206, 0.032214753329753876, 0.09426871687173843, -0.006681948900222778, -0.030242303386330605, -0.029041893780231476, -0.07027412205934525, 0.09893948584794998, -0.0018392258789390326, -0.07829637080430984, -0.06831226497888565, 0.06177899241447449, 0.04761269688606262, 0.0642201229929924, -0.01093303132802248, -0.020067701116204262, 0.033227480947971344, -0.00010821862088050693, -0.030767854303121567, 0.024451743811368942, -0.05643216893076897, -0.06847188621759415, -0.02153274416923523, -0.07112710177898407, 0.002200634451583028, 0.015415344387292862, 0.097445547580719, 0.03013470396399498, -0.00047984093544073403, -0.07344689965248108, -0.04004015773534775, -0.016565537080168724, -0.07181636244058609, 0.019713636487722397, 0.07264530658721924, 0.056505315005779266, -0.039820075035095215, -0.02548898011445999, 0.0997866690158844, -0.015515085309743881, -0.09021993726491928, -0.0014119772240519524, 0.004743034485727549, -0.037370942533016205, -0.009988904930651188, -0.08260736614465714, 0.030509335920214653, -0.0690324530005455, 0.03477338328957558, 0.0760132297873497, -0.06479143351316452, -0.03985714167356491, 0.009572294540703297, 0.00011342312791384757, 0.05759970471262932, -0.0064046867191791534, 0.016596321016550064, -0.027345679700374603, 0.03337819501757622, 0.030441956594586372, -0.08091682195663452], [0.052841491997241974, -0.02159767784178257, 0.08844663202762604, -0.1000029668211937, -0.012676529586315155, 0.043909043073654175, 0.07117236405611038, 0.0005677785375155509, -0.05666985735297203, -0.0754745826125145, -0.07381594926118851, 0.08063491433858871, -0.08855874091386795, -0.043485209345817566, 0.05286224186420441, 0.05307832360267639, -0.08729084581136703, 0.019671140238642693, 0.014658338390290737, 0.08952105790376663, -0.0016740740975365043, 0.039841122925281525, -0.008063849993050098, 0.08899157494306564, -0.012269115075469017, -0.05592915043234825, 0.01859654113650322, -0.04973657429218292, 0.025747349485754967, 0.0700400173664093, -0.07740486413240433, -0.07384195923805237, 0.0025944733060896397, -0.004262841772288084, -0.02772940695285797, 0.08172677457332611, 0.0447000153362751, -0.03560500964522362, 0.01644706539809704, -0.01800183579325676, -0.06501650810241699, 0.04197372868657112, -0.06193595007061958, 0.011859836056828499, 0.06865322589874268, -0.09072260558605194, 0.0031922271009534597, 0.09379646927118301, 0.04290430620312691, -0.041018079966306686, 0.016366589814424515, -0.09185638278722763, 0.017087487503886223, 0.08937419950962067, 0.08672012388706207, -0.002921178238466382, 0.03630363568663597, -0.019130850210785866, -0.027143046259880066, -0.0007694863015785813, -0.05968455970287323, 0.03838594630360603, 0.049113426357507706, -0.02347155287861824, -0.025515997782349586, -0.06941977143287659, 0.01186863798648119, 0.040906064212322235, 0.09609293192625046, -0.09559038281440735, -0.08555680513381958, 0.0013172089820727706, 0.09324163943529129, 0.004114673472940922, 0.02988957054913044, 0.010695010423660278, -0.012039314955472946, 0.0705348327755928, 0.06997940689325333, -0.05701005458831787, -0.05996061861515045, -0.07644384354352951, -0.02388632483780384, -0.006294215098023415, 0.08603116869926453, -0.09491875767707825, 0.0692458227276802, -0.05747139826416969, 0.005758553743362427, -0.0014417953789234161, 0.05939355492591858, -0.08218055963516235, -0.06941287219524384, 0.004018726292997599, 0.0508645623922348, -0.050199445337057114, 0.02952669933438301, -0.027593975886702538, -0.09267707169055939, 0.010638044215738773], [-0.02133626863360405, 0.05886150524020195, 0.06595517694950104, 0.06577097624540329, 0.06193099170923233, -0.0775105208158493, -0.09565821290016174, 0.010612668469548225, 0.028437169268727303, -0.07709825783967972, 0.08186917006969452, -0.01079931017011404, -0.058771420270204544, 0.06841904670000076, 0.08791778236627579, 0.014631693251430988, -0.03227506950497627, 0.01063147746026516, -0.07395476847887039, 0.06114659458398819, 0.06326396018266678, -0.018643304705619812, -0.07915546745061874, 0.07401449978351593, -0.08196573704481125, -0.05185438320040703, 0.060073256492614746, -0.048998232930898666, 0.01875358633697033, 0.08139291405677795, -0.01268264651298523, -0.02325766161084175, 0.09269256889820099, 0.01744711399078369, 0.035235319286584854, -0.0334760881960392, -0.08641638606786728, 0.01569223217666149, -0.05854969471693039, -0.05187789723277092, -0.08256880939006805, 0.08598389476537704, -0.06665517389774323, 0.029087381437420845, 0.02072773315012455, 0.06944490224123001, 0.05524458363652229, -0.06901369988918304, 0.04422752186655998, -0.050577107816934586, 0.004157555289566517, -0.03581986576318741, -0.08549023419618607, 0.030597563832998276, -0.06634748727083206, 0.09776455163955688, 0.039928752928972244, -0.03535384684801102, -0.05881892889738083, -0.09224820137023926, -0.05046440660953522, -0.06534487754106522, 0.010976537130773067, 0.023915598168969154, 0.08676145225763321, 0.013290396891534328, -0.021189343184232712, -0.044510744512081146, -0.013323734514415264, 0.05266935005784035, -0.09667409211397171, -0.09205782413482666, 0.03915617614984512, -0.03127271309494972, -0.029376329854130745, -0.0656275600194931, -0.06606429070234299, -0.07540902495384216, -0.06140410527586937, -0.09130312502384186, -0.04712925851345062, 0.07999657094478607, -0.054980430752038956, 0.07555696368217468, -0.08381186425685883, 0.011591571383178234, -0.008290297351777554, -0.07557928562164307, 0.0021378875244408846, 0.08386045694351196, -0.04448423907160759, 0.09220005571842194, -0.010479673743247986, -0.03148205578327179, 0.04971396550536156, 0.07759177684783936, 0.08611845225095749, 0.05899256095290184, -0.08974884450435638, -0.020254336297512054], [0.011317012831568718, -0.04552852734923363, 0.04679923504590988, -0.0220046304166317, -0.0855846107006073, -0.044454727321863174, -0.06435251981019974, 0.03875211253762245, -0.07207268476486206, -0.02468472719192505, 0.01015500258654356, 0.042802777141332626, -0.07693883776664734, 0.06931085884571075, -0.06448964774608612, -0.07635675370693207, 0.04128541424870491, -0.058476563543081284, 0.07258982211351395, 0.04311090335249901, -0.048967394977808, -0.022125598043203354, -0.04561229795217514, -0.00914375577121973, -0.00034805064206011593, -0.0029032158199697733, -0.06590516120195389, -0.08423908799886703, -0.06379149854183197, 0.017482714727520943, 0.010317975655198097, -0.05032293498516083, -0.015812654048204422, -0.002196590881794691, 0.07113336771726608, -0.08978520333766937, -0.04478607326745987, -0.014656264334917068, -0.017160464078187943, -0.05267268791794777, 0.0501614548265934, -0.038084834814071655, 0.010168434120714664, -0.057396553456783295, 0.09882242232561111, -0.05914578214287758, -0.02158167213201523, 0.07973148673772812, 0.05432291701436043, -0.09042519330978394, 0.06357058882713318, -0.05325760692358017, -0.011843043379485607, 0.04041352868080139, 0.00616298895329237, 0.07680507004261017, -0.03754369914531708, -0.06951260566711426, 0.00657335901632905, 0.034468453377485275, 0.07482770085334778, -0.05104503035545349, -0.011577114462852478, -0.057781241834163666, -0.07268388569355011, -0.07805247604846954, -0.07007083296775818, -0.06028195843100548, 0.032123930752277374, -0.013612573966383934, 0.0632108747959137, 0.07684917747974396, -0.07703908532857895, 0.01970420777797699, 0.054603707045316696, -0.08031336218118668, -0.02964625135064125, 0.07294541597366333, -0.0737973228096962, 0.08911155164241791, 0.008547864854335785, 0.018500395119190216, 0.0833999365568161, -0.09264970570802689, 0.08516667038202286, -0.08915373682975769, 0.04783738777041435, -0.004353290423750877, 0.0789526104927063, -0.0017381375655531883, -0.0989033579826355, 0.012635973282158375, 0.06667302548885345, -0.01961458846926689, -0.09520760178565979, -0.01378039363771677, 0.00803905725479126, 0.025213979184627533, 0.03235618770122528, -0.014066577889025211], [-0.005131452344357967, -0.02121785841882229, 0.03791842609643936, 0.022573010995984077, 0.040457550436258316, -0.009657736867666245, -0.009078189730644226, 0.07374728471040726, 0.012103065848350525, -0.09044305980205536, 0.02058381959795952, 0.09253453463315964, -0.019804684445261955, 0.058988310396671295, -0.0724194198846817, -0.01549609936773777, 0.06712480634450912, -0.05001886188983917, -0.024819958955049515, 0.000445137353381142, -0.014637570828199387, -0.028950344771146774, 0.06912305951118469, -0.018095482140779495, -0.027365176007151604, 0.07491115480661392, 0.022107891738414764, -0.04701963812112808, 0.06021905317902565, -0.04838142916560173, 0.10009074956178665, 0.08338572084903717, 0.009901381097733974, 0.028179911896586418, -0.03962027654051781, -0.07776099443435669, -0.07118453830480576, 0.030557570978999138, 0.04907757788896561, -0.0019235197687521577, -0.03257550671696663, -0.03259302303195, -0.05423416197299957, -0.0607064925134182, 0.06596469134092331, -0.08625036478042603, 0.07250948250293732, 0.05462973564863205, -0.028285255655646324, -0.013695570640265942, -0.03382069990038872, -0.08352743089199066, 0.00849310401827097, -0.07646598666906357, 0.002306475304067135, -0.035172238945961, -0.07720015943050385, -0.09295281022787094, 0.08923190832138062, 0.07683373987674713, 0.0907314270734787, 0.03662717342376709, 0.04429033398628235, 0.04981227591633797, 0.07299396395683289, 0.016270993277430534, 0.01050201803445816, 0.07305504381656647, 0.0444892942905426, -0.06099814921617508, 0.06750179082155228, 0.09662768244743347, -0.03719489276409149, -0.09523055702447891, -0.016637317836284637, -0.06546776741743088, 0.09896403551101685, -0.020924421027302742, -0.05154968798160553, 0.0641217976808548, 0.03889641910791397, 0.030477501451969147, 0.08970030397176743, -0.09949733316898346, 0.04888424649834633, -0.04639827460050583, 0.033057790249586105, -0.03669748082756996, 0.09071317315101624, -0.019149668514728546, 0.009754159487783909, -0.0787009596824646, 0.06102042272686958, -0.09807086735963821, 0.05470516160130501, 0.07073818892240524, 0.07944901287555695, 0.0607743114233017, -0.03126850351691246, 0.020233692601323128], [0.025289010256528854, -0.09585019201040268, 0.053662896156311035, -0.08050714433193207, 0.035302042961120605, 0.09632933884859085, -0.027520494535565376, 0.013181522488594055, 0.09155970811843872, 0.0824279710650444, 0.08367494493722916, -0.06942789256572723, 0.01952522248029709, -0.08430109173059464, 0.018773868680000305, -0.09548821300268173, 0.01121185626834631, 0.05046888068318367, -0.08885987848043442, -0.01557741779834032, -0.04070409759879112, 0.08188072592020035, -0.009795061312615871, 0.06188990920782089, -0.07411345839500427, 0.014789865352213383, 0.07285565882921219, 0.0064973714761435986, -0.03530875965952873, -0.04138955473899841, 0.0682530477643013, 0.03894880786538124, 0.006601099390536547, 0.05958270654082298, -0.051098402589559555, -0.07657331973314285, -0.022973690181970596, 0.07290800660848618, -0.09587199985980988, -0.05038564279675484, 0.02353254146873951, 0.03791321814060211, 0.0509645938873291, 0.030620839446783066, -0.0879056379199028, -0.03205358237028122, 0.06190042197704315, 0.017977358773350716, 0.0543847419321537, 0.0748831182718277, 0.057550959289073944, -0.04105311632156372, -0.047846246510744095, -0.016938891261816025, -0.0027388576418161392, -0.009078641422092915, -0.01964569091796875, 0.012432345189154148, 0.06278620660305023, -0.06926790624856949, 0.050401270389556885, -0.09923870861530304, 0.006794454529881477, -0.09062092006206512, 0.09642889350652695, -0.08495277911424637, -0.05673673376441002, 0.06616281718015671, 0.053703371435403824, 0.07842455059289932, 0.027202114462852478, -0.09022790938615799, -0.07038919627666473, 0.0677393227815628, -0.09694357216358185, 0.08868422359228134, -0.016257738694548607, 0.018081605434417725, -0.004889401141554117, -0.02493317238986492, -0.024467697367072105, 0.029857536777853966, -0.01734582707285881, 0.015908939763903618, 0.08462657034397125, 0.009368136525154114, -0.07886476814746857, -0.07586637884378433, 0.06299556791782379, 0.035682350397109985, 0.020096998661756516, -0.04959917441010475, -0.08190326392650604, 0.08653465658426285, -0.09349850565195084, -0.050413843244314194, 0.03863609954714775, 0.08600743114948273, -0.034790217876434326, -0.0968436449766159], [0.02336982823908329, -0.002435822505503893, -0.016373397782444954, -0.08981844037771225, 0.06976599991321564, 0.013996079564094543, 0.0297223050147295, 0.06048236042261124, 0.08587387949228287, -0.024188155308365822, 0.009567064233124256, -0.06313538551330566, -0.07588788866996765, -0.052364617586135864, 0.01507735438644886, -0.0634504035115242, 0.05732245743274689, 0.035176146775484085, -0.030919617041945457, 0.04909038916230202, 0.07513424009084702, -0.03492865338921547, 0.05580776184797287, 0.06238250434398651, 0.09265392273664474, -0.07253584265708923, 0.07935679703950882, 0.0008399668149650097, 0.007281466852873564, 0.0012333678314462304, -0.06446300446987152, -0.03641977161169052, -0.08092078566551208, -0.01238457765430212, 0.026647239923477173, -0.07129774242639542, 0.0380442813038826, -0.09970512241125107, 0.048749133944511414, -0.10003817081451416, -0.003366695949807763, 0.07349152863025665, -0.08894989639520645, -0.07885099947452545, -0.04782024025917053, -0.03949977084994316, -0.05579700693488121, 0.08709939569234848, -0.07319176197052002, 0.04403284564614296, -0.07192182540893555, -0.0316968597471714, -0.07974281907081604, -0.09547833353281021, -0.05457116290926933, 0.002357891295105219, 0.05396879464387894, 0.038555748760700226, 0.04083051532506943, 0.08321350067853928, -0.00537895318120718, 0.09681794047355652, -0.08780532330274582, -0.0447302907705307, -0.0817708820104599, -0.06846992671489716, -0.07379263639450073, -0.0942419022321701, 0.09377752244472504, 0.018796861171722412, 0.050149694085121155, -0.032313406467437744, 0.02342774160206318, -0.05457775667309761, -0.09824643284082413, 0.008497612550854683, 0.06083226948976517, -0.015241804532706738, 0.030636882409453392, 0.04232332110404968, 0.07288089394569397, -0.0934855192899704, -0.04157712310552597, 0.004922045394778252, 0.09754946827888489, 0.09795860201120377, -0.06870660930871964, -0.0664074644446373, 0.09400088340044022, -0.006393805146217346, -0.057498060166835785, -0.05114312097430229, 0.05799577012658119, 0.06315566599369049, 0.08402767032384872, -0.020694827660918236, -0.03091123327612877, -0.005069434642791748, 0.0886150449514389, -0.08951472491025925], [0.05037738010287285, -0.06923165172338486, -0.010042750276625156, 0.08456676453351974, -0.035059940069913864, -0.040543332695961, -0.00441695423796773, 0.04402592033147812, 0.08153770118951797, -0.0964745581150055, 0.07805294543504715, -0.03384573385119438, 0.014678304083645344, -0.05367015674710274, 0.025175491347908974, -0.09694758802652359, -0.06439211964607239, -0.0454925000667572, 0.06168083846569061, 0.026910895481705666, -0.07119434326887131, 0.015718311071395874, 0.09690757095813751, -0.09663350880146027, 0.08824077248573303, 0.0770856961607933, 0.005800541955977678, -0.05156366154551506, 0.045203644782304764, 0.04663766920566559, -0.039083972573280334, -0.05625995621085167, -0.06665545701980591, -0.038960590958595276, 0.019432496279478073, 0.04367484897375107, -0.0014804668026044965, 0.08784401416778564, 0.02819909155368805, 0.030717164278030396, -0.049245402216911316, -0.029154065996408463, 0.06495004147291183, 0.051375359296798706, -0.012024463154375553, -0.06759460270404816, -0.09282831847667694, 0.0050425161607563496, -0.06290369480848312, -0.0148417167365551, -0.016886187717318535, -0.09463340789079666, 0.06852708756923676, -0.019046682864427567, -0.09824541211128235, -0.09044118970632553, -0.07958538085222244, -0.03823614493012428, -0.006700530648231506, 0.04247099906206131, -0.0022152697201818228, -0.09465616941452026, -0.07940454035997391, -0.052757155150175095, 0.07961130887269974, -0.004928493872284889, -0.09241732209920883, -0.04232281073927879, -0.0729827806353569, 0.014114480465650558, -0.008390545845031738, -0.01550535298883915, 0.07704395055770874, -0.05964435636997223, 0.009701545350253582, -0.03514567017555237, 0.017784733325242996, -0.07945311814546585, -0.03751588612794876, 0.047052644193172455, -0.06680040061473846, -0.04652571678161621, -0.013244962319731712, -0.07682507485151291, -0.06558668613433838, -0.03343964368104935, -0.011122935451567173, -0.02091851457953453, 0.006403010338544846, -0.09257347136735916, -0.016916612163186073, -0.08808460831642151, 0.05260433629155159, 0.08997832238674164, -0.0787958949804306, -0.07142853736877441, 0.0976053774356842, 0.07790400832891464, 0.045511726289987564, -0.06249857321381569], [0.03135871887207031, -0.0035644271411001682, 0.07492408156394958, 0.061926208436489105, -0.011700047180056572, 0.03828335925936699, -0.0869855135679245, 0.0172883328050375, -0.061526838690042496, 0.08062797784805298, -0.07204986363649368, -0.08682914823293686, -0.08580535650253296, -0.030074574053287506, 0.004873183090239763, -0.011881471611559391, 0.041241299360990524, -0.08890688419342041, -0.058053381741046906, -0.0146712651476264, -0.002373139141127467, -0.02270410768687725, 0.04610570892691612, 0.051943711936473846, -0.01832457259297371, -0.004799446556717157, -0.015618633478879929, 0.0103346211835742, -0.04168925806879997, -0.015048992820084095, -0.07280685007572174, 0.08392317593097687, 0.06890653073787689, -0.06726063787937164, 0.00834826659411192, -0.05959990993142128, -0.020019974559545517, -0.09010838717222214, -0.0062988330610096455, -0.046295907348394394, -0.08974229544401169, 0.09835071861743927, 0.027021760120987892, 0.06815096735954285, 0.016406670212745667, 0.0052816192619502544, -0.046079959720373154, 0.03953420743346214, 0.06287047266960144, -0.00666387565433979, -0.05174481123685837, 0.08090478181838989, -0.008150017820298672, -0.06527761369943619, 0.056174587458372116, 0.09792156517505646, 0.09445646405220032, 0.08140932768583298, 0.09077959507703781, -0.10009808093309402, 0.03354956582188606, 0.08578820526599884, -0.06500058621168137, 0.02795288898050785, -0.03247452527284622, -0.0065138270147144794, 0.04630286619067192, 0.0076048350892961025, -0.036210231482982635, -0.019271105527877808, 0.03805136680603027, -0.05075880512595177, 0.06736735254526138, -0.09325049817562103, -0.06711643189191818, 0.027156895026564598, 0.0021751222666352987, 0.056830838322639465, 0.041782405227422714, 0.005198576021939516, 0.015976937487721443, 0.0743064135313034, -0.09418074786663055, -0.03535202518105507, 0.0651436597108841, -0.07765398174524307, 0.04697249084711075, 0.06531254947185516, 0.03118322603404522, -0.06525573134422302, -0.0557204969227314, 0.023231999948620796, -0.044058095663785934, 0.04042002186179161, 0.02840353175997734, 0.017999663949012756, 0.09046659618616104, -0.09672284126281738, -0.0953531563282013, 0.06637559831142426], [-0.032812315970659256, -0.09481751918792725, -0.03281053900718689, 0.07720315456390381, -0.012523041106760502, 0.04952073469758034, 0.057304032146930695, -0.030543498694896698, 0.053278598934412, 0.029211608693003654, -0.0653606727719307, -0.021326029673218727, 0.06186356395483017, 0.04212183877825737, 0.014582246541976929, -0.012035670690238476, -0.07069598883390427, -0.035933636128902435, 0.06294921785593033, -0.0039019600953906775, -0.0614280067384243, 0.015218251384794712, -0.08259596675634384, -0.012384414672851562, -0.0033355290070176125, -0.0954698920249939, -0.0675153061747551, -0.06984218955039978, -0.0704878717660904, 0.08064381778240204, 0.03815170004963875, -0.07138678431510925, -0.043474890291690826, 0.073207788169384, 0.08526573330163956, 0.010727832093834877, 0.0001428250689059496, -0.07444290071725845, 0.05809321254491806, -0.026540065184235573, 0.00914037600159645, -0.021535232663154602, 0.032073114067316055, 0.023786231875419617, -0.023287268355488777, -0.016104264184832573, 0.0006876116967760026, -0.07877304404973984, -0.004067846108227968, -0.07352986931800842, 0.07805962860584259, 0.04998166859149933, -0.010989190079271793, 0.06756167113780975, 0.00986035168170929, 0.09163166582584381, 0.019107243046164513, 0.048193953931331635, 0.014759141020476818, -0.0546574704349041, -0.09349490702152252, -0.038629114627838135, 0.07788803428411484, 0.04823886603116989, -0.0305299274623394, -0.06285972148180008, 0.09858475625514984, 0.010272392071783543, -0.08613529056310654, -0.03433030843734741, 0.007609924301505089, -0.008819990791380405, -0.03995819762349129, 0.03065713308751583, -0.06977581232786179, 0.04285050183534622, -0.0671287402510643, 0.08116227388381958, 0.0823216438293457, 0.09850036352872849, -0.09880519658327103, -0.03829386830329895, -0.019784975796937943, -0.08932274580001831, 0.0441412627696991, -0.041389793157577515, 3.799166006501764e-05, 0.04984784498810768, -0.07593216001987457, 0.012193357571959496, -0.04536742717027664, 0.06928956508636475, 0.08632268756628036, -0.024519057944417, -0.06464468687772751, 0.05921585112810135, -0.07078810036182404, 0.09604807198047638, 0.07894426584243774, 0.0427781417965889], [-0.04739295318722725, 0.07475843280553818, 0.07664009183645248, -0.01902160793542862, -0.05910612642765045, 0.0036783076357096434, 0.07768949121236801, 0.08718712627887726, -0.091187484562397, 0.08887382596731186, -0.06516005098819733, 0.018890919163823128, -0.0439925454556942, -0.05463641509413719, -0.015560001134872437, 0.004296636208891869, -0.08125081658363342, -0.03461911901831627, 0.07102111726999283, -0.0035884063690900803, -0.05268843099474907, 0.08681222051382065, -0.0937604159116745, -0.026038099080324173, 0.08348896354436874, 0.009244878776371479, -0.036190878599882126, -0.05488322675228119, -0.02499200589954853, -0.055864118039608, -0.007254233118146658, 0.09340846538543701, 0.054359447211027145, -0.03342531993985176, 0.06053610518574715, -0.09194126725196838, 0.001752619631588459, 0.08620892465114594, 0.02179606631398201, -0.07979749888181686, 0.018538394942879677, 0.061757687479257584, -0.009795955382287502, 0.058892250061035156, -0.08486083894968033, 0.06768426299095154, -0.09716115891933441, 0.043793968856334686, -0.08426705002784729, -0.0295881237834692, -0.009528042748570442, -0.016145991161465645, -0.011764392256736755, -0.07668551802635193, -0.07129542529582977, 0.06402222067117691, 0.024276692420244217, 0.011746486648917198, -0.017445793375372887, 0.02734479494392872, 0.023165976628661156, -0.02039359137415886, 0.007779115345329046, 0.006170438602566719, 0.088802769780159, -0.09219866245985031, -0.06532878428697586, -0.04511762410402298, 0.043352264910936356, -0.06768789887428284, -0.04863715544342995, -0.04394371062517166, -0.009055051021277905, -0.09259109199047089, 0.016861416399478912, 0.0767994076013565, 0.062358736991882324, -0.07839620113372803, -0.029176201671361923, 0.05185826122760773, 0.03515196964144707, 0.043446991592645645, 0.04192538186907768, -0.022598842158913612, 0.0646904855966568, 0.09263036400079727, 0.03783596679568291, -0.015695519745349884, 0.04447478801012039, 0.06885895878076553, -0.07975531369447708, 0.02757413685321808, -0.04787590354681015, 0.0035489555448293686, -0.06627582758665085, -0.018155580386519432, 0.0009362703422084451, -0.06157146021723747, -0.07280600070953369, 0.04685040935873985], [-0.04900791123509407, 0.06937769800424576, -0.04725194349884987, 0.0833359807729721, 0.004148625303059816, -0.05831050127744675, -0.005103097762912512, -0.033796098083257675, 0.017798254266381264, 0.04243715852499008, -0.09171552211046219, 0.015293467789888382, 0.05321121960878372, -0.045645833015441895, 0.0014477120712399483, 0.05386946722865105, -0.07748577743768692, 0.0744505450129509, -0.08347483724355698, -0.029716182500123978, 0.031620439141988754, -0.01686326041817665, -0.09737450629472733, -0.06803014874458313, -0.004930463153868914, -0.03809409216046333, -0.0179353766143322, 0.06622681021690369, -0.04406870901584625, 0.010239680297672749, -0.08591866493225098, -0.041915711015462875, 0.0805179700255394, 0.041604720056056976, -0.001539950375445187, -0.04023481532931328, -0.07695715129375458, 0.04956674575805664, -0.04434667155146599, -0.0036413129419088364, -0.05957873538136482, -0.03687725588679314, 0.004165444523096085, -0.015084551647305489, -0.007173802703619003, -0.019461536779999733, 0.03869336470961571, 0.017977267503738403, 0.08924277126789093, 0.00648531736806035, -0.09064358472824097, -0.0052436827681958675, 0.0815063863992691, -0.03960137814283371, 0.0025255768559873104, 0.005214812234044075, -0.06964371353387833, 0.0954122394323349, -0.02416468784213066, -0.07466916739940643, -0.0714990645647049, -0.060576874762773514, -0.09691774100065231, 0.07721798121929169, 0.08960144221782684, -0.08164441585540771, -0.05587446317076683, 0.04823950678110123, -0.02748790755867958, 0.09756220877170563, -0.05031405761837959, -0.038174666464328766, -0.0817246213555336, -0.030833911150693893, 0.06809879094362259, -0.015076401643455029, -0.09182340651750565, 0.056103575974702835, -0.05526310205459595, 0.029862919822335243, 0.09844817221164703, 0.04695190116763115, 0.08286377787590027, -0.07500603795051575, -0.07511556893587112, -0.09721692651510239, 0.06646478921175003, -0.01851624622941017, 0.038182295858860016, -0.06541560590267181, 0.0067605385556817055, 0.09181400388479233, 0.004377966281026602, 0.08503331989049911, -0.06831064075231552, 0.010588168166577816, -0.02204318344593048, -0.09124165773391724, -0.05046640709042549, -0.009467233903706074], [-0.09549979865550995, 0.07125547528266907, -0.05680921673774719, -0.004911480005830526, -0.00020875116751994938, -0.09624771028757095, -0.04600559547543526, 0.016193130984902382, -0.029003940522670746, 0.05872895196080208, 0.027997111901640892, 0.037099581211805344, 0.05675673112273216, 0.017190445214509964, -0.08784093707799911, 0.04315541684627533, -0.028252603486180305, 0.020464615896344185, 0.033242154866456985, 0.029318474233150482, 0.07251899689435959, 0.09309817850589752, -0.07354283332824707, -0.04667918384075165, 0.04298632964491844, -0.00278604868799448, 0.02269730344414711, -0.08980493992567062, -0.04136016592383385, -0.0017054426716640592, 0.08166391402482986, -0.07320643216371536, 0.022210456430912018, 0.012699081562459469, -0.04330664873123169, 0.043192412704229355, 0.08725832402706146, 0.08272600919008255, -0.022702446207404137, -0.0046533807180821896, -0.00454371003434062, 0.057935748249292374, -0.05409922078251839, 0.025332359597086906, 0.057573623955249786, 0.05729852616786957, -0.03959532454609871, 0.056624170392751694, -0.08526049554347992, -0.030734937638044357, 0.08050617575645447, -0.0464995838701725, 0.0036576255224645138, 0.0935620367527008, 0.012297814711928368, -0.0008965484448708594, 0.02350737899541855, -0.07142478227615356, -0.0991232767701149, 0.04731795936822891, -0.01135617308318615, 0.045243389904499054, -0.005665624048560858, 0.07200577110052109, -0.06966422498226166, -0.05065904185175896, -0.08880442380905151, -0.08298686146736145, 0.07726228982210159, -0.020986076444387436, -0.04946499690413475, -0.03107656165957451, -0.008608119562268257, -0.030252190306782722, -0.09511366486549377, 0.06832713633775711, -0.0049288030713796616, -0.07119344919919968, 0.029478097334504128, -0.031159240752458572, -0.02634076587855816, -0.0792062059044838, -0.09797129034996033, 0.030339647084474564, -0.08880462497472763, 0.064915731549263, -0.09061124175786972, -0.012581144459545612, -0.07741805166006088, -0.07066386193037033, -0.08369956165552139, -0.057250455021858215, 0.006145431660115719, 0.01231456734240055, -0.045705970376729965, 0.0604722797870636, -0.0377974808216095, -0.008887283504009247, 0.05246362462639809, 0.09233567118644714], [0.08535357564687729, -0.09441862255334854, -0.07012207806110382, 0.0174602922052145, -0.0025714498478919268, 0.07613667845726013, 0.05639459565281868, -0.07365160435438156, 0.09174071252346039, 0.029428783804178238, 0.0515662245452404, -0.01938197761774063, 0.0729551836848259, -0.06796950101852417, 0.04565111920237541, 0.09809957444667816, 0.029061388224363327, -0.03461834043264389, 0.0038355898577719927, 0.0718422457575798, -0.0582682341337204, -0.050228819251060486, 0.016360322013497353, -0.043747544288635254, -0.028706340119242668, -0.03353945165872574, 0.05444355681538582, -0.08502110093832016, -0.08576857298612595, 0.07727281749248505, -0.014854968525469303, -0.09017407894134521, 0.04832012206315994, -0.06285367906093597, -0.004831344820559025, 0.06012231484055519, -0.05743812024593353, -0.07674556970596313, 0.014463729225099087, -0.05003146454691887, -0.024601241573691368, 0.0728963315486908, 0.021748270839452744, 0.02879001572728157, -0.08978740125894547, -0.060819175094366074, -0.06963767856359482, 0.10016903281211853, -0.03699049353599548, 0.09652526676654816, 0.1001405417919159, -0.060330670326948166, 0.04306711629033089, -0.038304638117551804, 0.09743403643369675, 0.07134762406349182, 0.027491370216012, 0.05110780522227287, -0.012991499155759811, -0.01292941439896822, -0.013935255818068981, -0.04614010080695152, -0.08394715189933777, -0.013100337237119675, -0.031008100137114525, 0.08045442402362823, 0.09077070653438568, -0.08841288089752197, -0.07776830345392227, -0.04944874346256256, -0.09876637160778046, -0.00845350231975317, 0.025398854166269302, -0.011804595589637756, 0.023712795227766037, -0.019346510991454124, -0.09686245024204254, -0.05731065198779106, -0.09828461706638336, -0.05712875723838806, -0.06011548265814781, 0.0718226507306099, -0.033735089004039764, -0.02071494609117508, -0.07646679878234863, -0.032175108790397644, 0.014219081960618496, 0.022988252341747284, 0.09102135896682739, 0.04356779158115387, -0.007016243878751993, 0.07480262219905853, -0.05773935094475746, -0.00997839868068695, 0.020389048382639885, 0.026611370965838432, -0.03458132967352867, 0.011464005336165428, 0.03975386172533035, -0.007273698225617409], [-0.032680705189704895, -0.09145396947860718, -0.06083613261580467, -0.10037927329540253, -0.020848562940955162, -0.08012731373310089, -0.04316496104001999, 0.07851149141788483, -0.0946357250213623, 0.06767352670431137, -0.0013238906394690275, -0.05368833616375923, 0.09250857681035995, 0.08125560730695724, -0.05575735494494438, 0.003645758144557476, -0.040771905332803726, -0.09291131049394608, 0.053763486444950104, 0.05298067629337311, 0.08590471744537354, -0.062062669545412064, -0.026500649750232697, 0.052721764892339706, -0.05737227573990822, 0.08915634453296661, 0.0016888719983398914, 0.019248640164732933, 0.03716254234313965, 0.06065287068486214, 0.02583002857863903, -0.05952296406030655, 0.09080538153648376, -0.04352286458015442, 0.014080465771257877, -0.10023228824138641, -0.042604461312294006, 0.07164561748504639, 0.012629247270524502, 0.007421779911965132, -0.03860592097043991, -0.08063776791095734, 0.07698497921228409, -0.06878315657377243, 0.0726931169629097, -0.056887704879045486, 0.08047272264957428, 0.00463695265352726, -0.06474456936120987, 0.09729690104722977, 0.08098623901605606, -0.052201300859451294, -0.0574386790394783, -0.026434866711497307, -0.002040314022451639, 0.06830476224422455, -0.060599491000175476, 0.047578416764736176, 0.08993455767631531, 0.033150412142276764, -0.03815888240933418, 0.09357080608606339, -0.03707285597920418, 0.018542412668466568, -0.07139816880226135, 0.0406394861638546, -0.0978342667222023, 0.03697258234024048, -0.07203174382448196, -0.09555842727422714, -0.039395708590745926, 0.07234223932027817, 0.047013059258461, -0.08347510546445847, 0.05343674495816231, -0.027040520682930946, -0.04814408719539642, -0.09561063349246979, -0.05184930935502052, 0.09401790052652359, -0.062412310391664505, 0.07016090303659439, -0.03457507863640785, -0.09446780383586884, -0.05838504061102867, 0.03586457297205925, 0.04392196610569954, 0.020111210644245148, 0.026682788506150246, -0.03135326877236366, -0.00192674167919904, 0.027183623984456062, -0.012939087115228176, 0.07961848378181458, 0.0870487317442894, 0.08755826950073242, 0.08917897939682007, -0.09741799533367157, -0.0309019535779953, 0.06496728956699371], [0.06535333395004272, 0.009749866090714931, 0.03049609065055847, 0.012981077656149864, 0.015285707078874111, -0.03470858186483383, -0.009477701038122177, 0.051067959517240524, 0.09391039609909058, -0.023076539859175682, 0.06648354232311249, 0.0966905951499939, 0.02427823841571808, 0.08140915632247925, 0.05219991132616997, -0.08726925402879715, -0.060754671692848206, 0.039248351007699966, -0.038596801459789276, -0.030382540076971054, -0.027956973761320114, -0.08227546513080597, -0.0052200439386069775, 0.017812754958868027, -0.01649640128016472, -0.06453592330217361, -0.06882375478744507, 0.0838443785905838, 0.08612389862537384, -0.04749206081032753, -0.06840278953313828, -0.020861834287643433, -0.05790610611438751, -0.09769133478403091, -0.07267192751169205, 0.08589700609445572, -0.025059498846530914, 0.07555538415908813, 0.0399179682135582, -0.007843158207833767, -0.03670467436313629, 0.06854500621557236, -0.024386154487729073, -0.0661168172955513, 0.03878087177872658, -0.04605260491371155, 0.04701307788491249, 0.026278575882315636, -0.0874646008014679, -0.04837331175804138, 0.016162456944584846, 0.003142606932669878, 0.01854107901453972, -0.005807777401059866, -0.02292069047689438, -0.015347429551184177, 0.04817993938922882, -0.03998540714383125, -0.0330313965678215, 0.09056561440229416, 0.09284643083810806, 0.07284042984247208, 0.08587730675935745, -0.0650319755077362, -0.027494968846440315, -0.07315871864557266, -0.04293786361813545, 0.05224665254354477, 0.09339926391839981, -0.09516241401433945, 0.06902794539928436, -0.0397038571536541, 0.07998895645141602, 0.020951110869646072, -0.0631452426314354, 0.06583452224731445, -0.06193232536315918, -0.041099969297647476, -0.050545405596494675, 0.044613081961870193, 0.03726325184106827, -0.052502624690532684, -0.059866469353437424, 0.01594093255698681, 0.046297021210193634, 0.04873959720134735, -0.08173023164272308, 0.03386515751481056, -0.090669184923172, -0.019588056951761246, -0.00764176482334733, -0.09070941805839539, 0.058268602937459946, -0.07283855229616165, 0.018930893391370773, -0.05052534118294716, 0.047857776284217834, 0.02809855528175831, 0.04661962017416954, -0.009998222813010216], [-0.045067429542541504, -0.03864786773920059, 0.07284770905971527, 0.023115649819374084, -0.04535984992980957, -0.05057733505964279, -0.022169847041368484, -0.03748728707432747, -0.03673875704407692, 0.09433424472808838, -0.0520905926823616, 0.09993968158960342, 0.08263454586267471, -0.0598563514649868, 0.0434710793197155, -0.0899014100432396, 0.010698994621634483, -0.06532526016235352, -0.0645398199558258, 0.029290108010172844, 0.042750611901283264, -0.02581091597676277, -0.012527459301054478, 0.03597584366798401, -0.09558020532131195, 0.003186049172654748, -0.09698469191789627, -0.033763181418180466, 0.08757352083921432, -0.03479716181755066, -0.043692149221897125, -0.09260011464357376, 0.09756674617528915, 0.03542822599411011, -0.07753441482782364, -0.023051824420690536, 0.08162205666303635, -0.0964476689696312, 0.008358454331755638, -0.07340829074382782, -0.07442314922809601, -0.021252179518342018, 0.03937297686934471, -0.09961262345314026, -0.05558183416724205, -0.0805342048406601, -0.021502617746591568, -0.0476878322660923, 0.053919579833745956, -0.0002135655377060175, -0.013694699853658676, -0.0876225009560585, 0.0029878157656639814, -0.09708189964294434, -0.09051191806793213, 0.041239965707063675, -0.0984853059053421, 0.005395860876888037, 0.024333219975233078, -0.006267124321311712, 0.06743158400058746, -0.021872736513614655, 0.06961245834827423, -0.05619924142956734, -0.0799906924366951, -0.06556020677089691, 0.05553990975022316, 0.009114439599215984, -0.013263572007417679, -0.01105442177504301, -0.04053213447332382, 0.02045394852757454, -0.0055782487615942955, -0.010016222484409809, -0.042497776448726654, -0.0643266811966896, 0.011936603114008904, 0.005932436790317297, -0.08832208812236786, 0.08680988848209381, 0.04562602564692497, 0.062173906713724136, 0.029131125658750534, -0.02679898962378502, 0.033437639474868774, 0.04626516252756119, 0.038918960839509964, -0.03310416638851166, -0.05249491333961487, -0.003981874790042639, 0.07845472544431686, -0.0546155720949173, -0.0397004596889019, -0.04955726116895676, -0.08413413166999817, 0.039067357778549194, -0.01393053401261568, 0.01004714984446764, 0.09818863868713379, 0.01564645953476429], [0.07814926654100418, 0.016513274982571602, -0.00991060957312584, 0.06148378923535347, 0.031520552933216095, -0.011175669729709625, 0.002656706841662526, -0.048498548567295074, -0.006700202357023954, 0.06551657617092133, 0.08391247689723969, 0.09181850403547287, -0.07239743322134018, -0.0006904799956828356, 0.04002414643764496, 0.06844332814216614, -0.05531396344304085, -0.07968635857105255, 0.032603051513433456, -0.05682619288563728, 0.09900592267513275, -0.06131206080317497, 0.04908450320363045, -0.030662845820188522, -0.08139500021934509, -0.09851343184709549, 0.04338866472244263, -0.0318615585565567, -0.0647255927324295, 0.09811972826719284, 0.05840813368558884, 0.06586993485689163, -0.04527836665511131, -0.0054661426693201065, 0.02046005055308342, -0.010864032432436943, 0.04806441813707352, -0.02315095067024231, -0.023268084973096848, -0.009004470892250538, -0.039017677307128906, -0.06896867603063583, -0.013006451539695263, -0.03582145273685455, -0.03199758380651474, -0.04409118369221687, 0.046955619007349014, -0.016318775713443756, 0.07821518927812576, 0.030140336602926254, -0.03321221098303795, 0.09878754615783691, -0.0315062440931797, -0.08509163558483124, 0.06744783371686935, -0.05636024847626686, 0.06820708513259888, 0.07759618759155273, -0.032487932592630386, -0.03819775581359863, 0.035661015659570694, 0.05082894116640091, 0.0781012624502182, 0.023800354450941086, 0.00886264443397522, 0.08717655390501022, 0.023349596187472343, -0.08151236921548843, -0.026566846296191216, -0.0053244056180119514, 0.03442564234137535, -0.0956096351146698, -0.09985583275556564, -0.05596357583999634, 0.011785540729761124, 0.06901630014181137, -0.08336000889539719, 0.04146962985396385, 0.006461971439421177, 0.08789088577032089, -0.05608491972088814, -0.031973931938409805, 0.03247829154133797, -0.09326790273189545, 0.062387000769376755, -0.041749853640794754, 0.02063852734863758, -0.061584681272506714, 0.08144117146730423, 0.02930998057126999, 0.011241302825510502, -0.048912666738033295, -0.07292140275239944, 0.007046436425298452, -0.06914163380861282, -0.0035336895380169153, 0.08773165941238403, 0.02880670316517353, -0.027937596663832664, -0.03622715175151825], [0.040984224528074265, -0.0795585885643959, -0.07899405062198639, -0.00829857774078846, 0.05974055826663971, -0.05541721358895302, -0.015842033550143242, -0.04464667662978172, 0.06411852687597275, -0.049432165920734406, -0.014992712065577507, 0.0473516620695591, -0.04120142385363579, 0.09763910621404648, -0.054717350751161575, -0.03256477788090706, 0.007077571004629135, -0.04690396413207054, -0.09908702969551086, -0.07517310976982117, -0.09119920432567596, 0.09055955708026886, 0.006423350889235735, -0.04881035163998604, 0.038638677448034286, 0.003381283488124609, -0.04917173832654953, 0.06380703300237656, -0.0646333321928978, -0.06985675543546677, 0.07081249356269836, 0.05060954391956329, 0.0469011515378952, 0.03186730667948723, -0.0442834235727787, -0.09502719342708588, 0.0032183912117034197, 0.05503831431269646, 0.06390276551246643, -0.01514863409101963, -0.039266496896743774, -0.002635320881381631, 0.07601162791252136, -0.0057243406772613525, 0.08782412856817245, -0.036683209240436554, 0.02340952679514885, -0.06532230973243713, 0.0057295323349535465, 0.07699960470199585, -0.08368507027626038, 0.0862792581319809, 0.07521555572748184, 0.07563531398773193, -0.041326649487018585, 0.06981454789638519, 0.01563846506178379, -0.06088878959417343, 0.09560612589120865, -0.0982411727309227, 0.09000643342733383, 0.03030424378812313, 0.06568099558353424, 0.03024156019091606, -0.09132188558578491, 0.04062563180923462, -0.07063423097133636, -0.03893064707517624, -0.02470177412033081, -0.05459956079721451, -0.016278672963380814, 0.0698668360710144, 0.05193612352013588, -0.06570016592741013, 0.06409908086061478, 0.023160306736826897, 0.01092587597668171, 0.008550616912543774, -0.04809119552373886, -0.08341491222381592, 0.022809235379099846, 0.08604387938976288, -0.08243974298238754, 0.09107168763875961, 0.0499509796500206, 0.06444244831800461, 0.06183053180575371, 0.07586358487606049, 0.013920832425355911, -0.04960259050130844, -0.05938282608985901, -0.08163183927536011, 0.004979750607162714, 0.011565103195607662, 0.018733149394392967, -0.06418241560459137, 0.07051350176334381, -0.05380326136946678, -0.08759739995002747, 0.09838294982910156], [0.0231296569108963, 0.08039736747741699, 0.005925668403506279, 0.0910862386226654, -0.0028906292282044888, -0.04007462412118912, 0.047950081527233124, 0.019139863550662994, -0.039108697324991226, -0.02176845259964466, 0.045604076236486435, 0.06925494968891144, -0.07835234701633453, -0.0702589601278305, 0.06249774247407913, 0.07183000445365906, -0.013622255064547062, -0.03719871863722801, 0.034958090633153915, 0.0809568539261818, -0.06477773934602737, 0.02320302277803421, 0.05544266104698181, 0.028038110584020615, 0.09390860795974731, -0.09368224442005157, 0.057580042630434036, -0.008894581347703934, -0.04931940510869026, 0.024234741926193237, -0.022201744839549065, 0.08915642648935318, -0.0706888735294342, 0.013349253684282303, -0.035997577011585236, 0.0540548637509346, 0.07619895786046982, 0.06708872318267822, 0.03267654776573181, -0.0927128717303276, 0.013766704127192497, -0.056272849440574646, 0.03209121525287628, 0.014035928063094616, -0.08416320383548737, -0.07141675055027008, -0.050522249191999435, -0.08979441970586777, -0.040073733776807785, -0.03843297064304352, 0.055266864597797394, 0.048033423721790314, -0.07506453990936279, -0.013828457333147526, 0.023007413372397423, -0.025601264089345932, 0.024353882297873497, -0.009156263433396816, -0.08863116055727005, 0.08217553049325943, -0.07244855165481567, -0.044878847897052765, 0.010469579137861729, 0.03165300935506821, 0.032239723950624466, 0.028116796165704727, -0.08034653216600418, -0.08159541338682175, 0.08397466689348221, -0.08327355235815048, -0.02926805056631565, 0.059391506016254425, 0.07672600448131561, 0.06837662309408188, -0.03561174124479294, 0.03960553929209709, 0.04322812706232071, -0.06815154105424881, -0.06832294166088104, 0.007921945303678513, 0.08996688574552536, -0.031625960022211075, 0.030617589130997658, 0.07323909550905228, 0.05750422924757004, 0.0400027334690094, -0.031678762286901474, 0.0569039061665535, -0.09619403630495071, 0.006658895406872034, 0.013237755745649338, -0.02331823669373989, 0.09351154416799545, -0.09345507621765137, -0.06337544322013855, 0.08451162278652191, -0.027411920949816704, 0.016726989299058914, 0.010598546825349331, 0.08272814005613327], [0.0015009265625849366, -0.03050103224813938, -0.03786815330386162, -0.03238939493894577, -0.06982384622097015, 0.03462284430861473, -0.006134438794106245, -0.06311730295419693, -0.027729222550988197, -0.02016204595565796, 0.033769749104976654, -0.06888478994369507, 0.09654345363378525, -0.09339967370033264, 0.0968436598777771, 0.00048236866132356226, 0.038102567195892334, 0.033795468509197235, -0.08515188843011856, -0.09016961604356766, 0.016377246007323265, -0.023247750476002693, -0.044663168489933014, -0.04335642233490944, -0.06706288456916809, 0.042823269963264465, -0.0812012255191803, -0.04991304501891136, 0.052701547741889954, 0.04283110052347183, -0.03597639501094818, -0.09530120342969894, 0.04854642227292061, 0.08280909806489944, -0.04734225198626518, 0.08719637244939804, -0.05022084340453148, 0.06811729073524475, 0.044329896569252014, -0.08063489198684692, 0.025768056511878967, -0.05153462290763855, -0.02625504694879055, 0.0030485480092465878, -0.07205873727798462, 0.03827394172549248, 0.06995408236980438, -0.021773144602775574, 0.04476575553417206, -0.08871759474277496, 0.009468999691307545, -0.05169475078582764, 0.0033912535291165113, -0.03300611674785614, -0.05482160300016403, -0.025395384058356285, -0.09203320741653442, -0.045453980565071106, -0.04587356001138687, 0.09602829813957214, -0.050895754247903824, -0.0033632474951446056, -0.02899998240172863, -0.07760533690452576, 0.02794996090233326, -0.07285517454147339, 0.038079530000686646, -0.040531035512685776, 0.043791525065898895, 0.0655437484383583, 0.07724306732416153, 0.09426894783973694, -0.020793918520212173, -0.09943538159132004, -0.031694576144218445, -0.096768818795681, 0.016551639884710312, -0.08992819488048553, -0.016734762117266655, 0.0826583281159401, -0.0027674830053001642, 0.002763621974736452, 0.09253497421741486, -0.050401248037815094, -0.09201116859912872, 0.05360062047839165, 0.06729928404092789, -0.009120745584368706, -0.07974877953529358, 0.021485690027475357, 0.030682312324643135, -0.06954490393400192, -0.044919487088918686, 0.0016362646128982306, 0.059019170701503754, -0.0003818619588855654, 0.09445025771856308, 0.09368012100458145, 0.025833675637841225, -0.06119229272007942], [-0.04768699035048485, -0.02607719786465168, 0.0075124939903616905, -0.062259625643491745, -0.09456443786621094, -0.030180780217051506, -0.07174520939588547, 0.03421953320503235, -0.05326957628130913, -0.029171118512749672, 0.04281047731637955, 0.020075548440217972, 0.0012837676331400871, 0.05612429231405258, -0.07047969847917557, 0.026640601456165314, 0.00858303438872099, -0.09474647045135498, 0.031576745212078094, -0.06640271097421646, -0.026742687448859215, -0.04847532883286476, -0.0937243327498436, 0.06490106135606766, 0.05776049569249153, 0.012357029132544994, -0.0665593296289444, 0.08908406645059586, -0.03497151657938957, -0.06657785922288895, -0.07369488477706909, 0.013942726887762547, -0.05715800076723099, -0.013730884529650211, -0.09606640785932541, -0.025578245520591736, -0.0950666069984436, -0.07278534770011902, -0.08212229609489441, -0.06538373231887817, 0.08066606521606445, -0.03233594819903374, -0.06629861891269684, 0.09517553448677063, 0.008993344381451607, -0.08146857470273972, -0.08025434613227844, 0.009992595762014389, -0.07101912796497345, 0.03978146240115166, -0.08526456356048584, -0.08927757292985916, 0.05685318261384964, -0.003629157319664955, 0.031730443239212036, -0.02929353155195713, -0.07015936076641083, 0.06682910025119781, -0.015317483805119991, 0.07384253293275833, -0.028910933062434196, -0.02200467698276043, -0.023981057107448578, -0.055950380861759186, 0.01840192824602127, 0.02528493106365204, -0.0382414236664772, -0.060683295130729675, 0.07827985286712646, -0.004365575034171343, 0.01586531102657318, -0.05757136270403862, 0.07182207703590393, 0.0818137675523758, 0.02136223018169403, 0.0891219973564148, 0.05698579177260399, 0.08208610117435455, -0.05077343061566353, -0.03653331473469734, -0.01671760343015194, -0.04995911568403244, 0.051148299127817154, -0.05545011907815933, 0.03519075736403465, 0.056266121566295624, -0.01856532320380211, 0.09163331240415573, -0.046516209840774536, 0.044496964663267136, 0.067629873752594, -0.06077485531568527, -0.002363185165449977, 0.0781092569231987, -0.0802788957953453, -0.051797498017549515, -0.09955111891031265, 0.07004078477621078, 0.0683579072356224, 0.015260778367519379], [0.016901792958378792, -0.09672506898641586, 0.04574338719248772, -0.04840461164712906, 0.05268021300435066, -0.06453561782836914, -0.06799504905939102, 0.06978312134742737, 0.027791431173682213, -0.028402861207723618, -0.06401768326759338, -0.08633696287870407, 0.06956727057695389, -0.03556772321462631, 0.02053852379322052, 0.05935877561569214, -0.025161180645227432, -0.04764643311500549, -0.05596723407506943, 0.007345930673182011, -0.009240073151886463, 0.00036203773925080895, 0.02567044459283352, -0.08961134403944016, 0.05123184621334076, -0.016176801174879074, -0.08516406267881393, -0.03498775139451027, 0.0018104733899235725, 0.08661521226167679, 0.09814097732305527, -0.03652705252170563, -0.06794171035289764, 0.0776219516992569, 0.0813341736793518, 0.05711420252919197, -0.09059952199459076, 0.016545850783586502, 0.023395825177431107, 0.030356518924236298, -0.06837616115808487, 0.009592995047569275, -0.0386129692196846, 0.062145788222551346, -0.06600971519947052, 0.08423828333616257, 0.029028628021478653, 0.023838473483920097, -0.04636035114526749, -0.006028331350535154, 0.024136655032634735, -0.0756801962852478, 0.013476907275617123, 0.05510608106851578, 0.039040833711624146, 0.08460264652967453, -0.04970835521817207, -0.10014355182647705, -0.03981510177254677, -0.05723132565617561, 0.05798465758562088, 0.030385171994566917, -0.09436721354722977, -0.06972156465053558, -0.09050748497247696, 0.033380523324012756, 0.02427566982805729, 0.007421739399433136, -0.028420010581612587, 0.08669628202915192, 0.07645895332098007, -0.09097062051296234, -0.06222885102033615, 0.026741331443190575, -0.06667552888393402, 0.014795779250562191, 0.02211245521903038, -0.07421332597732544, 0.06829959154129028, -0.06474310904741287, -0.06122211366891861, -0.015525027178227901, -0.045066796243190765, 0.006714859511703253, -0.06281398981809616, 0.062300052493810654, -0.09573372453451157, -0.033822719007730484, 0.024322014302015305, 0.030271470546722412, 0.05035166069865227, -0.026588844135403633, 0.019963867962360382, -0.08274737000465393, -0.0323934406042099, 0.0034878032747656107, -0.08942299336194992, -0.056081708520650864, 0.026931120082736015, -0.08660572022199631], [0.07272027432918549, -0.04804461449384689, -0.039814744144678116, 0.05894693359732628, -0.028247445821762085, 0.02601723186671734, 0.0865946114063263, 0.07710470259189606, 0.08076964318752289, -0.08599793910980225, 0.09340788424015045, 0.034415584057569504, 0.031785998493433, -0.027127670124173164, -0.058962490409612656, -0.07214771956205368, 0.006385765504091978, -0.07748716324567795, -0.09635357558727264, 0.07189153134822845, -0.03639264777302742, -0.08639299124479294, -0.09676356613636017, 0.02113671414554119, -0.06456245481967926, -0.03160553053021431, -0.06284581869840622, -0.04710032418370247, 0.0546514093875885, 0.1002410426735878, 0.05336654931306839, 0.00795599166303873, 0.008713087998330593, -0.07812127470970154, -0.029551642015576363, 0.05600866675376892, -0.06713878363370895, 0.05405891686677933, -0.058668430894613266, -0.09082897007465363, 0.03572317957878113, 0.0942748412489891, 0.09372968971729279, -0.0675925686955452, -0.08466921001672745, 0.06630146503448486, -0.08245150744915009, 0.06528762727975845, 0.09861330687999725, -0.001352236489765346, 0.08645067363977432, -0.08146276324987411, 0.07752107828855515, 0.07694832980632782, 0.09701415151357651, 0.026812361553311348, 0.061326757073402405, -0.06237601488828659, 0.01336886640638113, 0.03301656246185303, 0.012288009747862816, 0.03493909537792206, 0.037963565438985825, 0.08241390436887741, -0.07920756936073303, 0.03219907730817795, -0.0725589320063591, 0.021918877959251404, 0.06912187486886978, -0.06209903955459595, -0.041387856006622314, -0.06651192158460617, 0.0871686115860939, -0.010435950942337513, -0.036500465124845505, -0.009548229165375233, 0.0022105504758656025, -0.053900718688964844, -0.025461861863732338, -0.06131903454661369, 0.09505777806043625, 0.08345639705657959, 0.053618140518665314, -0.026907119899988174, -0.0512971505522728, 0.09762802720069885, -0.06342840194702148, -0.08827998489141464, 0.0506659559905529, 0.06987887620925903, 0.0725930705666542, 0.0196632482111454, -0.007306782528758049, -0.09856217354536057, -0.02091752365231514, 0.049183811992406845, -0.04165145754814148, -0.03632931783795357, 0.053573187440633774, -0.031197750940918922], [0.004324428271502256, -0.09682100266218185, -0.01868392340838909, 0.03054109215736389, 0.05436934530735016, 0.017821932211518288, 0.042905014008283615, 0.07864009588956833, 0.05268540233373642, 0.04850786179304123, 0.09832537919282913, 0.02516576647758484, 0.07374797761440277, 0.060214072465896606, -0.08225879073143005, -0.06676167249679565, 0.07088620215654373, 0.09582344442605972, -0.03938464820384979, 0.038673825562000275, 0.08192485570907593, -0.08087224513292313, -0.0361449234187603, 0.08719054609537125, -0.01760450378060341, 0.03357823193073273, 0.032892562448978424, 0.02162308432161808, 0.09253232926130295, 0.08720343559980392, -0.0834694653749466, 0.025363920256495476, 0.02767249010503292, 0.015054193325340748, -0.03237824887037277, 0.04070548713207245, -0.03181322291493416, 0.048358719795942307, 0.025996118783950806, 0.06358235329389572, -0.008805337361991405, -0.027338044717907906, 0.07752079516649246, 0.06393168121576309, 0.03570401668548584, -0.09529510140419006, 0.041419439017772675, 0.008958272635936737, -0.09909094125032425, -0.09792494028806686, 0.015331385657191277, -0.018369527533650398, 0.07678571343421936, 0.006674605421721935, -0.0791822075843811, -0.016992835327982903, 0.05337938293814659, 0.02282225899398327, 0.08945207297801971, 0.07433171570301056, 0.030595077201724052, 0.09142597764730453, -0.061106689274311066, 0.00800321064889431, -0.032844219356775284, -0.024558067321777344, 0.013062621466815472, 0.06831048429012299, -0.019428759813308716, 0.039885181933641434, 0.0778142437338829, 0.014074056409299374, -0.013285517692565918, -0.0739877000451088, -0.05575023218989372, 0.0020831215661019087, 0.020332973450422287, -0.013209890574216843, -0.0899495854973793, -0.05934066325426102, 0.0584370382130146, -0.005320136435329914, -0.0989646390080452, -0.09114917367696762, -0.045473627746105194, -0.08966941386461258, 0.07536554336547852, 0.07617110759019852, -0.010513868182897568, 0.0666564479470253, 0.01784486323595047, -0.039132095873355865, -0.006853331811726093, -0.03401318937540054, -0.03232010081410408, 0.06152019649744034, -0.004514445085078478, 0.045285116881132126, 0.05178919807076454, 0.060589347034692764], [0.00678460206836462, 0.005529251415282488, 0.05815313011407852, -0.05354558303952217, 0.0024894289672374725, 0.027713855728507042, -0.007821536622941494, -0.01743193157017231, 0.050251614302396774, 0.04230073094367981, -0.07193800061941147, -0.024344490841031075, -0.05986439064145088, 0.05327158421278, 0.0153561532497406, 0.010893349535763264, 0.048583779484033585, -0.007200506515800953, 0.0655256062746048, -0.016326382756233215, 0.05505811423063278, 0.04439844191074371, 0.08753437548875809, -0.014334840700030327, 0.014155499637126923, -0.09937858581542969, 0.08319015055894852, 0.006480310577899218, 0.03330422565340996, -0.09015732258558273, -0.015232550911605358, 0.084562748670578, -0.048169177025556564, 0.028176791965961456, 0.05763239413499832, 0.05466726794838905, 0.09255298972129822, -0.050244517624378204, -0.07099845260381699, 0.02946179173886776, -0.01459699496626854, -0.0714259147644043, -0.09401065856218338, 0.08133391290903091, -0.054435644298791885, 0.08649058640003204, -0.06951094418764114, 0.04932470619678497, -0.038173701614141464, 0.07211232930421829, 0.09584386646747589, 0.08434279263019562, -0.050577469170093536, -0.028862662613391876, -0.06401152908802032, -0.016376737505197525, 0.03026541881263256, 0.06909625977277756, -0.04824409633874893, 0.06474003940820694, -0.02105621248483658, 0.0022215028293430805, -0.06613156199455261, 0.022825611755251884, 0.013414467684924603, 0.0657387375831604, 0.07209204882383347, 0.08106379210948944, 0.057209499180316925, -0.0016931022983044386, 0.06079254671931267, 0.07536935061216354, 0.07136673480272293, 0.06473661959171295, -0.03268778696656227, 0.00537059735506773, -0.040757957845926285, 0.051038239151239395, -0.022198263555765152, -0.09877981245517731, -0.04262138158082962, -0.07603742927312851, 0.020865680649876595, 0.0736427903175354, -0.005353517830371857, -0.0409451425075531, -0.030872192233800888, -0.0801386758685112, 0.04538166895508766, 0.05721042677760124, -0.07241817563772202, 0.07083867490291595, 0.03442497178912163, -0.08745580911636353, 0.06015302240848541, -0.0057941400445997715, -0.05147041007876396, 0.04936232417821884, -0.052001841366291046, 0.019219418987631798], [0.095851369202137, 0.05590936914086342, 0.06490029394626617, -0.053967200219631195, -0.05804683268070221, 0.03319256752729416, 0.0003290523309260607, 0.07285889238119125, 0.08800103515386581, 0.07874763756990433, 0.028964489698410034, -0.06528683006763458, 0.007618416100740433, 0.01382466685026884, -0.06096900254487991, -0.012492485344409943, 0.012293683364987373, -0.07068052142858505, 0.010002736002206802, -0.08956557512283325, 0.05230424553155899, -0.05190766602754593, 0.036740660667419434, -0.013916611671447754, -0.05899234488606453, 0.025017408654093742, -0.07408997416496277, 0.061748962849378586, 0.006244197487831116, 0.02046847902238369, 0.06320922076702118, 0.06708680838346481, 0.07960203289985657, 0.06620775163173676, -0.01561584323644638, -0.0007767100469209254, -0.08542633801698685, 0.02342809922993183, 0.002365012187510729, 0.07382867485284805, 0.07236208766698837, -0.015172312036156654, -0.05254778638482094, 0.06805102527141571, 0.02126803807914257, -0.05232558399438858, -0.06340572983026505, -0.09853576868772507, 0.04589638486504555, 0.03824446722865105, 0.028091654181480408, 0.051521990448236465, -0.058269232511520386, 0.09543066471815109, 0.05015823245048523, 0.03284742310643196, -0.02057681977748871, -0.08727815002202988, 0.02091352641582489, 0.020426718518137932, 0.01685534417629242, 0.04939897358417511, -0.04680498689413071, 0.0923396497964859, 0.004271280020475388, -0.09111324697732925, -0.04629082232713699, 0.023916129022836685, 0.06994204968214035, -0.04218105599284172, -0.04527237266302109, -0.003966993652284145, -0.09575843811035156, -0.03567250818014145, -0.06257794797420502, 0.08650460839271545, -0.061861440539360046, -0.008596336469054222, 0.07674966752529144, -0.01921207457780838, 0.08302602916955948, -0.052177220582962036, -0.008636882528662682, 0.0054758572950959206, -0.07063024491071701, 0.0584881417453289, -0.002629090566188097, -0.09917856007814407, 0.04085826873779297, -0.04044612869620323, -0.05377473682165146, 0.038975585252046585, 0.02543453499674797, -0.061563897877931595, 0.03666393831372261, 0.012892673723399639, -0.03211260586977005, 0.06688550114631653, 0.0024695878382772207, 0.04585284739732742], [-0.021592609584331512, 0.033395301550626755, 0.02447386458516121, 0.07726138830184937, -0.08839794248342514, -0.02279679849743843, 0.05491684004664421, -0.04611869528889656, -0.073055699467659, 0.06543834507465363, 0.05222076177597046, 0.09291216731071472, 0.037896864116191864, -0.028084952384233475, -0.07766594737768173, -0.0645291656255722, 0.029991386458277702, -0.018990397453308105, 0.09561055153608322, 0.06458304822444916, 0.009020674973726273, 0.09549764543771744, 0.011337067931890488, 0.021633125841617584, 0.09697604924440384, -0.010983461514115334, 0.030675936490297318, -0.004900273401290178, 0.09368746727705002, 0.04434480890631676, -0.032992638647556305, -0.08921148627996445, 0.013691565953195095, 1.3479486369760707e-05, 0.043842364102602005, 0.04163478687405586, -0.09059940278530121, -0.08316732943058014, -0.027133144438266754, -0.08645361661911011, 0.005200806073844433, 0.060412656515836716, 0.0975036472082138, -0.00603722408413887, -0.07665248215198517, 0.06855083256959915, -0.06932788342237473, 0.0708751454949379, -0.03574301674962044, -0.04832207038998604, 0.06339116394519806, 0.02933076210319996, -0.02718067169189453, 0.06259912997484207, -0.08833541721105576, -0.028065042570233345, -0.05362474545836449, -0.07406605780124664, -0.08690352737903595, -0.06353618204593658, 0.03449965640902519, -0.05484875291585922, 0.09094072878360748, -0.01819460839033127, 0.02172108367085457, 0.07260967046022415, 0.02314266934990883, -0.06390471756458282, -0.01416191179305315, -0.017457690089941025, -0.046298619359731674, 0.013813162222504616, -0.07534205913543701, 0.02028643898665905, -0.0677221342921257, 0.07783058285713196, -0.012775924056768417, -0.08959941565990448, -0.07753587514162064, -0.08882120996713638, 0.00766062643378973, 0.005692467559129, -0.050048403441905975, -0.04980983957648277, -0.0756908431649208, -0.0581178292632103, -0.037843313068151474, -0.06014450639486313, 0.03378000482916832, 0.040645524859428406, -0.04372953996062279, 0.0036377154756337404, -0.03020736388862133, -0.028727611526846886, -0.011439415626227856, -0.02644871175289154, 0.0827445313334465, 0.005484047345817089, 0.051890306174755096, 0.081091970205307], [0.007224689703434706, 0.012618095614016056, 0.029796823859214783, 0.08925417810678482, -0.08005309104919434, 0.016827771440148354, 0.02592914178967476, 0.04226652532815933, 0.0065958560444414616, 0.08317597210407257, 0.08883786201477051, -0.09145203977823257, -0.06696692109107971, 0.05008740723133087, 0.03648439794778824, 0.054070133715867996, 0.0740978941321373, 0.09652967751026154, 0.04521661251783371, 0.012113635428249836, 0.08989369869232178, -0.010528644546866417, -0.018949422985315323, -0.06985490024089813, 0.09131374210119247, -0.05713777244091034, -0.0166688933968544, 0.09755107015371323, -0.08909514546394348, 0.05018928647041321, -0.001216729637235403, 0.03695470094680786, 0.07240860164165497, 0.07883661240339279, 0.061662137508392334, -0.0007623296114616096, -0.04874108359217644, 0.06919616460800171, 0.04948259890079498, -0.0006351890624500811, 0.014149635098874569, 0.03288045525550842, -0.06938385963439941, -0.07633243501186371, -0.08436048775911331, 0.012898488901555538, -0.05739867314696312, 0.0072072958573699, -0.07957164198160172, 0.06762062013149261, 0.06418634206056595, -0.02331402152776718, 0.018183009698987007, -0.007984834723174572, -0.01877657137811184, 0.07374940067529678, 0.08879829943180084, 0.07094250619411469, -0.08113548904657364, 0.07183635979890823, 0.05884202569723129, -0.021217752248048782, 0.04642537981271744, 0.012054510414600372, -0.06863314658403397, -0.0612778402864933, -0.029289603233337402, 0.05582836642861366, -0.0036368027795106173, -0.04408102482557297, 0.09359858185052872, 0.06851091235876083, 0.03418414667248726, -0.09151370823383331, 0.04094664752483368, 0.09603702276945114, -0.027435049414634705, -0.04575062170624733, -0.0001605172292329371, -0.02237687073647976, 0.07850584387779236, 0.09432156383991241, 0.08327075093984604, -0.040481291711330414, -0.07014888525009155, 0.005277266260236502, -0.07662330567836761, -0.005561717785894871, -0.05160246044397354, 0.04624606668949127, 0.09189918637275696, 0.05756495147943497, 0.09548312425613403, 0.06277602165937424, -0.09577880799770355, 0.019265618175268173, 0.07931914180517197, 0.05438225716352463, -0.07858792692422867, -0.09843306988477707], [0.0018877586117014289, 0.0959576815366745, -0.0025857638102024794, -0.0407363697886467, -0.0765644982457161, 0.025223709642887115, 0.08294593542814255, -0.04258287698030472, 0.0900532454252243, 0.0615801140666008, 0.09846208989620209, 0.057146307080984116, -0.0198193471878767, 0.08220649510622025, -0.06928975135087967, 0.07270747423171997, 0.02592107281088829, 0.08434589207172394, -0.05290352553129196, -0.0812772735953331, -0.01587752066552639, -0.011669213883578777, -0.06926409900188446, -0.09852784126996994, -0.05764481797814369, 0.09610668569803238, 0.09635292738676071, -0.09500625729560852, -0.04519054666161537, 0.044089287519454956, 0.06911293417215347, 0.0768873542547226, -0.08479435741901398, 0.06014849990606308, -0.044579554349184036, -0.05230201408267021, 0.08752213418483734, 0.04461704194545746, 0.09394312649965286, -0.0007727564661763608, 0.018568752333521843, -0.05216458812355995, -0.019293861463665962, 0.09845812618732452, -0.02104528434574604, 0.03474537655711174, -0.02105705253779888, -0.08377517759799957, -0.0816057026386261, 0.09843485057353973, -0.039116647094488144, -0.06919600069522858, -0.08011525869369507, 0.07723015546798706, 0.00627829460427165, 0.008231700398027897, -0.07744219154119492, -0.058765094727277756, -0.08037696033716202, 0.02414250560104847, -0.04184054583311081, -0.02270539291203022, 0.0021581859327852726, -0.0013631746405735612, 0.01332238968461752, -0.03467731550335884, -0.05707313492894173, -0.026676559820771217, 0.03183966130018234, -0.07552268356084824, -0.020255642011761665, -0.052032921463251114, 0.0270835030823946, -0.07034727185964584, 0.08053850382566452, 0.08230613172054291, -0.0827002078294754, 0.020210398361086845, -0.09893116354942322, 0.028621012344956398, 0.07528568804264069, -0.008694607764482498, 0.058885157108306885, -0.03714236617088318, -0.04725208505988121, 0.028481099754571915, -0.06867751479148865, -0.039557404816150665, 0.07132497429847717, 0.049243323504924774, 0.07399071753025055, -0.01497010700404644, 0.09071391820907593, 0.06959083676338196, 0.04938562959432602, 0.056645408272743225, -0.08629774302244186, -0.054299063980579376, 0.0011742498027160764, 0.01710635982453823], [0.09741917997598648, 0.08703528344631195, -0.04324432834982872, 0.06507685780525208, 0.03760513663291931, -0.049565188586711884, -0.09907656162977219, 0.040816742926836014, 0.014556539244949818, 0.024066371843218803, 0.07027462124824524, -0.031839605420827866, -0.03916967660188675, 0.016288114711642265, -0.03856447711586952, -0.05959968641400337, -0.10022605210542679, -0.08356636017560959, -0.008258078247308731, 0.07959260791540146, -0.06242479756474495, 0.019486885517835617, 0.051934078335762024, 0.01479706447571516, -0.003173588076606393, 0.07321412116289139, -0.09465380012989044, 0.08401330560445786, 0.0824110358953476, 0.08542602509260178, 0.07435658574104309, -0.007076827809214592, -0.07984547317028046, -0.06277557462453842, -0.08519760519266129, -0.06267358362674713, -0.08912976086139679, 0.06901172548532486, 0.04421420395374298, -0.043458305299282074, -0.00112421833910048, 0.01015718188136816, 0.002361179795116186, 0.06988351792097092, 0.04389383643865585, -0.0763787180185318, 0.09533198922872543, -0.03869183361530304, -0.04210823401808739, 0.100166916847229, -0.06175153702497482, -0.0243762768805027, 0.00893987063318491, -0.08947279304265976, 0.07363971322774887, 0.07840614765882492, -0.09580036997795105, -0.015943994745612144, 0.017370285466313362, 0.05994104966521263, 0.041205234825611115, -0.009359592571854591, 0.08774774521589279, -0.021425247192382812, 0.005244363099336624, -0.06388179957866669, -0.058433402329683304, -0.02891436032950878, -0.09077980369329453, -0.013580395840108395, 0.020850373432040215, -0.06942787021398544, -0.08373775333166122, 0.08825604617595673, -0.043239105492830276, -0.07478713244199753, -0.024532245472073555, -0.04754406958818436, -0.0959678441286087, -0.013153363950550556, -0.04545905813574791, -0.03096710331737995, 0.04487638548016548, 0.004211893305182457, 0.08386188745498657, -0.09972111880779266, 0.0874168872833252, 0.05465669184923172, -0.027955900877714157, 0.06961534172296524, 0.09430103003978729, 0.010249606333673, 0.027968328446149826, -0.028048662468791008, -0.007051981519907713, -0.0052816737443208694, -0.05076202377676964, 0.017009342089295387, -0.042962074279785156, 0.051292963325977325], [0.01590673252940178, -0.0774172991514206, 0.08977143466472626, 0.02978230081498623, 0.047049302607774734, -0.09887665510177612, 0.0874946266412735, 0.030318500474095345, -0.007164661306887865, -0.0657503753900528, 0.019754981622099876, 0.03738940879702568, 0.030573775991797447, 0.07346431910991669, -0.08979448676109314, -0.07753537595272064, 0.09740302711725235, 0.08519434928894043, -0.007959658280014992, 0.055406730622053146, 0.07007922977209091, 0.07175936549901962, -0.04727498069405556, -0.03683188557624817, -0.05376822128891945, -0.02558956667780876, 0.09284862130880356, -0.0021162668708711863, -0.07925494015216827, 0.07617900520563126, 0.03822324797511101, 0.014278474263846874, -0.051671918481588364, 0.07263100892305374, -0.08144641667604446, 0.05019442364573479, -0.01297791674733162, -0.035951681435108185, -0.0906970426440239, 0.01307530328631401, -0.04752334579825401, -0.04012909159064293, 0.05622196942567825, -0.06433486938476562, 0.09490107744932175, 0.08157030493021011, 0.07103566825389862, 0.012084431014955044, -0.07521287351846695, -0.02304520085453987, -0.04702722653746605, -0.013881204649806023, -0.09291590750217438, -0.044422440230846405, -0.016297722235322, -0.0007026299135759473, -0.025866976007819176, 0.02145667001605034, -0.05299462750554085, -0.06775407493114471, -0.07305869460105896, 0.09264924377202988, 0.05916810780763626, -0.08836633712053299, -0.0855574756860733, -0.0055601210333406925, -0.05258580669760704, 0.034808509051799774, -0.03503711521625519, 0.009655534289777279, -0.026450039818882942, -0.07839886844158173, -0.009983810596168041, 0.09051752090454102, 0.07648565620183945, -0.06894311308860779, 0.028442680835723877, 0.08664669096469879, 0.005252826027572155, -0.010476965457201004, 0.08313142508268356, -1.4493712114926893e-05, -0.017381824553012848, 0.09292455017566681, 0.042355407029390335, -0.06259371340274811, 0.0750286653637886, 0.08411646634340286, 0.08128747344017029, -0.04550980031490326, -0.03718595579266548, 0.06479388475418091, 0.07304629683494568, -0.08315352350473404, 0.08298082649707794, 0.04864529147744179, 0.02888493239879608, -0.028144074603915215, -0.013473241589963436, 0.05900534614920616], [0.016394900158047676, 0.07935519516468048, -0.061681587249040604, -0.03544366732239723, -0.011888403445482254, 0.040683723986148834, 0.03556734696030617, 0.053100019693374634, -0.05490358546376228, -0.017885567620396614, 0.03911374509334564, -0.04422152042388916, 0.07150941342115402, 0.07084747403860092, 0.05584292113780975, 0.05372892692685127, 0.06069459393620491, -0.08440038561820984, -0.030103081837296486, 0.07479041069746017, -0.04211904853582382, 0.008234325796365738, -0.03392978385090828, -0.003892278764396906, -0.01775483973324299, -0.02800806425511837, 0.017286915332078934, 0.09186211973428726, 0.0023700434248894453, -0.028488419950008392, 0.08130336552858353, -0.04631159082055092, 0.026340266689658165, 0.0017438055947422981, 0.029184989631175995, 0.05886159464716911, 0.026494359597563744, -0.00669466657564044, 0.04251384362578392, 0.014462236315011978, -0.01715766452252865, 0.021586617454886436, -0.050997503101825714, 0.008764601312577724, -0.059309400618076324, 0.045087601989507675, 0.09624480456113815, -0.031902335584163666, -0.08878150582313538, 0.08239356428384781, -0.05116055905818939, 0.06616094708442688, 0.06935855746269226, -0.03483041003346443, 0.002843329682946205, -0.042424846440553665, 0.08387647569179535, -0.0715750977396965, 0.011827892623841763, -0.05598444119095802, 0.07902897149324417, 0.048773881047964096, 0.08654677122831345, -0.04595889151096344, 0.03323250263929367, -0.086044542491436, 0.059970058500766754, -0.03278026729822159, 0.033510442823171616, 0.048502061516046524, -0.0633588433265686, 0.055771131068468094, -0.0410439632833004, 0.03843920677900314, -0.03472808375954628, 0.004333463031798601, 0.009471096098423004, 0.08589727431535721, 0.028207719326019287, 0.017025338485836983, -0.019549712538719177, -0.10027705132961273, -0.04415629059076309, -0.03377177566289902, 0.037272389978170395, -0.007071889471262693, 0.031829655170440674, -0.05129915103316307, -0.09904985874891281, 0.03746683895587921, 0.05423520505428314, 0.05325482413172722, 0.08242519199848175, -0.09825417399406433, 0.029596181586384773, 0.07890129834413528, -0.051409702748060226, 0.0560692735016346, 0.05155372992157936, 0.02048271894454956], [-0.09563762694597244, 0.046586766839027405, 0.08905413746833801, -0.013982700183987617, 0.03977125138044357, 0.07557957619428635, -0.07471973448991776, -0.09295084327459335, 0.037882205098867416, -0.0032192766666412354, -0.04662047326564789, -0.09340553730726242, -0.06248761713504791, 0.02537267468869686, -0.018831424415111542, -0.08748907595872879, -0.010090826079249382, -0.005240416154265404, -0.0926131159067154, 0.05962171033024788, 0.1008668914437294, 0.07004690170288086, -0.08979728072881699, -0.061526354402303696, -0.07834048569202423, 0.0841655507683754, -0.04725019633769989, 0.07510486245155334, -0.041657306253910065, 0.03983747214078903, 0.07127577066421509, -0.04815114289522171, 0.04404139518737793, 0.049592528492212296, 0.024209918454289436, -0.03850032016634941, -0.07715365290641785, 0.019406097009778023, -0.0898183137178421, -0.03400968015193939, 0.0160300862044096, 0.06170833855867386, 0.07793167233467102, -0.08388005942106247, -0.04414273425936699, -0.008791429921984673, -0.0020490973256528378, -0.024724118411540985, -0.0648953765630722, -0.02299933321774006, -0.021240992471575737, -0.06002487242221832, 0.03273167461156845, -0.07176888734102249, -0.08629767596721649, 0.01763136126101017, -0.049613337963819504, 0.0871177613735199, -0.06565254926681519, -0.06930453330278397, 0.09231215715408325, 0.01732618547976017, 0.0953790470957756, 0.08969783782958984, 0.0988135039806366, 0.07779863476753235, 0.05865435302257538, -0.06040696054697037, -0.028492173179984093, -0.010666445828974247, 0.000351661816239357, 0.07841836661100388, 0.03094526380300522, -0.0933460220694542, 0.026475301012396812, 0.0052114264108240604, 0.045538198202848434, -0.04228977486491203, 0.08333100378513336, -0.07014570385217667, 0.0793212503194809, 0.09116584807634354, -0.081495001912117, -0.06893090903759003, 0.06668515503406525, -0.07904715090990067, -0.007272894028574228, -0.05837761610746384, 0.0514216385781765, 0.09478221833705902, 0.08183172345161438, 0.08606532961130142, 0.05521804466843605, -0.08427047729492188, 0.02709447592496872, 0.09163550287485123, 0.05614681541919708, -0.06308864802122116, 0.04360516741871834, 0.022598177194595337], [-0.06353740394115448, -0.08785943686962128, -0.05598687753081322, -0.07920140773057938, -0.03428253158926964, 0.009201547130942345, -0.050054170191287994, -0.02637377753853798, -0.033505216240882874, 0.04678061977028847, -0.07800956815481186, 0.09737404435873032, 0.09650442749261856, 0.08301956206560135, 0.07489552348852158, -0.010258590802550316, -0.05011839047074318, 0.09391424804925919, -0.09487882256507874, -0.09707310050725937, 0.08278714120388031, -0.040200840681791306, 0.06615102291107178, -0.05698009580373764, -0.0677911639213562, -0.07166929543018341, -0.0027623772621154785, -0.00031278133974410594, -0.035424042493104935, 0.021590998396277428, 0.0599755235016346, -0.04408775269985199, 0.05855449289083481, -0.03845851495862007, 0.03019816242158413, -0.03294379264116287, -0.05077236145734787, 0.0959092378616333, -0.0654062032699585, 0.023984380066394806, -0.09333101660013199, 0.054551638662815094, 0.06731712073087692, -0.0961541086435318, -0.06641552597284317, -0.09703386574983597, 0.023472964763641357, -0.09274324774742126, 0.03905421122908592, 0.09394266456365585, -0.01342227216809988, 0.04907815158367157, -0.07928239554166794, 0.03264296427369118, -0.013838554732501507, 0.05856311321258545, 0.10171209275722504, -0.07221804559230804, -0.01413587387651205, -0.06141665205359459, 0.041287861764431, 0.08935727924108505, -0.01432710513472557, 0.09167389571666718, 0.06674670428037643, -0.012789096683263779, 0.055340442806482315, 0.08437314629554749, 0.00595930777490139, -0.0850154235959053, 0.026444176211953163, -0.090115025639534, 0.007369281258434057, -0.06666088849306107, 0.09443377703428268, -0.05681699141860008, -0.04768859222531319, 0.06496630609035492, -0.06615202128887177, 0.09817522019147873, 0.016775254160165787, 0.05401504412293434, -0.060207709670066833, -0.005476594902575016, 0.045093290507793427, 0.050051551312208176, -0.009424654766917229, 0.08545780926942825, 0.04172726720571518, -0.02785797230899334, -0.09670887887477875, 0.08185507357120514, 0.07656164467334747, -0.05435793474316597, 0.004518709145486355, 0.04677915945649147, 0.07640792429447174, -0.039874546229839325, 0.04552662745118141, -0.05672557279467583], [0.007710050791501999, -0.02149171382188797, 0.020158667117357254, 0.055940818041563034, -0.0041097369976341724, 0.07613009959459305, -0.038120441138744354, -0.05758862942457199, 0.08121944218873978, 0.002863440429791808, -0.027007266879081726, 0.025951754301786423, 0.023249156773090363, -0.017421063035726547, -0.051807206124067307, -0.05127554014325142, -0.018840381875634193, 0.011107205413281918, -0.0914921909570694, 0.07639271765947342, 0.0010684854350984097, -0.0007834466523490846, 0.023110130801796913, 0.015259171836078167, 0.02389078214764595, 0.0007838443270884454, -0.010805127210915089, -0.0647088885307312, 0.04745558276772499, 0.09163502603769302, 0.0654468834400177, -0.09361398965120316, 0.06774809956550598, 0.03300224989652634, -0.08925875276327133, 0.09228859841823578, 0.08209403604269028, 0.08863990008831024, -0.08677750825881958, -0.030159451067447662, 0.007434810046106577, -0.05308162048459053, -0.0022778629790991545, -0.019116556271910667, 0.03202815726399422, -0.06578442454338074, -0.009834052063524723, 0.08428172767162323, 0.06516461074352264, 0.022763382643461227, -0.05322776734828949, -0.0582987405359745, 0.08838709443807602, 0.012610825709998608, -0.007340670097619295, -0.08283508569002151, -0.03305886685848236, -0.09691701829433441, 0.04532584920525551, 0.010054836980998516, -0.01936100423336029, 0.03206709399819374, 0.017745092511177063, 0.023133788257837296, 0.07524020224809647, -0.05493992939591408, -0.009617429226636887, 0.0850607305765152, -0.007393734995275736, 0.0019715840462595224, 0.06081244349479675, 0.008019620552659035, -0.007506201975047588, 0.05287474766373634, 0.03594818711280823, -0.09848044067621231, 0.03912992775440216, -0.019874688237905502, 0.015977097675204277, 0.006937019992619753, 0.06647239625453949, -0.08656121045351028, 0.04162813723087311, 0.05065140500664711, -0.07320967316627502, -0.010715903714299202, 0.051734331995248795, 0.014642789959907532, 0.06428561359643936, 0.08196894079446793, 0.08757317811250687, 0.09953243285417557, 0.09702447056770325, -0.01406361535191536, -0.020984100177884102, -0.07330026477575302, -0.022162852808833122, 0.02749982662498951, 0.051680441945791245, 0.007915082387626171], [0.06127256527543068, 0.08802073448896408, 0.06493102759122849, 0.0215745959430933, -0.005621304735541344, -0.06646627187728882, 0.05876074731349945, -0.04372374713420868, -0.004012503195554018, 0.07773816585540771, 0.09980221837759018, -0.011446944437921047, 0.07075528800487518, 0.07934490591287613, -0.06223173066973686, -0.01555316336452961, 0.033927787095308304, 0.06471086293458939, -0.0734718069434166, -0.03730878233909607, 0.07436832040548325, 0.07653241604566574, -0.04273715987801552, 0.058988723903894424, 0.007813266478478909, -0.04286756366491318, 0.045912791043519974, 0.07907415181398392, -0.07328126579523087, -0.06696531921625137, 0.0751902312040329, 0.004241587594151497, 0.05194472894072533, -0.08008694648742676, -0.07693018019199371, 0.0705476850271225, 0.001447727787308395, 0.005465710069984198, -0.03907795250415802, 0.05787133798003197, 0.022695599123835564, -0.0967278927564621, 0.05194684863090515, 0.012804880738258362, -0.03880992904305458, 0.0729471892118454, 0.03801541030406952, 0.042434610426425934, -0.07656025886535645, 0.054649896919727325, -0.08453134447336197, -0.039623960852622986, 0.03065337985754013, -0.051737938076257706, 0.05893606320023537, 0.05509798228740692, 0.017162293195724487, 0.07080201059579849, -0.003910146653652191, -0.0328872948884964, 0.06622910499572754, -0.0008955582743510604, -0.059226103127002716, -0.04003891348838806, 0.03048977069556713, -0.009007064625620842, -0.01960761845111847, 0.03646338731050491, -0.09301675856113434, -0.03682498261332512, -0.09598773717880249, 0.004334073048084974, 0.028632648289203644, 0.015757283195853233, 0.010951400734484196, 0.025146393105387688, 0.018893221393227577, 0.02336982451379299, -0.03741932287812233, 0.08722662925720215, -0.032298628240823746, 0.04864693433046341, 0.09570945054292679, 0.011881285347044468, -0.053945206105709076, -0.023371148854494095, -0.09109941869974136, -0.06920793652534485, 0.009295940399169922, 0.08557368069887161, -0.01546145137399435, -0.005070225335657597, 0.007141736336052418, -0.05154823884367943, -0.04763380438089371, -0.07637401670217514, -0.006943627260625362, 0.02800070494413376, 0.07124516367912292, -0.09822209179401398], [0.050280891358852386, -0.083330437541008, 0.04832984879612923, -0.06984551250934601, -0.06567125022411346, -0.08288033306598663, 0.08547890931367874, -0.0836232528090477, 0.06729397177696228, 0.0864792987704277, -0.06414510309696198, 0.07325111329555511, 0.09108569473028183, -0.09781192988157272, -0.06479969620704651, 0.07520155608654022, -0.06102577969431877, 0.06547434628009796, -0.0571417473256588, -0.016651371493935585, -0.0632508248090744, 0.014437676407396793, -0.060788843780756, -0.05616319924592972, 0.017978401854634285, -0.002316064201295376, -0.005918240174651146, -0.04702376574277878, 0.058439627289772034, 0.09137512743473053, 0.05983320623636246, -0.07821206748485565, -0.0816582441329956, -0.06363921612501144, 0.006200768984854221, -0.007285411935299635, 0.015005523338913918, 0.03236958384513855, 0.06128447875380516, 0.04900530353188515, 0.02552361972630024, -0.03041294403374195, -0.00777876190841198, -0.03254827857017517, -0.07209248095750809, -0.011981932446360588, -0.09310195595026016, 0.075812928378582, 0.0700087696313858, 0.005691728554666042, -0.06639480590820312, -0.029382716864347458, 0.024459710344672203, 0.08150360733270645, 0.08310145884752274, 0.03124035894870758, 0.045472100377082825, -0.026156412437558174, -0.06701286882162094, -0.051496658474206924, -0.03526049479842186, 0.012836847454309464, 0.0018997332081198692, 0.06735803186893463, 0.08654402196407318, -0.07078251242637634, 0.04144233465194702, -0.03524143993854523, 0.049260810017585754, -0.09502378851175308, -0.031604018062353134, 0.05506236478686333, -0.0215813796967268, -0.05111498758196831, 0.07439766079187393, -0.06427550315856934, -0.06713231652975082, -0.00687453243881464, 0.05050411447882652, -0.0651048943400383, 0.067341648042202, -0.09661109745502472, 0.07742587476968765, -0.07608350366353989, 0.005696064792573452, -0.06700052320957184, -0.08489914983510971, 0.09749336540699005, 0.04038292542099953, -0.08246522396802902, -0.0862923115491867, -0.004356116522103548, 0.06488821655511856, 0.05566708371043205, 0.004914106335490942, 0.011990039609372616, -0.07428368180990219, 0.06024634465575218, -0.010736244730651379, -0.08037728816270828], [-0.07457470148801804, -0.020433122292160988, -0.06232934445142746, -0.03213242441415787, 0.09780332446098328, 0.0048251673579216, -0.05579930171370506, 0.077641062438488, 0.09291709959506989, 0.00909638311713934, -0.0028327354229986668, 0.013129494152963161, -0.020143667235970497, 0.01871950551867485, 0.03015243634581566, 0.07581157237291336, 0.04623628780245781, 0.018973466008901596, -0.00893647875636816, 0.07654237002134323, 0.04138732701539993, 0.03276757895946503, 0.06807175278663635, 0.02050536498427391, -0.02622823230922222, 0.024043690413236618, -0.04838266596198082, 0.06144202500581741, -0.0322970412671566, 0.02165919356048107, 0.028708741068840027, -0.07327363640069962, -0.08432000130414963, 0.06746308505535126, -0.09955807775259018, -0.034867752343416214, 0.05879693850874901, 0.025893110781908035, 0.09622263163328171, -0.09661010652780533, 0.035387538373470306, -0.03701312839984894, 0.04078175127506256, 0.07347458600997925, 0.06738828122615814, -0.09990067034959793, -0.03351762890815735, 0.06896040588617325, 0.03739209845662117, -0.08056311309337616, 0.060372818261384964, 0.0686706006526947, -0.04937450587749481, -0.02990783378481865, -0.07261165231466293, -0.07433168590068817, -0.045728880912065506, -0.023042339831590652, 0.0010917712934315205, 0.06135440617799759, 0.09116416424512863, 0.011750482022762299, 0.0601535364985466, 0.05875576287508011, 0.0206577368080616, -0.046780120581388474, 0.03925079107284546, 0.014781974256038666, -0.07949704676866531, -0.03141878917813301, -0.0388609878718853, 0.03527987003326416, -0.04622270166873932, 0.04281260445713997, -0.005093131214380264, -0.01594645343720913, 0.03969908133149147, 0.06061301752924919, 0.03165625035762787, 0.017557306215167046, -0.00471471669152379, -0.017613345757126808, 0.048409271985292435, -0.020448949187994003, 0.0745420753955841, 0.06170284003019333, -0.09736088663339615, 0.017790140584111214, -0.01735028252005577, 0.030549976974725723, 0.07763536274433136, 0.004354090429842472, -0.04896910488605499, -0.016692571341991425, -0.04237477108836174, 0.05591031536459923, 0.05517689883708954, 0.0025124254170805216, -0.08358210325241089, 0.0024125545751303434], [0.0981292873620987, -0.004666758701205254, 0.03128742426633835, -0.07500754296779633, 0.025160569697618484, 0.06958119571208954, 0.006370167247951031, -0.014382037334144115, 0.028800223022699356, -0.09028585255146027, -0.0005782188381999731, -0.030756724998354912, 0.09588532894849777, -0.014987527392804623, 0.07971678674221039, -0.032046593725681305, -0.048230331391096115, 0.04083814471960068, 0.030749384313821793, -0.09185627102851868, -0.050513237714767456, -0.01815778948366642, 0.05678708478808403, 0.05552008002996445, -0.05399595946073532, -0.08934260159730911, 0.08734588325023651, -0.03752705454826355, 0.04552553966641426, -0.0872248113155365, -0.011640502139925957, 0.02376461774110794, 0.07770740240812302, -0.06174950674176216, -0.07268902659416199, -0.042186979204416275, 0.0678652822971344, -0.04907500371336937, -0.09780904650688171, -0.03887438401579857, 0.08648320287466049, -0.09567166119813919, 0.026106862351298332, 0.083737812936306, 0.06119164824485779, 0.09062696248292923, -0.03130212053656578, 0.06904520839452744, -0.08405248820781708, 0.0516950748860836, 0.04481842741370201, 0.059057775884866714, 0.06444184482097626, 0.07120378315448761, 0.07212857902050018, -0.0841229185461998, 0.019932497292757034, -0.08800344914197922, 0.03263960778713226, -0.039986662566661835, -0.047373976558446884, -0.0006808951147831976, 0.01317203976213932, -0.06338899582624435, -0.09083656966686249, 0.08573471754789352, 0.025248289108276367, 0.09667045623064041, 0.028522206470370293, 0.06341306865215302, -0.0884370505809784, 0.000432374159572646, 0.09329235553741455, -0.002028397750109434, -0.041539695113897324, 0.017239555716514587, -0.07124385237693787, -0.06392615288496017, -0.055096615105867386, 0.021117914468050003, -0.08515191823244095, 0.05129656940698624, -0.048415809869766235, -0.08459504693746567, -0.06619201600551605, 0.007222810760140419, 0.051807623356580734, -0.01433700229972601, -0.09500490874052048, -7.929546700324863e-05, 0.09268051385879517, -0.054716914892196655, 0.01343089435249567, 0.02708367630839348, -0.05913649871945381, -0.07569817453622818, 0.04751301929354668, -0.050827883183956146, 0.0073677715845406055, 0.08570798486471176], [0.041319817304611206, 0.05598174035549164, 0.07282745838165283, -0.048651885241270065, 0.03127962350845337, 0.07717975974082947, -0.07238274812698364, 0.029466399922966957, -0.036675117909908295, -0.08341160416603088, -0.0805075615644455, 0.014716217294335365, -0.06986192613840103, 0.05240122601389885, -0.011964172124862671, -0.022151703014969826, -0.0768146961927414, 0.04415125772356987, -0.05588213726878166, 0.07147706300020218, -0.002849986543878913, -0.07341014593839645, 0.07631510496139526, 0.07667645812034607, -0.08741489797830582, -0.05786464363336563, -0.044122107326984406, -0.06904944777488708, 0.010995650663971901, -0.07168516516685486, 0.05311652272939682, -0.08148904889822006, -0.08897276222705841, 0.0029564378783106804, -0.0007308179046958685, 0.021210381761193275, -0.08209916204214096, 0.08731010556221008, -0.08084427565336227, 0.07489144057035446, 0.091524139046669, 0.026086822152137756, 0.02408081665635109, -0.05508750304579735, 0.030922794714570045, 0.082550048828125, -0.08680063486099243, 0.0843660980463028, -0.047304291278123856, 0.022022144868969917, 0.048340220004320145, 0.00533968536183238, -0.05890214070677757, -0.05932939052581787, 0.004046841058880091, -0.05542295053601265, -0.07554149627685547, -0.09431598335504532, 0.09896028786897659, -0.09246492385864258, -0.0702357068657875, 0.04005492106080055, 0.06417059898376465, 0.0978773757815361, 0.038890570402145386, 0.08991973102092743, -0.05525778979063034, 0.03515113890171051, 0.013738637790083885, -0.09104476124048233, 0.08877253532409668, 0.033679261803627014, 0.05099457874894142, 0.0546007938683033, 0.03295265883207321, -0.05174567177891731, 0.060301829129457474, 0.007950704544782639, -0.05015460401773453, 0.05719292163848877, -0.002789056394249201, 0.01927974261343479, 0.06561397016048431, 0.07513423264026642, -0.007208417169749737, 0.0039995876140892506, 0.005587045568972826, 0.04704466462135315, 0.03684108704328537, 0.06323406100273132, -0.030103718861937523, -0.04677003249526024, -0.022751616314053535, 0.05923091992735863, 0.08065612614154816, -0.04246518388390541, 0.0511305145919323, 0.0962381511926651, 0.07190621644258499, 0.008294221945106983], [-0.018825970590114594, 0.09708653390407562, 0.03248117119073868, -0.03831164911389351, 0.020624753087759018, -0.05703331530094147, -0.052449677139520645, 0.08474404364824295, 0.05134308710694313, -0.0677807405591011, -0.01589370332658291, -0.010051751509308815, -0.05078212171792984, -0.058640457689762115, 0.05931690335273743, 0.06410148739814758, -0.034894611686468124, -0.09633108228445053, 0.0763891190290451, -0.06331310421228409, -0.00591755798086524, -0.05345959588885307, -0.03919532522559166, 0.06545253098011017, 0.07988240569829941, -0.0467732809484005, 0.07464599609375, -0.05787551775574684, -0.08166404068470001, 0.027939973399043083, -0.005969277583062649, 0.05145882070064545, 0.07284276932477951, -0.035868704319000244, 0.022899925708770752, 0.034179944545030594, 0.0036121248267591, -0.01793520152568817, 0.028901100158691406, -0.0866175964474678, -0.007919319905340672, -0.06125088408589363, -0.09105215221643448, 0.08300740271806717, -0.06519341468811035, -0.040498193353414536, 0.0827251523733139, -0.06078660488128662, -0.01680782623589039, 0.08154165744781494, 0.04401083663105965, 0.05435602366924286, 0.03297022730112076, -0.008776242844760418, 0.07653540372848511, 0.08351276814937592, -0.009755930863320827, -0.03522416204214096, -0.041946325451135635, 0.008280600421130657, -0.07868903130292892, -0.08930737525224686, -0.09852354973554611, 0.048767607659101486, 0.01677149534225464, 0.04426245018839836, -0.0053550065495073795, -0.05877212807536125, 0.012371726334095001, -0.012715347111225128, -0.03035377897322178, 0.06188676133751869, -0.03792084380984306, 0.009878898970782757, -0.04714994505047798, -0.004780432675033808, 0.004546396434307098, -0.059502843767404556, 0.04256509244441986, -0.018025638535618782, -0.07005923986434937, 0.0681469514966011, -0.04663345590233803, 0.0887337401509285, -0.019065024331212044, 0.0912666991353035, -0.07084517180919647, -0.033128876239061356, 0.09277807921171188, -0.012969198636710644, -0.04127063229680061, 0.03716373071074486, -0.06918155401945114, 0.06299415230751038, -0.03447739779949188, 0.08432266861200333, 0.06843526661396027, 0.08427315950393677, -0.01717201992869377, -0.016999557614326477], [-0.009045732207596302, 0.04831991717219353, -0.09920044243335724, 0.05534501373767853, 0.08056417852640152, -0.04388010501861572, 0.07395859062671661, 0.07309380918741226, 0.047448527067899704, 0.00531108770519495, 0.07727761566638947, 0.04084722697734833, -0.012934095226228237, 0.0796528160572052, -0.08214391767978668, -0.03991939499974251, -0.09354159235954285, 0.06006867066025734, 0.06620228290557861, 0.09538466483354568, -0.09773118048906326, 0.02890923246741295, -0.05087662115693092, -0.049867283552885056, -0.06357870995998383, -0.09631796181201935, -0.029296524822711945, 0.056299347430467606, 0.0011481995461508632, -0.04818792641162872, -0.016311591491103172, -0.05587988346815109, -0.040625039488077164, 0.08457402884960175, -0.07617617398500443, 0.03883206844329834, -0.08820820599794388, 0.0017798047047108412, 0.03428281471133232, -0.00552331143990159, 0.0793113186955452, 0.024981077760457993, -0.01903473772108555, -0.08578914403915405, -0.06426989287137985, 0.05055158957839012, 0.06651756912469864, 0.07274524867534637, -0.02360466867685318, -0.06909912824630737, 0.09047551453113556, -0.08408330380916595, 0.03743723779916763, 0.05303093418478966, 0.04079342633485794, 0.08387283980846405, 0.004807915072888136, -0.08137726783752441, -0.012672300450503826, 0.008321299217641354, -0.01849028281867504, 0.03934644162654877, 0.04977621138095856, -0.044209934771060944, -0.023889953270554543, -0.0321684256196022, -0.04211583361029625, -0.08553148806095123, -0.0346648171544075, -0.013076809234917164, -0.08221328258514404, -0.07956629991531372, 0.08310890197753906, 0.06475961208343506, -0.08066193014383316, -0.06599629670381546, 0.0377403125166893, 0.02414192445576191, -0.06724336743354797, -0.012318981811404228, -0.07565934211015701, -0.005451300647109747, -0.06779775023460388, -0.0153520992025733, -0.005211416631937027, -0.05987042561173439, 0.08349728584289551, -0.018055034801363945, 0.051503315567970276, 0.01604050025343895, -0.042580656707286835, 0.05554663762450218, -0.00945936981588602, -0.04717949777841568, -0.01185492891818285, -0.08013104647397995, -0.018075071275234222, 0.024637306109070778, 0.06707873195409775, -0.045720309019088745]], \"hidden2.bias\": [-0.09351526200771332, -0.05297594144940376, -0.04371781274676323, 0.003650747239589691, -0.03801266849040985, 0.0820767879486084, -0.03067031130194664, -0.024479668587446213, -0.07546201348304749, 0.07856253534555435, 0.06936778873205185, -0.0878499373793602, -0.005094331689178944, 0.04999241977930069, 0.04591991752386093, -0.051493074744939804, -0.0043536643497645855, 0.011125397868454456, -0.07700252532958984, -0.09525056183338165, 0.034014828503131866, -0.07471874356269836, 0.029220355674624443, -0.02301073633134365, -0.0006836246466264129, 0.007746845483779907, -0.007644721306860447, -0.02419303171336651, -0.012683600187301636, -0.02104530669748783, 0.01141173392534256, 0.09261228144168854, 0.07262105494737625, -0.020042138174176216, 0.07122689485549927, 0.049766283482313156, 0.05970446765422821, -0.02104288525879383, -0.05489785224199295, 0.0573359839618206, 0.03702117130160332, -0.08568082004785538, 0.04837251827120781, -0.03562052175402641, -0.01022083219140768, 0.017377279698848724, 0.04586125165224075, 0.030024241656064987, -0.048897694796323776, -0.028859548270702362], \"hidden3.weight\": [[-0.012202915735542774, 0.1087258830666542, 0.04090534895658493, 0.13542558252811432, 0.09529127925634384, -0.04507996514439583, -0.03842322155833244, -0.04395956173539162, 0.12013717740774155, 0.008878513239324093, 0.13339509069919586, 0.06495973467826843, 0.0827830508351326, 0.0645948201417923, -0.06621458381414413, 0.050077248364686966, -0.06345570087432861, -0.07942532002925873, 0.1156446635723114, -0.11520452797412872, 0.0766884982585907, 0.07515180110931396, -0.017793959006667137, -0.0013215519720688462, 0.0493912436068058, -0.021463938057422638, -0.13005784153938293, 0.09084601700305939, 0.0324985533952713, 0.05799718201160431, 0.08242124319076538, 0.1004662811756134, 0.036429595202207565, -0.09701289981603622, 0.14156989753246307, -0.10506367683410645, 0.021479148417711258, -0.045109041035175323, 0.1099737137556076, 0.08670624345541, -0.06298496574163437, 0.12000343948602676, 0.09129001200199127, -0.13526767492294312, 0.0813777893781662, -0.02459699660539627, -0.08602610975503922, 0.13892260193824768, 0.13662531971931458, 0.00487400870770216], [-0.12745334208011627, 0.0024653058499097824, 0.0749085322022438, 0.02420767769217491, -0.13842236995697021, 0.1335042417049408, 0.015141049399971962, -0.040915243327617645, -0.12050378322601318, 0.0750461146235466, 0.06423891335725784, 0.08465639501810074, 0.09451072663068771, 0.039980676025152206, -0.10989325493574142, -0.10002852231264114, 0.13341188430786133, 0.0567765086889267, -0.1261511743068695, -0.0981132909655571, 0.11720467358827591, -0.06689389795064926, 0.04771508276462555, -0.0310203917324543, 0.05553090199828148, -0.03253931924700737, 0.037406813353300095, -0.08010881394147873, 0.09933263808488846, -0.06393889337778091, -0.1378442943096161, 0.03436201065778732, 0.04971754550933838, -0.08130554109811783, -0.04202166199684143, 0.11461615562438965, -0.025845356285572052, 0.08220452070236206, 0.12852901220321655, 0.06645751744508743, 0.0852765217423439, -0.10454972833395004, -0.08661726117134094, -0.029394280165433884, -0.03604399785399437, -0.0068069021217525005, 0.09040365368127823, -0.13563063740730286, 0.08651767671108246, 0.12869234383106232], [0.08905047923326492, 0.10146614164113998, 0.11641637235879898, -0.024350835010409355, -0.04324745759367943, -0.11677426844835281, 0.10126648843288422, 0.030613793060183525, 0.002119588665664196, -0.07730872929096222, 0.018167831003665924, -0.0881858840584755, -0.13772258162498474, -0.04988745599985123, 0.035060685127973557, -0.12287317961454391, 0.12536273896694183, 0.038369908928871155, -0.06714224815368652, 0.0071150618605315685, -0.04894738644361496, 0.09887634962797165, -0.1063588559627533, 0.1177455484867096, -0.07627017050981522, 0.12400476634502411, -0.09071552753448486, -0.11163164675235748, -0.058376964181661606, -0.011365605518221855, -0.05481061711907387, -0.12883232533931732, -0.11892346292734146, 0.13058604300022125, -0.13429920375347137, 0.023603565990924835, -0.12475576251745224, 0.12972168624401093, -0.08978276699781418, 0.12974175810813904, 0.10914134234189987, -0.12493274360895157, -0.06422234326601028, -0.05596838891506195, 0.07766305655241013, -0.026761196553707123, 0.017346568405628204, -0.06744232028722763, -0.036345914006233215, 0.10193526744842529], [-0.11394517123699188, 0.006344923283904791, 0.058574751019477844, -0.01595255732536316, 0.04695623740553856, -0.06506512314081192, -0.07211831957101822, 0.09977980703115463, 0.002989098895341158, 0.06795943528413773, -0.09282352030277252, -0.143673375248909, -0.06142016872763634, 0.1315912902355194, 0.1025131419301033, 0.11871467530727386, -0.13398467004299164, 0.0904429629445076, -0.0871184691786766, 0.09241396188735962, -0.028647148981690407, 0.13027988374233246, -0.08578122407197952, -0.0901670828461647, -0.059787530452013016, 0.00348120485432446, -0.020384350791573524, 0.10042644292116165, 0.09893259406089783, -0.03892574831843376, 0.04858365282416344, 0.06760282814502716, 0.04857989400625229, -0.008798089809715748, -0.07914482057094574, 0.096597820520401, 0.13396760821342468, -0.0745735764503479, 0.11462699621915817, -0.011522767134010792, 0.02342441864311695, -0.026691649109125137, 0.002411280758678913, 0.01609034463763237, -0.0772901326417923, 0.04358230158686638, 0.06341831386089325, 0.10575307905673981, 0.08124428987503052, -0.05591481551527977], [-0.1426709145307541, 0.021791357547044754, -0.021760324016213417, 0.12170568853616714, 0.12804970145225525, 0.10855581611394882, 0.1296338438987732, 0.030024761334061623, 0.01750616915524006, -0.10745669901371002, -0.01970258727669716, 0.04136187583208084, -0.13646212220191956, 0.08128868043422699, 0.07405687868595123, -0.11444460600614548, 0.026019785553216934, 0.07150515168905258, 0.13165166974067688, 0.03955549746751785, -0.05790426954627037, 0.09326323866844177, -0.10587673634290695, -0.06838330626487732, 0.030456149950623512, -0.06927222013473511, 0.08527113497257233, 0.0638175681233406, -0.05459153652191162, -0.12867124378681183, -0.11997087299823761, 0.009058810770511627, 0.01134142093360424, 0.01927647925913334, -0.12229039520025253, 0.0548962727189064, -0.11498899757862091, 0.02748219296336174, -0.026266228407621384, -0.07566451281309128, -0.057171013206243515, -0.03704341873526573, -0.031056227162480354, -0.07238999009132385, -0.12825722992420197, -0.09239748120307922, 0.050822075456380844, 0.08922558277845383, -0.10164789855480194, -0.08666419982910156], [-0.05194033309817314, -0.0458926297724247, 0.006798946764320135, 0.04813118651509285, 0.09586978703737259, -0.020750436931848526, -0.09310971200466156, -0.09304388612508774, 0.035786084830760956, 0.022857263684272766, -0.06563582271337509, 0.0007388731464743614, 0.11842776834964752, 0.1276557445526123, -0.018028942868113518, 0.09878940135240555, -0.003666284028440714, -0.10089690238237381, 0.027610352262854576, -0.0029808294493705034, 0.018071170896291733, -0.09403592348098755, 0.0024188596289604902, -0.09519070386886597, 0.1345890909433365, -0.10027693957090378, -0.12402621656656265, 0.05681991204619408, 0.0063729919493198395, -0.028342803940176964, 0.07081039994955063, 0.10828693956136703, 0.121441550552845, -0.008229636587202549, -0.0077606369741261005, 0.08496151864528656, 0.1390472799539566, -0.02364896424114704, -0.025816502049565315, -0.009043663740158081, 0.02376558631658554, -0.08787345141172409, 0.13362832367420197, -0.01451095100492239, 0.05809460207819939, 0.12850257754325867, 0.06157122552394867, -0.0470576174557209, 0.12039132416248322, 0.035881757736206055], [-0.05602081120014191, 0.1208963394165039, -0.12494026124477386, 0.07216930389404297, -0.09656896442174911, 0.11148063093423843, -0.123501256108284, -0.04669828712940216, 0.10773851722478867, 0.03560943529009819, -0.14046010375022888, -0.12731608748435974, -0.10545140504837036, 0.0802188515663147, -0.11596071720123291, -0.006045527756214142, -0.08253111690282822, 0.10924161225557327, 0.08604533970355988, -0.051160238683223724, -0.054758843034505844, -0.07697033137083054, -0.06585383415222168, -0.06321042031049728, -0.020174801349639893, 0.09570516645908356, -0.118387371301651, 0.048350390046834946, -0.13990984857082367, -0.05096793547272682, -0.07033351063728333, 0.07352443784475327, 0.04802267998456955, -0.04420933127403259, -0.12233332544565201, -0.08796460926532745, -0.1416490375995636, 0.006345334462821484, 0.08305594325065613, -0.019377851858735085, 0.0756264328956604, -0.047820352017879486, 0.03809212148189545, -0.07763544470071793, -0.04332619532942772, 0.0971512421965599, 0.11789920181035995, 0.04236455634236336, -0.0208718404173851, -0.017506757751107216], [-0.12161663919687271, -0.017342541366815567, -0.049248483031988144, 0.07379699498414993, -0.11434237658977509, 0.05292874202132225, 0.022492187097668648, -0.1364646703004837, -0.054727040231227875, 0.09177524596452713, -0.03338728845119476, 0.0805070549249649, 0.12448333203792572, -0.09577876329421997, 0.0480368435382843, 0.08759485185146332, -0.07040238380432129, -0.08828002959489822, 0.06634914875030518, -0.03452715650200844, -0.018434720113873482, -0.03861306607723236, -0.1298617124557495, 0.028584962710738182, 0.02792227640748024, 0.09013617783784866, -0.031450361013412476, 0.1311357468366623, -0.11029116064310074, 0.05344788730144501, 0.12874098122119904, 0.060555070638656616, 0.02238290011882782, -0.04080462083220482, -0.008519246242940426, 0.13381746411323547, -0.03666206821799278, 0.10150719434022903, -0.10247385501861572, 0.04641122743487358, -0.07714550942182541, -0.11096332222223282, 0.12647311389446259, -0.034760043025016785, -0.022485772147774696, -0.00800224207341671, 0.0990348532795906, -0.08290655165910721, -0.050889987498521805, 0.005382310599088669], [0.014528313651680946, -0.11061076819896698, 0.06263915449380875, -0.12784041464328766, 0.13999922573566437, 0.10076432675123215, 0.02474803291261196, 0.1093723401427269, -0.13102099299430847, 0.07628034055233002, -0.12483729422092438, 0.1224593073129654, -0.06199943646788597, 0.08463193476200104, 0.012154805473983288, -0.0781608521938324, -0.05055166780948639, 0.12525574862957, -0.06997330486774445, 0.0501156784594059, 0.11100608855485916, -0.02549593709409237, -0.004309834446758032, 0.03984267637133598, 0.09155697375535965, 0.03962108492851257, 0.008503674529492855, -0.10733580589294434, -0.10608592629432678, -0.041718997061252594, -0.09169749915599823, -0.030481526628136635, 0.08453642576932907, -0.06781815737485886, 0.09396997094154358, -0.11493577063083649, 0.09029547870159149, -0.06563986092805862, 0.13365261256694794, -0.10325158387422562, -0.11090162396430969, 0.12089803814888, 0.0696236714720726, -0.12137054651975632, -0.10994801670312881, 0.13731242716312408, 0.06922942399978638, -0.14253783226013184, -0.042883288115262985, -0.015032200142741203], [-0.07199222594499588, 0.015371326357126236, -0.06803151965141296, -0.023535942658782005, 0.02561044879257679, 0.1277611404657364, -0.0736631378531456, 0.023248907178640366, -0.13113825023174286, -0.031544145196676254, 0.014800990000367165, -0.054640792310237885, 0.0065912241116166115, 0.09574532508850098, 0.09158056974411011, -0.046917349100112915, -0.08078223466873169, 0.09841594845056534, 0.006605681963264942, -0.1258350908756256, -0.07762793451547623, 0.050513871014118195, 0.07422119379043579, -0.004659619648009539, 0.08334008604288101, 0.12980622053146362, 0.015134111978113651, 0.101113460958004, 0.13348396122455597, 0.12054794281721115, 0.13797667622566223, -0.10454077273607254, -0.07226225733757019, -0.05788272246718407, 0.03814245015382767, 0.006428323220461607, -0.0025250855833292007, 0.0404338464140892, -0.07446365058422089, 0.14152240753173828, -0.13196706771850586, 0.0832865759730339, 0.10564526170492172, 0.047931354492902756, 0.12352434545755386, 0.11616389453411102, 0.011553335003554821, 0.06304074823856354, -0.042863328009843826, 0.05577409267425537], [0.07922250777482986, 0.09410665929317474, -0.09684683382511139, 0.09996818751096725, -0.014600581489503384, 0.10106625407934189, 0.12013231962919235, -0.0036397031508386135, -0.07180419564247131, -0.024532947689294815, 0.06488126516342163, 0.09200315922498703, -0.09065528213977814, -0.12288972735404968, -0.11754678934812546, 0.05513929948210716, 0.08611633628606796, 0.11924786120653152, -0.019011590629816055, 0.04715839773416519, 0.022008787840604782, -0.023395661264657974, -0.10845497250556946, 0.015125073492527008, -0.05209359899163246, 0.11328397691249847, 0.10708015412092209, 0.09920182079076767, 0.10463347285985947, -0.011081084609031677, -0.00798888225108385, 0.09018587321043015, -0.11853620409965515, 0.06868372857570648, 0.09241969883441925, -0.009515113197267056, 0.0328056663274765, -0.11022502183914185, 0.12395252287387848, -0.025859754532575607, -0.11099811643362045, -0.09446974843740463, -0.025649122893810272, -0.007665388751775026, -0.08229704201221466, -0.0593208447098732, 0.09150009602308273, -0.04782935976982117, -0.11150512099266052, 0.06892365962266922], [0.0030869462061673403, 0.024635018780827522, -0.10386179387569427, -0.11932281404733658, 0.005566989537328482, -0.0018063628813251853, -0.1118704080581665, -0.09191641211509705, 0.021269360557198524, 0.033849913626909256, 0.011303046718239784, 0.04220268875360489, -0.1356106698513031, -0.018102599307894707, 0.11043153703212738, -0.07674387842416763, 0.03162867948412895, 0.0366649404168129, -0.08695409446954727, 0.09124016761779785, 0.0875212773680687, 0.1364423632621765, 0.1155085414648056, 0.0028133706655353308, 0.13090433180332184, 0.025428270921111107, -0.08935101330280304, -0.06482700258493423, -0.06347924470901489, -0.014001837000250816, -0.13915038108825684, -0.05884405970573425, -0.13916079699993134, 0.06485525518655777, -0.13787181675434113, 0.010142386890947819, -0.07138459384441376, -0.004897237289696932, 0.07915353775024414, 0.03742062300443649, -0.14413289725780487, -0.10186876356601715, -0.07668343931436539, 0.04049224033951759, -0.0098018329590559, -0.0046560936607420444, 0.10235808789730072, -0.08225935697555542, -0.07717736810445786, 0.09905043989419937], [0.1121767982840538, 0.10787693411111832, 0.1251874566078186, -0.07076246291399002, -0.009988966397941113, 0.13786599040031433, 0.10390132665634155, -0.07423742860555649, -0.10926636308431625, 0.08738181740045547, 0.07880208641290665, -0.11133366078138351, -0.09341532737016678, 0.0900503471493721, -0.13347946107387543, 0.022447500377893448, 0.07567249983549118, 0.007285254076123238, 0.0033419833052903414, 0.07249514013528824, 0.05620769411325455, -0.012935531325638294, -0.0562497079372406, -0.07045723497867584, 0.06628969311714172, 0.06679078191518784, 0.08945026248693466, 0.040931884199380875, -0.08763884752988815, 0.052811216562986374, -0.03457295522093773, -0.05093659088015556, 0.0991578921675682, 0.004290258511900902, -0.001066292985342443, 0.059052474796772, -0.11237666755914688, 0.04550960659980774, -0.06326683610677719, 0.07313231378793716, 0.11900888383388519, 0.007068280130624771, -0.09681832045316696, -0.014549666084349155, -0.10304921120405197, 0.04883366823196411, 0.06935928761959076, 0.03935442864894867, -0.13233834505081177, -0.06690923124551773], [-0.014137838035821915, 0.09754002839326859, 0.0003677210188470781, 0.06245112046599388, -0.009959235787391663, -0.02279786206781864, -0.042258065193891525, 0.07188697904348373, -0.1240021139383316, 0.0025926220696419477, 0.12206408381462097, -0.12808455526828766, 0.08826571702957153, -0.122225321829319, 0.1045946553349495, -0.11406862735748291, 0.10608268529176712, 0.11032278835773468, -0.12087906897068024, -0.13645322620868683, 0.054972611367702484, -0.06043331325054169, 0.075675368309021, -0.07590118795633316, 0.02842334657907486, 0.0234816987067461, -0.061427488923072815, 0.001391681027598679, 0.08470112085342407, 0.03952101245522499, -0.08093686401844025, -0.13961690664291382, 0.0263076052069664, 0.10668715834617615, 0.06949702650308609, -0.11353456974029541, -0.02950197272002697, -0.08678467571735382, -0.008318870328366756, 0.07879529893398285, -0.10552248358726501, -0.006877034902572632, -0.006985621061176062, 0.040080729871988297, -0.0230901837348938, -0.056633614003658295, 0.10998132079839706, -0.06341277062892914, -0.07806314527988434, -0.02139534056186676], [0.06868070363998413, -0.07884392142295837, 0.12009672820568085, -0.030562419444322586, 0.06608621031045914, 0.11127249151468277, -0.1260901689529419, 0.11321142315864563, -0.035535555332899094, 0.08585274964570999, 0.039293862879276276, -0.02585192583501339, -0.07761433720588684, 0.022456839680671692, 0.07436079531908035, 0.09500721096992493, 0.10934915393590927, 0.10523135960102081, -0.13320766389369965, 0.13722896575927734, 0.029425397515296936, -0.0898250862956047, -0.11978541314601898, 0.13923422992229462, -0.0677776113152504, 0.037910427898168564, -0.07285469025373459, -0.14032968878746033, 0.0806514322757721, -0.11041975766420364, 0.014307599514722824, 0.13948273658752441, 0.11925952136516571, 0.037313465029001236, -0.0680549219250679, -0.12766964733600616, -0.06740440428256989, -0.05903211236000061, 0.13782835006713867, -0.05076945573091507, -0.13901850581169128, -0.004587831441313028, -0.11233166605234146, -0.012634444050490856, -0.0802060216665268, 0.10904189944267273, -0.05783042311668396, -0.057453304529190063, 0.13976946473121643, 0.1015416607260704], [-0.002576671075075865, -0.12068156898021698, 0.04370531067252159, 0.10931707173585892, 0.10279778391122818, -0.0751727968454361, -0.1338617503643036, -0.0776313990354538, -0.12416528165340424, 0.08840188384056091, -0.08673298358917236, 0.018959682434797287, 0.12365865707397461, 0.04468598961830139, -0.11529971659183502, -0.0017152727814391255, 0.13128440082073212, 0.019704829901456833, -0.01665942743420601, 0.11522737145423889, -0.12517349421977997, -0.06716182082891464, 0.019487569108605385, 0.1080823540687561, -0.04698619619011879, -0.08473146706819534, -0.13313868641853333, -0.1305094212293625, 0.07351672649383545, -0.07995106279850006, 0.015567193739116192, 0.04549393057823181, 0.055359844118356705, -0.13749784231185913, 0.061238303780555725, -0.047485318034887314, -0.05269891023635864, -0.0267595537006855, -0.1316332221031189, -0.022186296060681343, 0.03661521524190903, -0.13777890801429749, -0.0687313973903656, -0.09909815341234207, -0.09208108484745026, -0.07877478003501892, 0.10622087121009827, 0.03819863125681877, 0.06681123375892639, -0.1155417412519455], [-0.06546124815940857, -0.11247820407152176, 0.08445972949266434, -0.13904637098312378, -0.12886759638786316, -0.005485424306243658, 0.03397474065423012, 0.04695815220475197, -0.023799724876880646, 0.01723143458366394, 0.02076609805226326, 0.030914949253201485, -0.04155638813972473, 0.09505053609609604, 0.005779871251434088, 0.07291952520608902, 0.03976903110742569, -0.08246679604053497, -0.06773290783166885, -0.08753998577594757, -0.018761100247502327, 0.009707021526992321, -0.006479100324213505, 0.1184312105178833, 0.12486647814512253, 0.030960090458393097, 0.0029427213594317436, -0.08865833282470703, 0.0907944068312645, -0.06669410318136215, -0.11584556102752686, 0.07054805010557175, 0.08605867624282837, 0.06911903619766235, 0.014780926518142223, -0.07036541402339935, 0.08797794580459595, 0.12817269563674927, 0.06349464505910873, 0.07900048792362213, -0.06494622677564621, 0.05626338720321655, -0.06162504106760025, -0.03747602552175522, 0.056015949696302414, 0.05023099109530449, 0.08212835341691971, -0.015270416624844074, -0.034934643656015396, 0.0321737602353096], [0.07370418310165405, 0.025882737711071968, -0.10405679792165756, 0.1284416764974594, 0.06474053859710693, 0.05107010155916214, -0.02948787808418274, 0.1298217922449112, 0.0030523631721735, -0.0035596820525825024, 0.06035338342189789, -0.010336245410144329, 0.025012461468577385, 0.000484364660223946, 0.03308999910950661, 0.0034441547468304634, -0.0883539617061615, -0.04237627610564232, 0.12305141985416412, 0.09220243990421295, -0.013202488422393799, 0.010478048585355282, -0.09246689081192017, 0.103279247879982, -0.13523821532726288, 0.06899742782115936, 0.10167131572961807, 0.03142372518777847, 0.06396695226430893, -0.12845256924629211, -0.045686494559049606, -0.06549422442913055, 0.059737443923950195, -0.12318874150514603, -0.08918708562850952, 0.054915107786655426, 0.03352685272693634, -0.13421630859375, 0.10291015356779099, 0.08628153055906296, 0.024213766679167747, 0.008990393951535225, -0.11333506554365158, -0.13967525959014893, -0.10822819173336029, -0.10107431560754776, -0.09302004426717758, -0.07410942763090134, 0.13429056107997894, -0.13919922709465027], [0.12286590784788132, 0.12127596139907837, 0.019188985228538513, 0.006432227790355682, 0.04514346644282341, 0.015598584897816181, -0.10876684635877609, -0.11473909020423889, 0.13291959464550018, -0.04897148162126541, 0.07643501460552216, 0.05080914869904518, -0.07161965221166611, -0.01242685317993164, -0.026411671191453934, 0.13183343410491943, 0.013023337349295616, 0.0461767315864563, -0.08675400912761688, -0.07353375852108002, -0.10461743175983429, 0.0649896115064621, -0.11522148549556732, 0.06993911415338516, 0.07112062722444534, 0.024083001539111137, -0.045367177575826645, 0.08322449028491974, -0.10658581554889679, 0.09451080858707428, 0.007549243047833443, -0.11248952895402908, 0.04699178412556648, -0.06840464472770691, 0.11515054851770401, -0.11296556144952774, 0.09574731439352036, 0.02774014323949814, -0.14278461039066315, 0.04796118289232254, -0.047288063913583755, 0.04963213950395584, 0.01378380972892046, -0.0750792995095253, 0.031910888850688934, -0.07892361283302307, -0.025961153209209442, -0.0039815776981413364, 0.06542403995990753, 0.017467917874455452], [-0.13717398047447205, 0.058085571974515915, -0.01940024271607399, -0.08764059096574783, 0.13683351874351501, 0.035550978034734726, 0.06678853929042816, -0.00036834870115853846, -0.12423009425401688, 0.0949220061302185, 0.04089908301830292, -0.05623457953333855, 0.1254551112651825, -0.11879432201385498, 0.10270033031702042, 0.07068368047475815, 0.09224604815244675, 0.007489196490496397, -0.08804769814014435, 0.021007895469665527, -0.09239507466554642, -0.031505391001701355, -0.053297415375709534, -0.10741432756185532, 0.11544822156429291, -0.12221232056617737, 0.09880296885967255, -0.042377740144729614, -0.09369225800037384, 0.050952259451150894, -0.07047780603170395, 0.10665380954742432, 0.08428752422332764, 0.12316388636827469, -0.0007087367703206837, -0.14235173165798187, 0.02507942169904709, -0.02744925394654274, 0.049207597970962524, 0.0257542934268713, -0.12407843768596649, 0.08823172003030777, -0.0995640903711319, -0.12434232980012894, -0.030514586716890335, 0.13536453247070312, -0.09477819502353668, -0.001450241543352604, -0.0837845504283905, 0.08540184050798416], [-0.04451243206858635, -0.11475560069084167, 0.05748772621154785, -0.10462221503257751, 0.1111735999584198, 0.06649082154035568, 0.013787415809929371, 0.051323264837265015, -0.08105781674385071, 0.12366694957017899, 0.006674553267657757, -0.11133177578449249, 0.02646986022591591, -0.10653861612081528, -0.019655248150229454, -0.10416803508996964, -0.14025871455669403, 0.03782984986901283, -0.1159648522734642, 0.08706893026828766, 0.10871085524559021, 0.046843938529491425, -0.018171504139900208, -0.04081535339355469, -0.05003637075424194, -0.0018279456999152899, 0.08645126968622208, 0.09169020503759384, 0.04242989048361778, 0.12074000388383865, -0.12664978206157684, 0.0008155932882800698, 0.1250489056110382, -0.02324264496564865, -0.03733115643262863, 0.01095771323889494, -0.04211434721946716, 0.12590855360031128, -0.13653478026390076, -0.090863436460495, -0.04729733243584633, -0.018971631303429604, -0.07820698618888855, 0.12010648101568222, -0.0021721269004046917, 0.13055101037025452, -0.08402327448129654, -0.05176427215337753, -0.11848535388708115, 0.05581485480070114], [0.056097906082868576, -0.11990350484848022, -0.14165334403514862, 0.09519512951374054, 0.022649580612778664, 0.04229949787259102, -0.010080268606543541, -0.11117445677518845, -0.04363979026675224, -0.0673900842666626, -0.0060928636230528355, 0.05565076321363449, 0.06568162143230438, -0.008792861364781857, -0.13921695947647095, -0.00952929724007845, 0.09460774064064026, 0.044638633728027344, 0.062201667577028275, -0.12558358907699585, -0.13101373612880707, -0.0767253041267395, -0.03405582904815674, -0.11120534688234329, 0.029159201309084892, -0.025839906185865402, 0.003468712791800499, -0.005189490970224142, 0.01169875729829073, -0.043456096202135086, -0.13638873398303986, 0.03279556706547737, -0.0025428992230445147, -0.07367447018623352, -0.12287826091051102, 0.0017227674834430218, -0.12206241488456726, -0.0215274840593338, 0.06129884347319603, 0.008948083966970444, 0.0888042226433754, 0.1226816326379776, -0.02510179951786995, -0.09272309392690659, -0.1318216472864151, 0.01512520108371973, -0.04417213425040245, 0.12717604637145996, -0.026798313483595848, -6.184550875332206e-05], [0.06688683480024338, 0.03889022022485733, -0.031552504748106, 0.07298105955123901, -0.07893658429384232, -0.10999844968318939, 0.09612702578306198, -0.02565184235572815, -0.010860567912459373, -0.03534174710512161, -0.05244575813412666, -0.08606144785881042, 0.04540150612592697, 0.04451454058289528, -0.1209210753440857, 0.13592402637004852, -0.07272248715162277, -0.1254734992980957, 0.02954009361565113, -0.03990168496966362, -0.11650204658508301, -0.10809869319200516, -0.11445140838623047, -0.015390207059681416, -0.11860329657793045, 0.033924784511327744, -0.0992143452167511, 0.1279175728559494, 0.12434301525354385, -0.016786674037575722, 0.1099233627319336, 0.10636527836322784, 0.11414189636707306, -0.11708476394414902, 0.07767707854509354, -0.0031507976818829775, -0.09614446014165878, 0.021231677383184433, 0.089289091527462, 0.14094513654708862, -0.056882526725530624, 0.1303403228521347, -0.019816000014543533, 0.04112672805786133, -0.05229504406452179, -0.064397893846035, 0.07577741891145706, 0.09147729724645615, -0.1079849824309349, -0.07822965085506439], [-0.0462232269346714, -0.07204500585794449, 0.06883987039327621, -0.024946384131908417, -0.0554579459130764, 0.02990715391933918, -0.12446273118257523, 0.11903636157512665, -0.1225755587220192, -0.1098579615354538, -0.028738288208842278, -0.1322169005870819, 0.02979436330497265, -0.00681255292147398, 0.011071962304413319, -0.030393846333026886, 0.10636178404092789, 0.04295576363801956, 0.04850995913147926, -0.1373060643672943, 0.10733143240213394, -0.030681109055876732, -0.08220264315605164, -0.0427573099732399, 0.027147281914949417, 0.11120406538248062, -0.1065860465168953, -0.032457977533340454, 0.10780082643032074, -0.10149850696325302, 0.05145145580172539, 0.03237895667552948, -0.022688506171107292, -0.13056039810180664, 0.04101060703396797, 0.0892978087067604, 0.09665843844413757, 0.12580104172229767, 0.058916810899972916, 0.04532129317522049, 0.08700943738222122, -0.11222951859235764, 0.05915442481637001, 0.021747242659330368, 0.10974568128585815, -0.0873328149318695, -0.08607207238674164, 0.04537420719861984, -0.06517039239406586, 0.14306306838989258], [0.044081032276153564, -0.010982904583215714, -0.08317515254020691, -0.07193748652935028, -0.044952791184186935, 0.09494228661060333, -0.04530869051814079, 0.12040132284164429, -0.122196726500988, -0.13353918492794037, 0.13521215319633484, 0.0928790345788002, -0.12387415021657944, -0.0936647430062294, 0.13500642776489258, -0.08946982771158218, 0.015545379370450974, 0.039803843945264816, -0.006815174128860235, -0.006754042115062475, 0.008410791866481304, -0.037206027656793594, 0.0863003134727478, -0.009738704189658165, 0.13104301691055298, 0.1252746731042862, 0.026668496429920197, 0.06608846783638, 0.015199265442788601, 0.014802270568907261, 0.09513217210769653, -0.08137176185846329, 0.13174812495708466, 0.011992281302809715, -0.13387973606586456, 0.07541073113679886, 0.07505130767822266, -0.04236336052417755, -0.05402231961488724, -0.13939574360847473, 0.10857822000980377, -0.06611916422843933, -0.09074188023805618, -0.05645899847149849, -0.0444975383579731, -0.026282737031579018, -0.12841525673866272, -0.06350608170032501, -0.022737672552466393, -0.11969134956598282]], \"hidden3.bias\": [0.04437841475009918, -0.008466607891023159, 0.015586256980895996, -0.027318917214870453, -0.08227121829986572, 0.0594441294670105, 0.10144359618425369, 0.07443635165691376, -0.05775998905301094, -0.05414948984980583, -0.09083258360624313, 0.02120314911007881, -0.10311480611562729, 0.02452440746128559, -0.02008669264614582, -0.024488182738423347, -0.04011154547333717, -0.1081925556063652, -0.10044510662555695, 0.06948763132095337, -0.04545147716999054, -0.03720622509717941, -0.11031316220760345, -0.009416802786290646, -0.1158439889550209], \"output.weight\": [[0.0031439780723303556, -0.18326978385448456, 0.030141914263367653, 0.1722162514925003, -0.1770736426115036, 0.0842222273349762, -0.11959097534418106, 0.10336944460868835, -0.16619984805583954, 0.12101520597934723, 0.09505729377269745, 0.1869819313287735, 0.03894137218594551, 0.02065320685505867, -0.13769729435443878, 0.17220300436019897, 0.008611074648797512, 0.15618444979190826, 0.09952448308467865, -0.17444780468940735, 0.17799322307109833, 0.004035707097500563, 0.04710054397583008, -0.1373596489429474, 0.07861994951963425], [-0.05602165684103966, -0.06003626063466072, -0.007756119593977928, 0.08995331078767776, 0.05737234279513359, 0.013177013024687767, 0.1180880069732666, 0.19329825043678284, -0.031973980367183685, 0.11153692752122879, 0.09003128856420517, -0.08264967799186707, -0.1714424192905426, 0.008468479849398136, -0.10083253681659698, -0.1247941330075264, -0.1364578902721405, -0.19294394552707672, -0.12904389202594757, -0.0681910291314125, -0.01489532832056284, 0.1167883425951004, 0.06316684186458588, 0.09943855553865433, -0.0579863004386425], [0.03160601109266281, 0.19696949422359467, 0.09885423630475998, 0.11941645294427872, -0.045647043734788895, 0.1678113341331482, -0.0754258930683136, -0.003094843588769436, -0.10787228494882584, -0.1922023743391037, 0.12456034868955612, 0.1487545669078827, 0.08172988146543503, 0.030069146305322647, -0.19369736313819885, -0.14537452161312103, 0.0037343420553952456, 0.1033487617969513, -0.04444247856736183, -0.14703916013240814, 0.016324248164892197, 0.09798146784305573, -0.05189644917845726, 0.12089639157056808, -0.08752945065498352], [-0.11434345692396164, 0.07102145999670029, -0.16872574388980865, 0.06767106801271439, 0.019880272448062897, 0.1796887069940567, -0.11018485575914383, -0.0405493788421154, -0.1118682250380516, 0.038649652153253555, -0.16863103210926056, -0.1447066217660904, -0.03422609344124794, 0.20009905099868774, -0.03102927841246128, -0.015133125707507133, -0.18835175037384033, -0.18011444807052612, 0.0039371526800096035, -0.047560226172208786, 0.09260506927967072, -0.11450223624706268, 0.0035912450402975082, -0.007156887091696262, -0.12430114299058914], [-0.009107922203838825, -0.11338780075311661, 0.05621844530105591, 0.051969412714242935, -0.14455485343933105, -0.13861531019210815, -0.15704049170017242, 0.07780037820339203, 0.14208830893039703, -0.12932847440242767, -0.10080564767122269, -0.19962777197360992, -0.16559307277202606, 0.08752839267253876, 0.059780631214380264, 0.19768619537353516, 0.1213686466217041, -0.126960888504982, 0.04111529886722565, 0.16410112380981445, -0.0018384202849119902, -0.05263195186853409, 0.005344093311578035, -0.04834548383951187, 0.10678669065237045], [-0.11552334576845169, -0.09410849213600159, -0.042412079870700836, -0.05026145279407501, 0.10348216444253922, -0.1140713095664978, -0.1724044233560562, 0.00860302709043026, -0.04909648001194, -0.03496191278100014, 0.13498252630233765, -0.1487753838300705, -0.08169975876808167, -0.03331909328699112, -0.0524565763771534, 0.16662631928920746, 0.17310193181037903, -0.08719179779291153, -0.021478520706295967, -0.006162217818200588, -0.17621611058712006, 0.19923515617847443, 0.19179455935955048, -0.057191401720047, 0.1851242333650589], [-0.10123445093631744, -0.1905186027288437, 0.14512313902378082, 0.08805901557207108, -0.048567939549684525, -0.1351841241121292, 0.12999936938285828, -0.10382993519306183, -0.1716621220111847, -0.06065453961491585, -0.1030079573392868, -0.09545652568340302, 0.00823764968663454, 0.13331520557403564, -0.1819782853126526, 0.04043653979897499, 0.08671160787343979, -0.028852717950940132, 0.11907605826854706, 0.1200404241681099, -0.15817958116531372, -0.036408476531505585, 0.1125369742512703, 0.11243589967489243, 0.06031912937760353], [0.14477375149726868, 0.09180650860071182, -0.05523127317428589, -0.14105448126792908, -0.1830383986234665, -0.09683173894882202, 0.09296387434005737, -0.008434255607426167, 0.0787159726023674, 0.04944013059139252, -0.08738063275814056, 0.18010489642620087, -0.1494026482105255, -0.18187956511974335, 0.06613294780254364, 0.051672518253326416, 0.1899920403957367, -0.19605067372322083, -0.011268729344010353, 0.14222338795661926, 0.12016866356134415, -0.10558560490608215, -0.11562441289424896, -0.05639798566699028, -0.1023709699511528], [0.08339692652225494, -0.1388380527496338, -0.19716858863830566, 0.16967931389808655, -0.12807150185108185, -0.021437181159853935, -0.12563301622867584, -0.1074414774775505, 0.02053051069378853, -0.04019325599074364, -0.0754299908876419, -0.17221464216709137, 0.060643088072538376, -0.1795632541179657, -0.06627938151359558, -0.11933892220258713, -0.07433883845806122, 0.19341760873794556, -0.037917546927928925, 0.038421209901571274, -0.16186752915382385, 0.019470224156975746, -0.004519768059253693, -0.15587745606899261, -0.19349196553230286], [0.08380480855703354, -0.13096216320991516, -0.07334961742162704, 0.07613732665777206, 0.0985184982419014, 0.13799338042736053, -0.09069547802209854, -0.10367532074451447, 0.14400112628936768, 0.17385384440422058, -0.17731575667858124, 0.1251312792301178, 0.051108311861753464, 0.01065237820148468, -0.026177270337939262, -0.03610417991876602, -0.14002597332000732, -0.1348644345998764, -0.11648091673851013, -0.17079520225524902, -0.09254151582717896, 0.19906669855117798, -0.011151223443448544, -0.028448436409235, 0.03998805955052376], [0.11824945360422134, -0.0008235531859099865, -0.13027441501617432, -0.08638136833906174, -0.172015979886055, 0.11793654412031174, 0.06655253469944, -0.06284645944833755, -0.12719154357910156, -0.016060125082731247, -0.04068387299776077, -0.1422024667263031, 0.11765007674694061, -0.19332699477672577, -0.06064111366868019, -0.19933481514453888, 0.1678805649280548, -0.19595015048980713, 0.10740955173969269, 0.13983847200870514, -0.09168999642133713, 0.032050419598817825, -0.021257637068629265, -0.08118490129709244, -0.04266232252120972], [-0.020595647394657135, -0.14889059960842133, -0.1888304203748703, -0.17309844493865967, 0.18247535824775696, 0.05110340565443039, -0.09138346463441849, 0.08006592094898224, -0.13901479542255402, -0.1436053067445755, -0.1949702650308609, 0.02796023152768612, 0.07185817509889603, 0.10887608677148819, 0.1621783822774887, -0.08145883679389954, -0.002256115898489952, 0.09385878592729568, 0.1137339174747467, 0.1445583701133728, -0.022513270378112793, 0.09977773576974869, 0.0678020492196083, -0.17991282045841217, 0.08005915582180023], [0.11319848895072937, -0.046245027333498, 0.12400000542402267, 0.03174419701099396, 0.17736272513866425, -0.04384690895676613, -0.07856635004281998, -0.18685349822044373, -0.168321892619133, 0.014845125377178192, 0.16588637232780457, -0.03829189017415047, -0.02452179044485092, -0.08998711407184601, 0.09639380127191544, -0.06510167568922043, 0.1317051649093628, 0.08893734216690063, -0.11016101390123367, -0.15883973240852356, 0.028170308098196983, 0.07979397475719452, -0.12231069058179855, 0.16101783514022827, 0.07616560161113739], [-0.05919710546731949, 0.06402871012687683, -0.18227756023406982, 0.18378481268882751, 0.07108115404844284, 0.11114178597927094, -0.08595671504735947, -0.027978941798210144, 0.11357521265745163, -0.07964743673801422, -0.13344015181064606, -0.17130263149738312, -0.03992732614278793, 0.02684301882982254, -0.06224009394645691, -0.16581664979457855, 0.17240022122859955, 0.05582287162542343, -0.11430684477090836, -0.06897982954978943, -0.1760203093290329, 0.06178538501262665, -0.19625790417194366, -0.06868848949670792, -0.1510000228881836], [-0.1543790102005005, -0.05748334899544716, 0.0936768651008606, 0.1809922754764557, 0.12110842764377594, 0.019754402339458466, -0.03650966286659241, 0.0741986334323883, -0.0015725460834801197, -0.14137151837348938, -0.15812142193317413, 0.043561652302742004, -0.10538473725318909, -0.16210688650608063, 0.13366971909999847, -0.04707972705364227, -0.15404605865478516, -0.056484654545784, 0.17040543258190155, -0.15572407841682434, 0.18175019323825836, 0.05330067500472069, -0.1703750640153885, -0.0847121998667717, 0.026447640731930733], [0.11578641086816788, -0.0411355197429657, 0.13537591695785522, 0.09043470770120621, 0.1208048090338707, -0.018404120579361916, -0.044424012303352356, -0.1533413529396057, -0.19762003421783447, -0.12863689661026, -0.102956123650074, 0.03276805579662323, 0.13737447559833527, -0.02523040398955345, -0.04208929464221001, -0.05408768728375435, 0.18051889538764954, 0.13417570292949677, -0.17364734411239624, 0.11319977790117264, -0.0404202900826931, -0.11828574538230896, 0.07825915515422821, 0.11291641741991043, 0.19414152204990387], [0.001304158242419362, 0.07997748255729675, 0.01573786698281765, 0.1516968011856079, -0.13970211148262024, -0.19578388333320618, 0.059216808527708054, 0.029347626492381096, 0.07855125516653061, 0.11290328204631805, -0.04359167441725731, -0.15778136253356934, -0.06664460897445679, -0.011040069162845612, -0.1474585384130478, 0.03588884323835373, 0.1264682561159134, -0.03403182327747345, 0.07779618352651596, 0.09518112987279892, 0.19268189370632172, -0.003919522278010845, -0.031056301668286324, 0.019015738740563393, -0.07881692051887512], [0.1566571295261383, 0.013144390657544136, -0.1659497320652008, -0.12918710708618164, -0.08391676098108292, 0.027942977845668793, -0.02221822552382946, 0.11141012609004974, 0.15179234743118286, -0.033761631697416306, -0.009977401234209538, 0.182760089635849, -0.03368557617068291, 0.12314821034669876, -0.1324721723794937, 0.007278209552168846, 0.11829613149166107, -0.1796831637620926, -0.13668693602085114, -0.051569513976573944, -0.15812449157238007, 0.18904288113117218, 0.007600018754601479, 0.07704316079616547, -0.19831687211990356], [-0.13376589119434357, -0.06345327198505402, -0.022594662383198738, -0.16575540602207184, 0.04040788859128952, 0.09649819880723953, -0.1936466246843338, -0.14655590057373047, 0.017170121893286705, 0.006628755480051041, -0.13041500747203827, 0.13666166365146637, -0.03840024024248123, 0.1145595908164978, -0.15074506402015686, -0.04013447463512421, -0.1317436546087265, -0.02758759632706642, 0.13272564113140106, 0.07169834524393082, -0.1885165274143219, 0.07421790808439255, -0.15775549411773682, -0.1069328561425209, 0.11164471507072449], [0.12318577617406845, 0.11012572795152664, 0.030347192659974098, 0.012584423646330833, -0.11834512650966644, -0.16987530887126923, -0.06768961995840073, -0.13025423884391785, 0.18257473409175873, 0.06639856845140457, 0.043343450874090195, 0.0787452980875969, 0.10629502683877945, -0.015698999166488647, 0.056934867054224014, -0.1759635955095291, 0.17856095731258392, 0.17640420794487, 0.12637247145175934, 0.09592322260141373, 0.10048770904541016, 0.060871634632349014, 0.06978461146354675, -0.015456165187060833, 0.01090425904840231], [-0.16776272654533386, -0.035943351686000824, 0.03870074450969696, -0.18679183721542358, -0.08799374103546143, -0.015232487581670284, 0.17671658098697662, 0.19532418251037598, 0.18045839667320251, 0.03695645555853844, -0.158582866191864, -0.14195582270622253, 0.043197885155677795, 0.0323554202914238, 0.07357735186815262, -0.19717317819595337, -0.11988994479179382, -0.12494805455207825, 0.16586439311504364, -0.19751712679862976, -0.10062237083911896, 0.1337997317314148, -0.03539915010333061, -0.15119759738445282, -0.14673151075839996], [0.03341422230005264, 0.16251972317695618, 0.10710065811872482, 0.08612535893917084, -0.10155627876520157, 0.03673885390162468, -0.14243361353874207, -0.1226392388343811, -0.10057235509157181, 0.10604652017354965, -0.06956915557384491, -0.17264056205749512, 0.11998586356639862, -0.021239889785647392, 0.034757308661937714, 0.09424929320812225, -0.02049735188484192, 0.06572938710451126, 0.13654160499572754, -0.05490964651107788, 0.154575914144516, 0.05201582610607147, 0.007035108283162117, -0.1622198224067688, 0.15538322925567627], [0.044358331710100174, 0.12412319332361221, 0.02484072744846344, 0.10308339446783066, 0.1580817550420761, 0.020116470754146576, 0.023565897718071938, -0.17874911427497864, -0.04617885500192642, -0.13689970970153809, 0.16816562414169312, 0.0427745021879673, -0.15322747826576233, -0.051004115492105484, -0.06434169411659241, 0.1629503071308136, 0.11154325306415558, 0.0766461119055748, 0.14079608023166656, -0.12002965062856674, 0.08381278067827225, -0.09697069972753525, -0.0306998360902071, -0.05648046359419823, 0.05911235883831978], [0.20004111528396606, 0.1725849211215973, -0.09702754020690918, 0.16052301228046417, -0.06952329725027084, -0.02556438371539116, -0.09282331168651581, -0.15697713196277618, -0.06820490211248398, -0.03850055858492851, -0.1838228851556778, 0.0941668450832367, -0.026140250265598297, 0.05298299342393875, -0.16625544428825378, 0.027875104919075966, -0.0553448423743248, -0.15005548298358917, 0.02214862033724785, -0.17052707076072693, -0.17080242931842804, -0.059967007488012314, 0.18801197409629822, -0.045510854572057724, -0.03659861907362938], [0.0386999249458313, 0.19611437618732452, 0.1940835416316986, -0.06456738710403442, -0.0013168344739824533, -0.18123261630535126, -0.07490258663892746, 0.1312251091003418, -0.19718396663665771, -0.15616895258426666, 0.08134610950946808, -0.17683494091033936, -0.08236751705408096, -0.1584847867488861, -0.18374784290790558, -0.04797867313027382, 0.05410896986722946, -0.09624942392110825, -0.09302518516778946, -0.04707090184092522, -0.07792780548334122, 0.17993305623531342, 0.033882010728120804, -0.19100074470043182, -0.19282743334770203], [0.1914983093738556, 0.03843658044934273, -0.06622765958309174, 0.17591139674186707, -0.034605722874403, 0.023421965539455414, -0.1977480798959732, -0.17959092557430267, -0.19883990287780762, 0.07122322916984558, 0.13494658470153809, -0.11135250329971313, 0.08777745813131332, 0.20009282231330872, 0.04666615277528763, 0.0073925950564444065, -0.05098848044872284, 0.029605327174067497, 0.15187159180641174, 0.12856610119342804, -0.19563031196594238, 0.07465191185474396, -0.10310244560241699, 0.15734253823757172, -0.14258572459220886], [-0.19221940636634827, 0.10356516391038895, -0.06572933495044708, 0.0018020295538008213, -0.023114647716283798, 0.019994106143712997, -0.05821950361132622, 0.03440939635038376, -0.09180325269699097, 0.12951050698757172, 0.08007165044546127, -0.11556389182806015, -0.04061306267976761, 0.00021641886269208044, -0.0027940450236201286, 0.1574127972126007, -0.0755201131105423, 0.09619990736246109, -0.14276434481143951, 0.0011302040657028556, -0.13009318709373474, -0.03524203971028328, 0.06980033218860626, -0.04425232857465744, 0.16601820290088654], [-0.04049693048000336, 0.10511968284845352, -0.01691662333905697, -0.14648748934268951, 0.04919613525271416, -0.0701649934053421, 0.1046459972858429, -0.06119541451334953, -0.14993371069431305, 0.17972494661808014, -0.1978246569633484, 0.030662134289741516, 0.19328507781028748, -0.12712660431861877, -0.19579505920410156, 0.1396464705467224, 0.02344006858766079, 0.13077516853809357, 0.04129206761717796, 0.005726280156522989, 0.06932761520147324, -0.19964544475078583, 0.08473657816648483, -0.07896368950605392, -0.12411146610975266], [0.05323127657175064, 0.006386026740074158, 0.17162945866584778, -0.1148746982216835, -0.1696295440196991, 0.02270561456680298, 0.018703998997807503, -0.10425464063882828, -0.0728389322757721, 0.1597547084093094, 0.1637199968099594, -0.09462066739797592, -0.053050559014081955, -0.05804978311061859, -0.08017377555370331, -0.011706976220011711, 0.05921141430735588, -0.11359065026044846, -0.03701623156666756, -0.18273113667964935, 0.12407844513654709, -0.05397212877869606, -0.1309165209531784, 0.1906217485666275, 0.06766287237405777], [0.11139728128910065, 0.1354639232158661, 0.06766146421432495, -0.10859566926956177, -0.18737836182117462, -0.07600200921297073, -0.0547354482114315, -0.03500782698392868, 0.10948573797941208, -0.1878548413515091, 0.10242112725973129, 0.11021503061056137, 0.018015069887042046, -0.036776844412088394, 0.10987503081560135, -0.13190510869026184, -0.015304665081202984, 0.058279652148485184, 0.045550066977739334, 0.009793040342628956, 0.04741281643509865, -0.1636255532503128, 0.11535908281803131, -0.15361836552619934, 0.07403524965047836], [-0.024965224787592888, -0.09131427854299545, 0.007775457575917244, -0.04721197858452797, 0.18104663491249084, 0.16734616458415985, 0.07737750560045242, -0.005961241666227579, 0.1595015823841095, -0.17125092446804047, 0.18100449442863464, 0.08583135157823563, 0.13539192080497742, 0.1857336163520813, 0.015225804410874844, 0.05544213578104973, 0.10171955078840256, -0.1382773220539093, 0.19573354721069336, -0.011122673749923706, 0.07190041989088058, -0.1312301754951477, 0.17759832739830017, 0.03822782635688782, -0.1568697690963745], [0.13599234819412231, 0.06804540008306503, 0.09679024666547775, -0.018731771036982536, 0.035116977989673615, -0.13585050404071808, 0.17249353229999542, 0.14229457080364227, -0.012233284302055836, -0.17517182230949402, -0.13485506176948547, -0.09777575731277466, 0.13150626420974731, -0.12818464636802673, -0.09568879753351212, -0.08389141410589218, -0.15838339924812317, 0.0770009234547615, 0.06557686626911163, -0.1727822721004486, 0.14648807048797607, 0.0070428140461444855, -0.1194843202829361, 0.013291259296238422, 0.04407641664147377], [0.05981884524226189, -0.11529893428087234, 0.1977071613073349, 0.1562081277370453, 0.17553766071796417, -0.008547080680727959, -0.002406877465546131, -0.15297123789787292, 0.18520861864089966, -0.031736910343170166, -0.0509456992149353, -0.051585353910923004, -0.13017363846302032, -0.16512519121170044, -0.007332124747335911, -0.16415780782699585, -0.10494702309370041, 0.126203253865242, 0.07699943333864212, -0.1471157968044281, -0.03456336632370949, -0.08479642122983932, 0.06753632426261902, -0.016643814742565155, 0.08029790222644806], [0.0489814318716526, 0.04216543585062027, 0.1321861892938614, 0.07322487235069275, -0.1980217695236206, 0.17368263006210327, -0.1599208414554596, 0.11831137537956238, -0.01804434508085251, 0.09662119299173355, -0.07839443534612656, -0.02245616354048252, -0.05283474549651146, 0.18474479019641876, -0.11808523535728455, 0.1245279610157013, -0.17148646712303162, 0.06474994868040085, -0.16795963048934937, -0.11047706753015518, 0.10704880952835083, 0.12657035887241364, -0.028749169781804085, -0.1610042005777359, -0.15451987087726593], [0.03996104747056961, 0.11364126950502396, 0.07952335476875305, -0.19621869921684265, -0.09299066662788391, 0.08110111206769943, 0.1410527229309082, 0.10186357796192169, -0.04815144091844559, -0.02428995631635189, -0.06590460985898972, -0.07007693499326706, -0.036136332899332047, 0.06591866165399551, -0.008377855643630028, 0.04503900557756424, -0.020472019910812378, 0.10521642118692398, -0.07703020423650742, 0.022550296038389206, -0.009111050516366959, -0.13743220269680023, 0.0004851105622947216, 0.04281594231724739, 0.03302977979183197], [0.04327778518199921, -0.10024598985910416, -0.15050114691257477, 0.068386010825634, 0.013549575582146645, -0.08926060050725937, 0.19992005825042725, 0.07901499420404434, 0.16671176254749298, -0.1687062829732895, 0.0488903671503067, -0.18610543012619019, -0.11590233445167542, 0.05076832324266434, 0.0806138813495636, 0.07657883316278458, 0.09996882826089859, -0.11324688047170639, -0.07300756871700287, -0.031110651791095734, 0.025989027693867683, 0.02894393540918827, -0.09909994155168533, 0.15214334428310394, -0.1924038678407669], [0.10973968356847763, -0.188595250248909, -0.1989177167415619, -0.0014640986919403076, 0.17942236363887787, -0.012715349905192852, -0.010951352305710316, 0.14889582991600037, 0.0691312626004219, 0.17239579558372498, 0.14990726113319397, -0.06681541353464127, 0.06931943446397781, -0.13694550096988678, 0.1644512116909027, -0.06607039272785187, 0.15885142982006073, -0.1279294192790985, 0.05176739767193794, 0.016255691647529602, 0.17127129435539246, 0.15476001799106598, -0.1326611042022705, -0.128842294216156, -0.09983860701322556], [-0.13047154247760773, 0.02104974538087845, -0.06541353464126587, 0.037776269018650055, 0.16396917402744293, -0.12685205042362213, -0.1723160594701767, 0.08456689864397049, -0.15942589938640594, -0.17866642773151398, -0.14803168177604675, 0.05586731433868408, -0.195932075381279, 0.024103373289108276, 0.1958700716495514, 0.014823666773736477, 0.1357589066028595, 0.14856819808483124, -0.16076131165027618, 0.09339262545108795, 0.09689170122146606, 0.0411783792078495, -0.18313057720661163, 0.16605573892593384, -0.14976553618907928], [0.14673228561878204, -0.1759585291147232, 0.013269996270537376, 0.16565150022506714, -0.025418812409043312, -0.18017657101154327, -0.10215308517217636, 0.16520486772060394, -0.018209517002105713, 0.1256130039691925, 0.024372728541493416, 0.15178970992565155, -0.051932189613580704, 0.1461266577243805, 0.0019173186738044024, 0.12233389168977737, 0.0210708100348711, 0.1681254506111145, 0.18942612409591675, -0.18615218997001648, 0.07909419387578964, 0.046741217374801636, -0.01983718015253544, 0.07926758378744125, 0.1561485379934311], [-0.07393579930067062, 0.19553682208061218, 0.12025502324104309, 0.09351110458374023, -0.13802337646484375, 0.1898045539855957, 0.14525865018367767, -0.004923560656607151, -0.15955376625061035, -0.04305031895637512, 0.1346183717250824, -0.1399964690208435, 0.059255726635456085, -0.1299905627965927, -0.041753608733415604, 0.07062847167253494, 0.1039290726184845, -0.1116543710231781, -0.14040197432041168, 0.018809324130415916, -0.017604181542992592, -0.15097616612911224, -0.06675488501787186, -0.1818179190158844, 0.012602098286151886], [0.11970709264278412, -0.050796639174222946, 0.022425493225455284, 0.05506538227200508, -0.09195135533809662, 0.15763768553733826, 0.016033321619033813, 0.11625335365533829, -0.1298610419034958, 0.03826826065778732, -0.18124127388000488, -0.03762044012546539, 0.05172756686806679, -0.13715772330760956, -0.14504337310791016, -0.14147423207759857, -0.08624964952468872, 0.12539613246917725, 0.05429830402135849, -0.10341846197843552, 0.025929896160960197, -0.15237946808338165, -0.14428332448005676, -0.04362444579601288, -0.08063273131847382], [0.012645280919969082, 0.046275146305561066, -0.17198169231414795, -0.09602265805006027, -0.08420668542385101, -0.14524759352207184, 0.0779435932636261, 0.01104503870010376, -0.030779829248785973, -0.07973850518465042, 0.19984816014766693, -0.17357760667800903, -0.10719757527112961, -0.17971967160701752, 0.07046075165271759, -0.14228776097297668, 0.02797529846429825, -0.10092315822839737, -0.17369987070560455, -0.18151675164699554, -0.08099739998579025, 0.06697463989257812, -0.1351194977760315, -0.013286312110722065, 0.010477970354259014], [0.046089984476566315, -0.10444317013025284, -0.13989971578121185, 0.0857352614402771, 0.0415889210999012, -0.001615528017282486, 0.10582588613033295, 0.14116021990776062, 0.0760447308421135, 0.0628061592578888, -0.1055978387594223, 0.15893253684043884, -0.03617800027132034, 0.16920340061187744, -0.09300785511732101, -0.16636531054973602, -0.1450812816619873, -0.0020394260063767433, 0.004565033130347729, 0.010987581685185432, 0.1894306093454361, -0.17314516007900238, 0.016416851431131363, 0.1991865038871765, -0.011418926529586315], [-0.16224278509616852, 0.02519361674785614, -0.1730809062719345, -0.11764910072088242, -0.10713282227516174, 0.16518275439739227, 0.08638535439968109, -0.09082041680812836, 0.10447850078344345, -0.14480793476104736, -0.14791783690452576, 0.002183687873184681, -0.05083346739411354, 0.10426083952188492, 0.11432293057441711, 0.19591695070266724, -0.05792543664574623, 0.1656690537929535, -0.017361141741275787, -0.12795962393283844, -0.04734429344534874, 0.027851438149809837, 0.1147276759147644, 0.1652948558330536, -0.15359313786029816]], \"output.bias\": [0.17814545333385468, 0.1111394390463829, 0.019345860928297043, -0.17358586192131042, 0.02390998974442482, 0.03490881994366646, -0.0885310024023056, -0.09762953966856003, -0.1416892111301422, -0.0006182396318763494, 0.02785434015095234, 0.17542988061904907, 0.14335104823112488, -0.05434086546301842, 0.05638209730386734, 0.18144936859607697, 0.11638239026069641, 0.022781454026699066, 0.13091744482517242, -0.1120259165763855, 0.03314448893070221, 0.0169939287006855, 0.160708948969841, -0.025569837540388107, -0.1655178964138031, -0.01934877224266529, -0.008822309784591198, 0.05810607969760895, -0.18359880149364471, -0.11596892029047012, 0.11251730471849442, 0.15509836375713348, 0.04193803295493126, 0.021602198481559753, 0.19431322813034058, -0.1707809567451477, -0.006564370822161436, 0.1948218047618866, 0.196164071559906, -0.04142600670456886, 0.021912023425102234, -0.14668937027454376, 0.022401684895157814, 0.0868501141667366]}\n",
    "    };\n",
    "\n",
    "    // Model structure\n",
    "    class ModelForSyntheticReptile {\n",
    "      constructor(modelData) {\n",
    "        this.hidden1 = { weight: modelData[\"state_dict\"][\"hidden1.weight\"], bias: modelData[\"state_dict\"][\"hidden1.bias\"] };\n",
    "        this.hidden2 = { weight: modelData[\"state_dict\"][\"hidden2.weight\"], bias: modelData[\"state_dict\"][\"hidden2.bias\"] };\n",
    "        this.hidden3 = { weight: modelData[\"state_dict\"][\"hidden3.weight\"], bias: modelData[\"state_dict\"][\"hidden3.bias\"] };\n",
    "        this.output = { weight: modelData[\"state_dict\"][\"output.weight\"], bias: modelData[\"state_dict\"][\"output.bias\"] };\n",
    "      }\n",
    "\n",
    "      forward(x) {\n",
    "        x = this.applyLayer(x, this.hidden1, tanh);\n",
    "        x = this.applyLayer(x, this.hidden2, tanh);\n",
    "        x = this.applyLayer(x, this.hidden3, tanh);\n",
    "        x = this.applyLayer(x, this.output, (x) => x); // No activation on output\n",
    "        return x;\n",
    "      }\n",
    "\n",
    "      applyLayer(input, layer, activationFn) {\n",
    "        let z = matMul(input, layer.weight);\n",
    "        z = addBias(z, layer.bias);\n",
    "        return z.map(activationFn);\n",
    "      }\n",
    "    }\n",
    "\n",
    "    // Helper functions\n",
    "    function matMul(vec, weights) {\n",
    "      return weights.map(row => row.reduce((sum, weight, i) => sum + weight * vec[i], 0));\n",
    "    }\n",
    "\n",
    "    function addBias(vec, bias) {\n",
    "      return vec.map((v, i) => v + bias[i]);\n",
    "    }\n",
    "\n",
    "    function tanh(x) {\n",
    "      return Math.tanh(x);\n",
    "    }\n",
    "\n",
    "    function testSingleDataReturnPred(model, singleDataTest) {\n",
    "      return model.forward(singleDataTest);\n",
    "    }\n",
    "\n",
    "    // Function to calculate topk based on main and sub-checkbox groups\n",
    "    function calculateTopK() {\n",
    "      let topk = 10;\n",
    "      const checkboxGroups = [\n",
    "        ['checkbox1', 'checkbox1_1'],\n",
    "        ['checkbox2', 'checkbox2_1', 'checkbox2_2'],\n",
    "        ['checkbox3', 'checkbox3_1', 'checkbox3_2'],\n",
    "        ['checkbox4', 'checkbox4_1'],\n",
    "        ['checkbox5', 'checkbox5_1', 'checkbox5_2', 'checkbox5_3']\n",
    "      ];\n",
    "\n",
    "      checkboxGroups.forEach(group => {\n",
    "        if (group.some(id => document.getElementById(id).checked)) {\n",
    "          topk += 1;\n",
    "        }\n",
    "      });\n",
    "      \n",
    "      return topk;\n",
    "    }\n",
    "\n",
    "    function getTopKColumns(hasilPred, topk, outputColumnNames) {\n",
    "      // Create an array of indices from 0 to hasilPred.length - 1\n",
    "      const indices = Array.from(hasilPred.keys());\n",
    "\n",
    "      // Sort indices based on the values in `output` in descending order\n",
    "      indices.sort((a, b) => hasilPred[b] - hasilPred[a]);\n",
    "\n",
    "      // Get the top-k indices and corresponding values\n",
    "      const topIndices = indices.slice(0, topk);\n",
    "      const topValues = topIndices.map(i => hasilPred[i]);\n",
    "      const topColumnNames = topIndices.map(i => outputColumnNames[i]);\n",
    "\n",
    "      // Return top values, indices, and column names\n",
    "      return {\n",
    "        topValues,\n",
    "        topIndices,\n",
    "        topColumnNames\n",
    "      };\n",
    "    }\n",
    "\n",
    "    // Run test data\n",
    "    /*function runTestData(data_test) {\n",
    "      const model = new ModelForSyntheticReptile(model_json);\n",
    "      document.getElementById('output').innerText = `Data Uji: ${JSON.stringify(data_test)}\\n`;\n",
    "      const hasilPred = testSingleDataReturnPred(model, data_test);\n",
    "      document.getElementById('output').innerText += `Hasil Regresi: ${JSON.stringify(hasilPred)}\\n\\n`;\n",
    "    }*/\n",
    "    function runTestData(data_test) {\n",
    "      const model = new ModelForSyntheticReptile(model_json);\n",
    "      // document.getElementById('output').innerText = `Data Uji: ${JSON.stringify(data_test)}\\n`;\n",
    "\n",
    "      const hasilPred = testSingleDataReturnPred(model, data_test);\n",
    "      document.getElementById('output').innerText = `Hasil Prediksi: ${JSON.stringify(hasilPred)}\\n\\n`;\n",
    "\n",
    "      // Calculate topk based on checked main and sub-checkbox groups\n",
    "      //let topk = calculateTopK(); // Use the calculateTopK function\n",
    "\n",
    "      const result = getTopKColumns(hasilPred, topk, output_column_names);\n",
    "      /*document.getElementById('output').innerText = `Top Values: ${JSON.stringify(result.topValues)}\\nTop Indices: ${JSON.stringify(result.topIndices)}`;*/\n",
    "      document.getElementById('output').innerText += `Top K: ${topk}\\nTop Values: ${JSON.stringify(result.topValues)}\\nTop Indices: ${JSON.stringify(result.topIndices)}`;\n",
    "      //document.getElementById('final_output').innerText = `Top Column Names: ${JSON.stringify(result.topColumnNames)}`;\n",
    "      // Display top column names and disclaimer message in final_output\n",
    "      document.getElementById('final_output').innerText = `Top Column Names: ${JSON.stringify(result.topColumnNames)}\\n\\n*ini bukan hasil sempurna, tetap sesuaikan dengan pengalaman Anda saat mengkonsumsi makanan`;\n",
    "    }\n",
    "\n",
    "  </script>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "html_code = \"\"\"\n",
    "<div style=\"background-color: gray; width: 700px; height: 600px; padding: 10px;\">\n",
    "  <iframe src=\"static_web/index.html\" width=\"680\" height=\"580\" style=\"border: none;\"></iframe>\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_reg/model_reptile_Comb-KMT-Tiny-Reg.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "\n",
    "# Define your PyTorch model here with the specified dimensions\n",
    "# class MyModel(torch.nn.Module):\n",
    "#     def __init__(self, n_input=14, n_hidden1=100, n_hidden2=50, n_hidden3=25, n_output=44):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(n_input, n_hidden1)\n",
    "#         self.fc2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "#         self.fc3 = torch.nn.Linear(n_hidden2, n_hidden3)\n",
    "#         self.fc4 = torch.nn.Linear(n_hidden3, n_output)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         x = self.fc4(x)\n",
    "#         return x\n",
    "\n",
    "class ModelForSyntheticReptile(nn.Module):\n",
    "    def __init__(self, n_input=14, n_hidden1=100, n_hidden2=50, n_hidden3=25, n_output=44):\n",
    "        super(ModelForSyntheticReptile, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.output = nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "        # Remove the following lines if you need gradients for hidden layers\n",
    "        # for param in self.hidden1.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # for param in self.hidden2.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # for param in self.hidden3.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "# Assuming `model` is your PyTorch model\n",
    "n_input = 14\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 25\n",
    "n_output = 44\n",
    "\n",
    "elm_model_reptile = ModelForSyntheticReptile(n_input, n_hidden1, n_hidden2, n_hidden3,n_output)\n",
    "# load_model_json(f'model_reg/model_Sheet1-KM-SAR-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "# load_model_json(f'model_reg/model_Comb-KMT-Tiny-Reg.json', elm_model)  # Ganti dengan nama sheet yang sesuai\n",
    "# load_model_elm_reptile_from_json(f'model_reg/model_reptile_Comb-KMT-Tiny-Reg.json', elm_model_reptile)  # Ganti dengan nama sheet yang sesuai\n",
    "load_model_elm_reptile_from_json(f'model_reg_last/model_last_E-MAML_0.215_02-11-2024-17-51-52_fitness_ind_eg_maml_tp_2_gtvga.json')  # Ganti dengan nama sheet yang sesuai\n",
    "# hasil_pred = test_single_data_return_pred(elm_model_reptile, test_data)\n",
    "\n",
    "# Instantiate the model\n",
    "# model = MyModel()\n",
    "# model = elm_model_reptile\n",
    "\n",
    "# Create a dummy input with the correct shape\n",
    "dummy_input = torch.randn(1, 14)  # 1 batch, 14 input features\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(elm_model_reptile, dummy_input, \"model_onnx/model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemm\n",
      "Tanh\n",
      "Gemm\n",
      "Tanh\n",
      "Gemm\n",
      "Tanh\n",
      "Gemm\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"model_onnx/model.onnx\")\n",
    "\n",
    "# List all nodes and their operator types\n",
    "for node in model.graph.node:\n",
    "    print(node.op_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"onnx::Gemm_0\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 1\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 14\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ref:\n",
    "# [0] https://github.com/cj-mills/tfjs-yolox-unity-tutorial/blob/main/notebooks/ONNX-to-TF-to-TFJS.ipynb\n",
    "# [1] https://stackoverflow.com/questions/78218890/converting-onnx-model-to-tensorflow-lite-pytorch-half-pixel-not-supported\n",
    "\n",
    "onnx_model_path = \"model_onnx/model.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx_model.graph.input[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Semoga Bermanfaat. Aamiin. :D\n",
    "![]( https://docs.google.com/uc?export=download&id=1vJNmuncRehLc3WCZkFYCU6rRMVTy-w0k)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
